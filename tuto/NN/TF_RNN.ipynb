{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutoriel : Recurrent neural network (RNN) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import colors\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 15:17:30.850175: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-02 15:17:30.850213: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, RNN, LSTM, GRU, Bidirectional\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Presenting the API\n",
    "\n",
    "We will use two samples (dimension 1), four timestamps in the first and two in the second ; and three features for each timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1, 2, 3], [4, 5, 6], [7, 8, 9], [2, 2, 1]], [[2, 3, 4], [5, 6, 7]]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_in = [\n",
    "    [\n",
    "        [1, 2, 3],\n",
    "        [4, 5, 6],\n",
    "        [7, 8, 9],\n",
    "        [2, 2, 1]\n",
    "    ],\n",
    "    [\n",
    "        [2, 3, 4],\n",
    "        [5, 6, 7]\n",
    "    ]\n",
    "] # 2 samples : the first one has 4 sequences and the second 2 sequences. there are 4 features\n",
    "\n",
    "\n",
    "test_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 4, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [7., 8., 9.],\n",
       "        [2., 2., 1.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [2., 3., 4.],\n",
       "        [5., 6., 7.]]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will pad the seuences \n",
    "# it will calculate the maximum length of sequences\n",
    "# then add zeroes to the other sequences so they have the same length\n",
    "\n",
    "test_in = tf.keras.utils.pad_sequences(test_in)\n",
    "\n",
    "test_in = test_in.astype('float32')\n",
    "\n",
    "test_shape = test_in.shape\n",
    "\n",
    "print('shape:', test_shape)\n",
    "\n",
    "test_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1. LSTM with only the last state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lstm1_exp\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm1 (LSTM)                (None, 2)                 48        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48\n",
      "Trainable params: 48\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 15:17:32.957049: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-02 15:17:32.957083: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-02 15:17:32.957102: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (kariminf-pc): /proc/driver/nvidia/version does not exist\n",
      "2022-06-02 15:17:32.957322: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "lstm_model = Sequential([\n",
    "    LSTM(2, activation='relu', use_bias=True, name='lstm1')\n",
    "], name='lstm1_exp')\n",
    "\n",
    "# LSTM contains 4 gates; each gate contains 2 neurons as defined above\n",
    "# each neuron has parameters to combine x (3) and h (2) plus the bias (1) = 6 parameters\n",
    "# this gives us : 4 * 2 * 6 = 48\n",
    "lstm_model.build(input_shape=(None, test_shape[1], test_shape[2]))\n",
    "\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 277ms/step\n",
      "(2, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.7866642e-01, 2.7942717e-02],\n",
       "       [1.9537210e-02, 2.7138586e-04]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_out = lstm_model.predict(test_in)\n",
    "\n",
    "print(test_out.shape)\n",
    "\n",
    "# For each sample, we get the last state of LSTM, \n",
    "# which means, each sample (2) has an output of number of units (2)\n",
    "test_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2. LSTM which returns intermediate states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lstm2_exp\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm2 (LSTM)                (None, 4, 2)              48        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48\n",
      "Trainable params: 48\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# the same model, but we get all intermed\n",
    "lstm2_model = Sequential([\n",
    "    LSTM(2, return_sequences=True,  name='lstm2')\n",
    "], name='lstm2_exp')\n",
    "\n",
    "# LSTM contains 4 gates; each gate contains 2 neurons as defined above\n",
    "# each neuron has parameters to combine x (3) and h (2) plus the bias (1) = 6 parameters\n",
    "# this gives us : 4 * 2 * 6 = 48\n",
    "lstm2_model.build(input_shape=(None, test_shape[1], test_shape[2]))\n",
    "\n",
    "lstm2_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 350ms/step\n",
      "(2, 4, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-0.01813751,  0.05000282],\n",
       "        [ 0.44618767,  0.3395636 ],\n",
       "        [ 0.8624286 ,  0.310429  ],\n",
       "        [ 0.5880492 ,  0.32558584]],\n",
       "\n",
       "       [[ 0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ],\n",
       "        [ 0.14251402,  0.23120478],\n",
       "        [ 0.6533775 ,  0.343485  ]]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_out = lstm2_model.predict(test_in)\n",
    "\n",
    "print(test_out.shape)\n",
    "\n",
    "# For each sample, we get all states of LSTM (4), \n",
    "# each state has an output of number of units (2)\n",
    "# shape = [2 samples, 4 states, 2 values]\n",
    "test_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3. GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"gru1_exp\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru1 (GRU)                  (None, 2)                 42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gru_model = Sequential([\n",
    "    GRU(2, name='gru1') # by default : activation='tanh', use_bias=True, \n",
    "], name='gru1_exp')\n",
    "\n",
    "# GRU contains 3 gates; each gate contains 2 neurons as defined above\n",
    "# apparently h parameters and h parameters are calculated separatly\n",
    "# each neuron has parameters to combine x (3) and h (2) plus two bias (2) = 7 parameters\n",
    "# this gives us : 3 * 2 * 7 = 42\n",
    "gru_model.build(input_shape=(None, test_shape[1], test_shape[2]))\n",
    "\n",
    "gru_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 372ms/step\n",
      "(2, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.705231  , -0.29595575],\n",
       "       [ 0.511054  , -0.14981261]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_out = gru_model.predict(test_in)\n",
    "\n",
    "print(test_out.shape)\n",
    "\n",
    "# For each sample, we get the last state of LSTM, \n",
    "# which means, each sample (2) has an output of number of units (2)\n",
    "test_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.4. Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bilstm1_exp\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 4)                96        \n",
      " l)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 96\n",
      "Trainable params: 96\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bilstm_model = Sequential([\n",
    "    Bidirectional(LSTM(2, name='bilstm1'))\n",
    "], name='bilstm1_exp')\n",
    "\n",
    "# LSTM contains 4 gates; each gate contains 2 neurons as defined above\n",
    "# each neuron has parameters to combine x (3) and h (2) plus the bias (1) = 6 parameters\n",
    "# this gives us : 4 * 2 * 6 = 48 \n",
    "# Two cells, so 48 * 2 = 96 parameters\n",
    "bilstm_model.build(input_shape=(None, test_shape[1], test_shape[2]))\n",
    "\n",
    "bilstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 779ms/step\n",
      "(2, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.2246958 ,  0.16578221,  0.35787773,  0.0792452 ],\n",
       "       [-0.00085884,  0.02932756,  0.05654396,  0.08873055]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_out = bilstm_model.predict(test_in)\n",
    "\n",
    "print(test_out.shape)\n",
    "\n",
    "# For each sample, we get the last state of LSTM, \n",
    "# which means, each sample (2) has an output of number of units (2) * 2 (forward + backward)\n",
    "test_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.5. Bidirectional LSTM with intermediate states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bilstm1_exp\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_1 (Bidirectio  (None, 4, 4)             96        \n",
      " nal)                                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 96\n",
      "Trainable params: 96\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bilstm2_model = Sequential([\n",
    "    Bidirectional(LSTM(2, return_sequences=True, name='bilstm1'))\n",
    "], name='bilstm1_exp')\n",
    "\n",
    "# LSTM contains 4 gates; each gate contains 2 neurons as defined above\n",
    "# each neuron has parameters to combine x (3) and h (2) plus the bias (1) = 6 parameters\n",
    "# this gives us : 4 * 2 * 6 = 48 \n",
    "# Two cells, so 48 * 2 = 96 parameters\n",
    "bilstm2_model.build(input_shape=(None, test_shape[1], test_shape[2]))\n",
    "\n",
    "bilstm2_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0541c05af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 684ms/step\n",
      "(2, 4, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 4.2398680e-02, -2.8261372e-01, -2.2900328e-01, -3.0911764e-02],\n",
       "        [ 4.8246598e-03, -5.1124692e-01, -5.9111363e-01, -2.3039220e-02],\n",
       "        [ 2.4122061e-04, -6.8491870e-01, -7.2512102e-01, -2.5331583e-02],\n",
       "        [ 7.2909750e-02, -4.7772995e-01, -3.8514832e-01, -1.7888796e-02]],\n",
       "\n",
       "       [[ 0.0000000e+00,  0.0000000e+00, -9.8676279e-02, -3.7043985e-02],\n",
       "        [ 0.0000000e+00,  0.0000000e+00, -1.3407968e-01, -3.4704108e-02],\n",
       "        [ 1.9683568e-02, -2.9378051e-01, -3.6276820e-01, -3.1188214e-03],\n",
       "        [ 1.7498329e-03, -5.2954406e-01, -6.7740488e-01, -1.6986907e-05]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_out = bilstm2_model.predict(test_in)\n",
    "\n",
    "print(test_out.shape)\n",
    "\n",
    "# For each sample, we get all states of LSTM (4), \n",
    "# each state has an output of number of units (2) * 2\n",
    "# shape = [2 samples, 4 states, 4 values]\n",
    "test_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
