{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2CSSID-TP09. Apprentissage par renforcement\n",
    "\n",
    "Dans ce TP, nous allons implémenter un algorithme basé sur l'apprentissage par renforcement pour le problème du taxi et passager.\n",
    "Nous allons implémener l'agent (générique) qui se base sur Q-Learning et l'environnement (spécifique) qui attribue le récompenses en se basant sur les actions de l'agent.\n",
    "Ensuite, nous allons comparer entre l'exporation et l'exploitation.\n",
    "Aussi, nous allons tester l'effet du taux d'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.22.4', '1.5.0', '3.6.0')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.__version__, pd.__version__, matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Type, Dict, Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Réalisation des algorithmes\n",
    "\n",
    "Dans cette partie, nous allons implémenter un algorithme d'apprentissage par renforcement.\n",
    "Deux classes seront implémentées : l'agent et l'environnement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1. Agent\n",
    "\n",
    "Ici, nous commençons par réaliser les fonctions de l'agent.\n",
    "Nous allons impémenter Q-learning où l'agent possède une matrice des états et des actions.\n",
    "\n",
    "#### I.1.1. Création de la table Q\n",
    "\n",
    "Etant donné $n$ états et $m$ actions, nous devons créer une matrice $Q[n, m]$ initialisée à $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Création de la table Q\n",
    "def creer_Q(nbr_etats: int, nbr_actions: int) -> np.ndarray:\n",
    "    return None\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[0., 0., 0.],\n",
    "#        [0., 0., 0.],\n",
    "#        [0., 0., 0.],\n",
    "#        [0., 0., 0.],\n",
    "#        [0., 0., 0.]])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "Q5_3 = creer_Q(5, 3)\n",
    "\n",
    "Q5_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.2. Exploration et Exploration de la table Q\n",
    "\n",
    "Dans les deux fonctions, il faut choisir un entier entre $0$ et $m$ (nombre des actions).\n",
    "Dans l'exploration, ce nombre est choisi aléatoirement. \n",
    "Dans l'exploitation, ce nombre est l'action qui a une valeur max parmi celles de l'état courant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Exploration\n",
    "def exploration(Q: np.ndarray) -> int:\n",
    "    return None\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# un nombre aléatoire dans {0, 1, 2}\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "exploration(Q5_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 0, 1, 2, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Exploitation\n",
    "def exploitation(Q: np.ndarray, etat: int) -> int:\n",
    "    return None\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (2, 0, 1, 2, 1)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "Q_t = np.array([\n",
    "    [0.1, 0.2, 0.3],\n",
    "    [1.0, 0.5, 0.7],\n",
    "    [0.5, 1.0, 0.8],\n",
    "    [0.2, 0.8, 0.9],\n",
    "    [0.2, 1.0, 0.3]\n",
    "])\n",
    "\n",
    "exploitation(Q_t, 0), exploitation(Q_t, 1), exploitation(Q_t, 2), exploitation(Q_t, 3), exploitation(Q_t, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def choisir_action(Q: np.ndarray, etat: int, epsilon: float=0.2) -> int:\n",
    "    if np.random.random() < epsilon:\n",
    "        return exploration(Q)\n",
    "    else:\n",
    "        return exploitation(Q, etat)\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# Soit 2 soit un autre nombre dans {0, 1, 2}\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "choisir_action(Q_t, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.3. Mise à jours de la table Q\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) = Q(s_t, a_t) + \\alpha * (r + \\gamma * \\max_a Q(s_{t+1}, a) - Q(s_t, a_t))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1 , 0.2 , 0.3 ],\n",
       "       [1.  , 0.5 , 1.58],\n",
       "       [0.5 , 1.  , 0.8 ],\n",
       "       [0.2 , 0.8 , 0.9 ],\n",
       "       [0.2 , 1.  , 0.3 ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Mise à jours de la table Q\n",
    "def mettre_ajour_Q(Q: np.ndarray, etat: int, netat: int, action: int, alpha: float, r: float, gamma: float) -> np.ndarray:\n",
    "    new_Q = Q.copy()\n",
    "    return new_Q\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[0.1 , 0.2 , 0.3 ],\n",
    "#        [1.  , 0.5 , 1.58],\n",
    "#        [0.5 , 1.  , 0.8 ],\n",
    "#        [0.2 , 0.8 , 0.9 ],\n",
    "#        [0.2 , 1.  , 0.3 ]])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "mettre_ajour_Q(Q_t, 1, 2, 2, 0.2, 5, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.4. La classe Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.  ,  4.  ,  0.  ],\n",
       "       [-0.36, -0.2 , -2.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ],\n",
       "       [-0.2 ,  0.4 ,  0.  ],\n",
       "       [ 2.  ,  0.  ,  0.  ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Agent:\n",
    "    def __init__(self, nbr_etats: int, nbr_actions: int, alpha: float, epsilon=0.2):\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = creer_Q(nbr_etats, nbr_actions)\n",
    "    \n",
    "    def set_etat(self, etat: int):\n",
    "        self.etat = etat\n",
    "        self.action = 0\n",
    "        \n",
    "    def choisir_action(self):\n",
    "        self.action = choisir_action(self.Q, self.etat, self.epsilon)\n",
    "        return self.action\n",
    "    \n",
    "    def appliquer(self, netat: int, action: int, r: float, gamma: float):\n",
    "        self.Q = mettre_ajour_Q(self.Q, self.etat, netat, self.action, self.alpha, r, gamma)\n",
    "        self.etat = netat\n",
    "        \n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[-2.  ,  4.  ,  0.  ],\n",
    "#        [-0.36, -0.2 , -2.  ],\n",
    "#        [ 0.  ,  0.  ,  0.  ],\n",
    "#        [-0.2 ,  0.4 ,  0.  ],\n",
    "#        [ 2.  ,  0.  ,  0.  ]])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "netats_rs = [(0, -1), (1, -10), (3, -1), (1, 2), (4, -1), (1, 10), (1, -10), (0, -1), (0, 20)]\n",
    "\n",
    "agent = Agent(5, 3, 0.2, epsilon=0.) # exploitation: pour qu'il soit déterministe\n",
    "agent.set_etat(3) # etat initial = 3\n",
    "\n",
    "for netat, r in netats_rs:\n",
    "    action = agent.choisir_action()\n",
    "    # FeedBack de l'environnement (netat, r)\n",
    "    agent.appliquer(netat, action, r, gamma=0.5)\n",
    "    \n",
    "\n",
    "agent.Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2. Environnement\n",
    "\n",
    "Ici, nous allons implémenter le problème du taxi et passager : (https://arxiv.org/pdf/cs/9905014.pdf).\n",
    "Notre environnement est un espace divisé en $nb_l$ lignes et $nb_c$ colonnes pour indiquer la position.\n",
    "Il contient, aussi, un nombre d'arrêts $b_a$. \n",
    "La position du taxi est encodée en utilisant le numéro de la ligne et de la colonne (les coordonnées).\n",
    "La destination est le numéro de l'arrêt ($0<= dst < nb_a$).\n",
    "La position du passager est représentée par le numéro de l'arrêt ($0 <= psg < nb_a$) plus un numéro $psg = nb_a$ indiquant que le passager est à l'intérieur du taxi.\n",
    "\n",
    "#### I.2.1. Encodage et décodage des états\n",
    "\n",
    "**Rien à programmer ici**\n",
    "\n",
    "Ici, nous avons deux fonctions : une qui encode l'état en se basant sur la position du taxi, le numéro de l'arrêt de démarrage, le numéro de l'arrêt destinataire, le nombre des colonnes, des lignes et des arrêts.\n",
    "L'autre fonction décode un état en position du taxi : ligne et colonne plus l'arrêt du passager et l'arrêt destinataire. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encoder_etat(pos: Tuple[int, int], psg: int, dst: int, nb_l: int, nb_c: int, nb_a: int) -> int:\n",
    "    return (pos[0] * nb_c + pos[1]) * (nb_a + 1) * nb_a + (psg * nb_a + dst)\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# 153\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "encoder_etat((1, 2), 3, 1, 5, 5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decoder_etat(etat: int, nb_l:int, nb_c: int, nb_a: int) -> Tuple[int, int, int, int]:\n",
    "    nb_pa = (nb_a + 1) * nb_a # nombre max des positions passager * arret par case\n",
    "    \n",
    "    pa = etat % nb_pa # position passager * position arret\n",
    "    dst = pa % nb_a\n",
    "    psg = pa// nb_a\n",
    "    \n",
    "    lc = etat // nb_pa # ligne * colonne\n",
    "    l = lc // nb_c\n",
    "    c = lc % nb_c\n",
    "    \n",
    "    return l, c, psg, dst\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (1, 2, 3, 1)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "decoder_etat(153, 5, 5, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.2. Calculer la récompense\n",
    "\n",
    "La fonction de récompense a comme entrée : \n",
    "\n",
    "- etat : l'état courant de l'agent\n",
    "- action : numéro de l'action choisie par l'agent \n",
    "- nb_l, nb_c : nombre des lignes et des colonnes dans l'environnement\n",
    "- arrets : une liste des posittions des arrets. La position est encodée sous forme d'un tuple (x, y). P.S. Les tuples sont hashables ; donc on peut vérifier leur existance dans la liste en utilisant l'opérateur \"in\".\n",
    "- bar : un dictionnaire {pos: list entiers}. Si une position existe, on aura une liste des actions interdites (actions de positionnement).\n",
    "\n",
    "La fonction doit retourner : la récompense, l'état suivant et un booléan qui indique la fin.\n",
    "\n",
    "**La récompense**\n",
    "\n",
    "- Pour chaque action exécutée, une récompense de -1 est attribuée\n",
    "- Si l'agent essaye de déposer ou prendre un passager illégalement, une récompense de -10 est attribuée en plus. L'action \"déposer\" est considérée illégale si le passager n'est pas dans le taxi ou si le lieu de dépot n'est pas l'arrêt destinataire. L'action \"prendre\" est considérée illégale si le passager n'est pas dans la position actuelle ou il est déjà dans la voiture.\n",
    "- Si l'agent dépose le passager dans l'arrêt destinaire, il aura une récompense de +20 en plus.\n",
    "\n",
    "**L'état suivant**\n",
    "\n",
    "- L'état suivant est celui actuel sauf dans les cas suivants\n",
    "- Si le passager monte dans le taxi, l'index du passager sera \"nb_a\" (dans la voiture). \n",
    "- Si le passager arrive à sa destination en sortant du taxi, l'index du passager sera \"dst\". La fin sera True.\n",
    "- Dans le cas d'une action de postionnement ({0, 1, 2, 3}), si la position n'est pas dans celles du barrière \"bar\" ou elle existe mais l'action n'existe pas dans la liste des actions interdites, la nouvelle position et le nouveau état sont calculés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 2), (1, 2), (1, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# En se basant sur l'action et la position, retourner la nouvelle position.\n",
    "# Cette fonction ne prend pas en considération les contraintes \n",
    "def repositionner(pos: Tuple[int, int], action: int) -> Tuple[int, int]:  \n",
    "    if action == 0: # gauche\n",
    "        return pos[0], pos[1]  - 1\n",
    "    if action == 1: # droit\n",
    "        return pos[0], pos[1]  + 1\n",
    "    if action == 2: # avant \n",
    "        return pos[0] + 1, pos[1]\n",
    "    if action == 3: # arrière\n",
    "        return pos[0] - 1, pos[1]\n",
    "    \n",
    "    return pos # action > 3\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# ((2, 2), (1, 2), (1, 1))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "repositionner((1, 2), 2), repositionner((1, 2), 4), repositionner((1, 2), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-11, 56, False),\n",
       " (-11, 44, False),\n",
       " (-1, 96, False),\n",
       " (19, 0, True),\n",
       " (-11, 4, False),\n",
       " (-11, 56, False),\n",
       " (-11, 44, False),\n",
       " (-1, 44, False),\n",
       " (-1, 284, False),\n",
       " (-1, 224, False)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Récompense\n",
    "def calculer_recompense(etat: int, action: int, \n",
    "                        nb_l:int, nb_c: int, \n",
    "                        arrets: List[Tuple[int, int]],\n",
    "                        bar: Dict[Tuple[int, int], Set[int]]) -> Tuple[float, int, bool]:\n",
    "    \n",
    "    recompense = -1 # Toujours on applique cette récompense\n",
    "    netat = etat\n",
    "    fin = False\n",
    "    nb_a = len(arrets)\n",
    "    l, c, psg, dst = decoder_etat(etat, nb_l, nb_c, nb_a)\n",
    "    pos = (l, c)\n",
    "    # Compléter ici\n",
    "\n",
    "    return recompense, netat, fin\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# [(-11, 56, False),\n",
    "#  (-11, 44, False),\n",
    "#  (-1, 96, False),\n",
    "#  (19, 0, True),\n",
    "#  (-11, 4, False),\n",
    "#  (-11, 56, False),\n",
    "#  (-11, 44, False),\n",
    "#  (-1, 44, False),\n",
    "#  (-1, 284, False),\n",
    "#  (-1, 224, False)]\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "nb_l, nb_c = 5, 5\n",
    "arrets = [(0,0), (0,4), (4,0), (4,3)]\n",
    "nb_a = len(arrets)\n",
    "barrieres = {\n",
    "    (0, 1): set([1]), # barrière à droit\n",
    "    (0, 2): set([0]), # barrière à gauche\n",
    "    (3, 0): set([1]), # barrière à droit\n",
    "    (4, 0): set([1]), # barrière à droit\n",
    "    (3, 1): set([0]), # barrière à gauche\n",
    "    (4, 1): set([0]), # barrière à gauche\n",
    "    (3, 2): set([1]), # barrière à droit\n",
    "    (4, 2): set([1]), # barrière à droit\n",
    "    (3, 3): set([0]), # barrière à gauche\n",
    "    (4, 3): set([0]), # barrière à gauche\n",
    "}\n",
    "\n",
    "resultats = []\n",
    "\n",
    "tests = [# (etat, action)\n",
    "    # action = 4 (prendre un passager)\n",
    "    (encoder_etat((0, 2), 4, 0, nb_l, nb_c, nb_a), 4), # pos=(0,2); psg=dans la voiture; dst=arrêt0(0, 0)\n",
    "    (encoder_etat((0, 2), 1, 0, nb_l, nb_c, nb_a), 4), # pos=(0,2); psg=arrêt1; dst=arrêt0(0, 0)\n",
    "    (encoder_etat((0, 4), 1, 0, nb_l, nb_c, nb_a), 4), # pos=arrêt1; psg=arrêt1; dst=arrêt0(0, 0)\n",
    "    # action = 5 (déposer un passager)\n",
    "    (encoder_etat((0, 0), 4, 0, nb_l, nb_c, nb_a), 5), # pos=arrêt0; psg=dans la voiture; dst=arrêt0(0, 0)\n",
    "    (encoder_etat((0, 0), 1, 0, nb_l, nb_c, nb_a), 5), # pos=arrêt0; psg=arrêt1; dst=arrêt0(0, 0)\n",
    "    (encoder_etat((0, 2), 4, 0, nb_l, nb_c, nb_a), 5), # pos=(0, 2); psg=dans la voiture; dst=arrêt0(0, 0)\n",
    "    (encoder_etat((0, 2), 1, 0, nb_l, nb_c, nb_a), 5), # pos=(0, 2); psg=arrêt1; dst=arrêt0(0, 0)\n",
    "    # action = 0 (aller à gauche)\n",
    "    (encoder_etat((0, 2), 1, 0, nb_l, nb_c, nb_a), 0), # il existe une barrière à gauche\n",
    "    (encoder_etat((3, 0), 1, 0, nb_l, nb_c, nb_a), 0), # il existe une barrière à droite\n",
    "    (encoder_etat((2, 2), 1, 0, nb_l, nb_c, nb_a), 0), # il n'existe aucune barrière\n",
    "]\n",
    "\n",
    "for etat, action in tests:\n",
    "    resultats.append(calculer_recompense(etat, action, nb_l, nb_c, arrets, barrieres))\n",
    "\n",
    "resultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.3. La classe Envinonnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIN\n"
     ]
    }
   ],
   "source": [
    "import time, sys\n",
    "from IPython.display import HTML, display, clear_output\n",
    "\n",
    "class TaxiEnv():\n",
    "    def __init__(self, nb_l:int, nb_c: int, \n",
    "                 arrets: List[Tuple[int, int]], \n",
    "                 bar: Dict[Tuple[int, int], Set[int]], gamma: float = 0.5):\n",
    "        self.actions = ['gauche', 'droit', 'avant', 'arriere', 'prendre', 'deposer']\n",
    "        self.arrets = arrets\n",
    "        self.nb_l = nb_l\n",
    "        self.nb_c = nb_c\n",
    "        self.nb_etats = nb_l * nb_c * (len(arrets) + 1) * len(arrets)\n",
    "        self.bar = bar\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        for i in range(nb_l):\n",
    "            pos = (i, 0)\n",
    "            if pos not in bar:\n",
    "                bar[pos] = set()\n",
    "            bar[pos].add(0) # on ne peut pas aller à gauche\n",
    "            pos = (i, nb_c-1)\n",
    "            if pos not in bar:\n",
    "                bar[pos] = set()\n",
    "            bar[pos].add(1) # on ne peut pas aller à droit\n",
    "        for j in range(nb_c):\n",
    "            pos = (0, j)\n",
    "            if pos not in bar:\n",
    "                bar[pos] = set()\n",
    "            bar[pos].add(3) # on ne peut pas aller en avant\n",
    "            pos = (nb_l-1, j)\n",
    "            if pos not in bar:\n",
    "                bar[pos] = set()\n",
    "            bar[pos].add(2) # on ne peut pas aller en arrière\n",
    "            \n",
    "        \n",
    "    def ajouter_agent(self, alpha: float, epsilon=0.2):\n",
    "        self.agent = Agent(self.nb_etats, len(self.actions), alpha, epsilon=epsilon)\n",
    "    \n",
    "    def encoder_etat(self, pos: Tuple[int, int], psg: int, dst: int):\n",
    "        return encoder_etat(pos, psg, dst, self.nb_l, self.nb_c, len(self.arrets))\n",
    "    \n",
    "    def decoder_etat(self, etat: int) -> Tuple[int, int, int]:\n",
    "        return decoder_etat(etat, self.nb_l, self.nb_c, len(self.arrets))\n",
    "    \n",
    "    def initialiser(self, pos: Tuple[int, int], psg: int, dst: int):\n",
    "        etat = self.encoder_etat(pos, psg, dst)\n",
    "        self.agent.set_etat(etat)\n",
    "    \n",
    "    def transporter(self, plot=False):\n",
    "        \n",
    "        nb_l = self.nb_l\n",
    "        nb_c = self.nb_c\n",
    "        arrets = self.arrets\n",
    "        bar = self.bar\n",
    "        nb_a = len(arrets)\n",
    "        actions = self.actions\n",
    "        \n",
    "        etat = self.agent.etat\n",
    "        \n",
    "        etapes = []\n",
    "        fin = False\n",
    "        rt = 0\n",
    "        \n",
    "        while not fin:\n",
    "            action = self.agent.choisir_action()\n",
    "            r, netat, fin = calculer_recompense(etat, action, nb_l, nb_c, arrets, bar)\n",
    "            etapes.append((self.decoder_etat(etat), actions[action], r, fin))\n",
    "            self.agent.appliquer(netat, action, r, self.gamma)\n",
    "            if plot:\n",
    "                rt += r\n",
    "                html = self.dessiner()\n",
    "                html += '<div class=\"cont\">'\n",
    "                html += f'<p>Etape: {len(etapes)}</p>'\n",
    "                html += f'<p>Etat: {etat}</p>'\n",
    "                html += f'<p>Action: {actions[action]}</p>'\n",
    "                html += f'<p>Récompense: {r}</p>'\n",
    "                html += f'<p>Récompense totale: {rt}</p>'\n",
    "                html += '</div>'\n",
    "                time.sleep(0.5)\n",
    "                clear_output(wait=True)\n",
    "                display(HTML(html))\n",
    "                sys.stdout.flush()\n",
    "            etat = netat\n",
    " \n",
    "        return etapes\n",
    "    \n",
    "    def dessiner(self):\n",
    "        bordures = ['l', 'r', 'b', 't']\n",
    "        \n",
    "        nb_a = len(self.arrets)\n",
    "        \n",
    "        if hasattr(self.agent, 'etat'):\n",
    "            l, c, psg, dst = decoder_etat(self.agent.etat, self.nb_l, self.nb_c, nb_a)\n",
    "        else:\n",
    "            l, c, psg, dst = None, None, None, None\n",
    "        \n",
    "        html = \"\"\"<style>\n",
    "                div.cont {display:inline-block; margin:5px; vertical-align: top;}\n",
    "                table#t, table#t td, table#t tr {border: 1px dotted black; \n",
    "                                                background: white; padding: 1px;}\n",
    "                \n",
    "                table#t td {width: 1cm; height:1cm; text-align: center;}\n",
    "                table#t tr td.l {border-left: 2px solid red ;}\n",
    "                table#t tr td.r {border-right: 2px solid red ;}\n",
    "                table#t tr td.b {border-bottom: 2px solid red ;}\n",
    "                table#t tr td.t {border-top: 2px solid red ;}\n",
    "                table#t tr td.arret {background: yellow;}\n",
    "                </style>\n",
    "                <div class=\"cont\">\n",
    "                <table id=\"t\">\n",
    "                \"\"\"\n",
    "        for i in range(self.nb_l):\n",
    "            html += \"<tr>\"\n",
    "            for j in range(self.nb_c):\n",
    "                cls = None\n",
    "                html += \"<td \"\n",
    "                if (i, j) in self.bar:\n",
    "                    cls = 'class=\"'\n",
    "                    bl = self.bar[(i, j)]\n",
    "                    for b in bl:\n",
    "                        cls += bordures[b] + ' '\n",
    "                if (i, j) in self.arrets:\n",
    "                    if not cls:\n",
    "                        cls = 'class=\"'\n",
    "                    cls += 'arret'\n",
    "                if cls:\n",
    "                    cls += '\"'\n",
    "                    html += cls\n",
    "                html += '>'\n",
    "                cont = ''\n",
    "                if dst != None and self.arrets[dst] == (i, j):\n",
    "                    cont = '🏲'\n",
    "                if psg != None and psg != nb_a and self.arrets[psg] == (i, j):\n",
    "                    cont += '👽'\n",
    "                if (l, c) == (i, j):\n",
    "                    if psg != None and psg != nb_a:\n",
    "                        cont += '🚖'\n",
    "                    else:\n",
    "                        cont += '🚍'\n",
    "                if not cont:\n",
    "                    cont = ':'\n",
    "                html += cont + '</td>'\n",
    "            html += '</tr>'\n",
    "            \n",
    "        html += '</table></div>'\n",
    "        \n",
    "        return html\n",
    "        \n",
    "print('FIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "                div.cont {display:inline-block; margin:5px; vertical-align: top;}\n",
       "                table#t, table#t td, table#t tr {border: 1px dotted black; \n",
       "                                                background: white; padding: 1px;}\n",
       "                \n",
       "                table#t td {width: 1cm; height:1cm; text-align: center;}\n",
       "                table#t tr td.l {border-left: 2px solid red ;}\n",
       "                table#t tr td.r {border-right: 2px solid red ;}\n",
       "                table#t tr td.b {border-bottom: 2px solid red ;}\n",
       "                table#t tr td.t {border-top: 2px solid red ;}\n",
       "                table#t tr td.arret {background: yellow;}\n",
       "                </style>\n",
       "                <div class=\"cont\">\n",
       "                <table id=\"t\">\n",
       "                <tr><td class=\"l t arret\">🏲👽🚖</td><td class=\"r t \">:</td><td class=\"l t \">:</td><td class=\"t \">:</td><td class=\"r t arret\">:</td></tr><tr><td class=\"l \">:</td><td >:</td><td >:</td><td >:</td><td class=\"r \">:</td></tr><tr><td class=\"l \">:</td><td >:</td><td >:</td><td >:</td><td class=\"r \">:</td></tr><tr><td class=\"l r \">:</td><td class=\"l \">:</td><td class=\"r \">:</td><td class=\"l \">:</td><td class=\"r \">:</td></tr><tr><td class=\"l r b arret\">:</td><td class=\"l b \">:</td><td class=\"r b \">:</td><td class=\"l b arret\">:</td><td class=\"r b \">:</td></tr></table></div><div class=\"cont\"><p>Etape: 398</p><p>Etat: 16</p><p>Action: deposer</p><p>Récompense: 19</p><p>Récompense totale: -1068</p></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "arrets = [(0,0), (0,4), (4,0), (4,3)]\n",
    "barrieres = {\n",
    "    (0, 1): set([1]), # barrière à droit\n",
    "    (0, 2): set([0]), # barrière à gauche\n",
    "    (3, 0): set([1]), # barrière à droit\n",
    "    (4, 0): set([1]), # barrière à droit\n",
    "    (3, 1): set([0]), # barrière à gauche\n",
    "    (4, 1): set([0]), # barrière à gauche\n",
    "    (3, 2): set([1]), # barrière à droit\n",
    "    (4, 2): set([1]), # barrière à droit\n",
    "    (3, 3): set([0]), # barrière à gauche\n",
    "    (4, 3): set([0]), # barrière à gauche\n",
    "}\n",
    "\n",
    "taxi = TaxiEnv(5, 5, arrets, barrieres)\n",
    "taxi.ajouter_agent(0.1, 0.1)\n",
    "taxi.initialiser((3, 1), 2, 0)\n",
    "\n",
    "\n",
    "html = taxi.dessiner()\n",
    "display(HTML(html))\n",
    "hist = taxi.transporter(plot=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "                div.cont {display:inline-block; margin:5px; vertical-align: top;}\n",
       "                table#t, table#t td, table#t tr {border: 1px dotted black; \n",
       "                                                background: white; padding: 1px;}\n",
       "                \n",
       "                table#t td {width: 1cm; height:1cm; text-align: center;}\n",
       "                table#t tr td.l {border-left: 2px solid red ;}\n",
       "                table#t tr td.r {border-right: 2px solid red ;}\n",
       "                table#t tr td.b {border-bottom: 2px solid red ;}\n",
       "                table#t tr td.t {border-top: 2px solid red ;}\n",
       "                table#t tr td.arret {background: yellow;}\n",
       "                </style>\n",
       "                <div class=\"cont\">\n",
       "                <table id=\"t\">\n",
       "                <tr><td class=\"l t arret\">🏲👽🚖</td><td class=\"r t \">:</td><td class=\"l t \">:</td><td class=\"t \">:</td><td class=\"r t arret\">:</td></tr><tr><td class=\"l \">:</td><td >:</td><td >:</td><td >:</td><td class=\"r \">:</td></tr><tr><td class=\"l \">:</td><td >:</td><td >:</td><td >:</td><td class=\"r \">:</td></tr><tr><td class=\"l r \">:</td><td class=\"l \">:</td><td class=\"r \">:</td><td class=\"l \">:</td><td class=\"r \">:</td></tr><tr><td class=\"l r b arret\">:</td><td class=\"l b \">:</td><td class=\"r b \">:</td><td class=\"l b arret\">:</td><td class=\"r b \">:</td></tr></table></div><div class=\"cont\"><p>Etape: 11</p><p>Etat: 16</p><p>Action: deposer</p><p>Récompense: 19</p><p>Récompense totale: 9</p></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tester après l'exécution de la même initialisation plusieurs fois\n",
    "for i in range(1000):\n",
    "    taxi.initialiser((3, 1), 2, 0)\n",
    "    taxi.transporter() \n",
    "\n",
    "taxi.initialiser((3, 1), 2, 0)\n",
    "hist = taxi.transporter(plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((3, 1, 2, 0), 'arriere', -1, False),\n",
       " ((2, 1, 2, 0), 'gauche', -1, False),\n",
       " ((2, 0, 2, 0), 'avant', -1, False),\n",
       " ((3, 0, 2, 0), 'avant', -1, False),\n",
       " ((4, 0, 2, 0), 'prendre', -1, False),\n",
       " ((4, 0, 4, 0), 'gauche', -1, False),\n",
       " ((4, 0, 4, 0), 'arriere', -1, False),\n",
       " ((3, 0, 4, 0), 'arriere', -1, False),\n",
       " ((2, 0, 4, 0), 'arriere', -1, False),\n",
       " ((1, 0, 4, 0), 'arriere', -1, False),\n",
       " ((0, 0, 4, 0), 'deposer', 19, True)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# historique des étapes\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fin\n"
     ]
    }
   ],
   "source": [
    "# tester l'appentissage avec des initialisations aléatoires\n",
    "arrets2 = [(0,0), (0,4), (4,0), (4,3)]\n",
    "barrieres2 = {\n",
    "    (0, 1): set([1]), # barrière à droit\n",
    "    (0, 2): set([0]), # barrière à gauche\n",
    "    (3, 0): set([1]), # barrière à droit\n",
    "    (4, 0): set([1]), # barrière à droit\n",
    "    (3, 1): set([0]), # barrière à gauche\n",
    "    (4, 1): set([0]), # barrière à gauche\n",
    "    (3, 2): set([1]), # barrière à droit\n",
    "    (4, 2): set([1]), # barrière à droit\n",
    "    (3, 3): set([0]), # barrière à gauche\n",
    "    (4, 3): set([0]), # barrière à gauche\n",
    "}\n",
    "\n",
    "taxi2 = TaxiEnv(5, 5, arrets2, barrieres2)\n",
    "taxi2.ajouter_agent(0.1, 0.1)\n",
    "\n",
    "def exec_aleatoire(taxi_env, plot=False):\n",
    "    pos = np.random.randint(5), np.random.randint(5)\n",
    "    psg, dst = np.random.randint(len(arrets2)), np.random.randint(len(arrets2))\n",
    "    taxi_env.initialiser(pos, psg, dst)\n",
    "    return taxi_env.transporter(plot=plot) \n",
    "\n",
    "for i in range(10000):\n",
    "    exec_aleatoire(taxi2, plot=False)\n",
    "    \n",
    "print('fin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "                div.cont {display:inline-block; margin:5px; vertical-align: top;}\n",
       "                table#t, table#t td, table#t tr {border: 1px dotted black; \n",
       "                                                background: white; padding: 1px;}\n",
       "                \n",
       "                table#t td {width: 1cm; height:1cm; text-align: center;}\n",
       "                table#t tr td.l {border-left: 2px solid red ;}\n",
       "                table#t tr td.r {border-right: 2px solid red ;}\n",
       "                table#t tr td.b {border-bottom: 2px solid red ;}\n",
       "                table#t tr td.t {border-top: 2px solid red ;}\n",
       "                table#t tr td.arret {background: yellow;}\n",
       "                </style>\n",
       "                <div class=\"cont\">\n",
       "                <table id=\"t\">\n",
       "                <tr><td class=\"l t arret\">:</td><td class=\"r t \">:</td><td class=\"l t \">:</td><td class=\"t \">:</td><td class=\"r t arret\">🏲👽🚖</td></tr><tr><td class=\"l \">:</td><td >:</td><td >:</td><td >:</td><td class=\"r \">:</td></tr><tr><td class=\"l \">:</td><td >:</td><td >:</td><td >:</td><td class=\"r \">:</td></tr><tr><td class=\"l r \">:</td><td class=\"l \">:</td><td class=\"r \">:</td><td class=\"l \">:</td><td class=\"r \">:</td></tr><tr><td class=\"l r b arret\">:</td><td class=\"l b \">:</td><td class=\"r b \">:</td><td class=\"l b arret\">:</td><td class=\"r b \">:</td></tr></table></div><div class=\"cont\"><p>Etape: 14</p><p>Etat: 97</p><p>Action: deposer</p><p>Récompense: 19</p><p>Récompense totale: -14</p></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist = exec_aleatoire(taxi2, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Application et analyse\n",
    "\n",
    "Voici quelques outils pour tester l'apprentissage par renforcement :\n",
    "\n",
    "- OpenAI Baselines: https://github.com/openai/baselines\n",
    "- Intel Coach: https://github.com/IntelLabs/coach\n",
    "- Stable Baselines: https://github.com/DLR-RM/stable-baselines3\n",
    "- TF-Agents: https://github.com/tensorflow/agents\n",
    "- Keras-RL: https://github.com/keras-rl/keras-rl\n",
    "- Tensorforce: https://github.com/tensorforce/tensorforce\n",
    "- Chainer RL: https://github.com/chainer/chainerrl\n",
    "- Mushroom RL: https://github.com/MushroomRL/mushroom-rl\n",
    "- Acme: https://github.com/deepmind/acme\n",
    "- Dopamine: https://github.com/google/dopamine\n",
    "- RAY: https://github.com/ray-project/ray\n",
    "\n",
    "Environnements :\n",
    "\n",
    "- Gym: https://gym.openai.com/\n",
    "- iGibson: http://svl.stanford.edu/igibson/\n",
    "\n",
    "On va utiliser \"MushroomRL\" puisque l'outil implémente les méthodes traditionnelles.\n",
    "Aussi, on va utiliser \"Gym\" pour générer les environnements. \n",
    "L'environnement Taxi sera utilisé puisqu'il ne consomme pas beacoup de ressources (mémoire et calcul).\n",
    "Vous pouvez consulter \"Gym\" pour des environements plus complexes comme les jeux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /opt/penv/ml3.8/lib/python3.8/site-packages (0.26.2)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /opt/penv/ml3.8/lib/python3.8/site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/penv/ml3.8/lib/python3.8/site-packages (from gym) (1.22.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/penv/ml3.8/lib/python3.8/site-packages (from gym) (2.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /opt/penv/ml3.8/lib/python3.8/site-packages (from gym) (4.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/penv/ml3.8/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym) (3.8.1)\n",
      "Collecting mushroom_rl\n",
      "  Using cached mushroom_rl-1.7.2-cp38-cp38-linux_x86_64.whl\n",
      "Requirement already satisfied: tqdm in /opt/penv/ml3.8/lib/python3.8/site-packages (from mushroom_rl) (4.64.1)\n",
      "Requirement already satisfied: pygame in /opt/penv/ml3.8/lib/python3.8/site-packages (from mushroom_rl) (2.1.2)\n",
      "Requirement already satisfied: opencv-python in /opt/penv/ml3.8/lib/python3.8/site-packages (from mushroom_rl) (4.7.0.68)\n",
      "Requirement already satisfied: numpy-ml in /opt/penv/ml3.8/lib/python3.8/site-packages (from mushroom_rl) (0.1.2)\n",
      "Requirement already satisfied: scipy in /opt/penv/ml3.8/lib/python3.8/site-packages (from mushroom_rl) (1.9.1)\n",
      "Requirement already satisfied: torch in /opt/penv/ml3.8/lib/python3.8/site-packages (from mushroom_rl) (1.13.1)\n",
      "Requirement already satisfied: joblib in /opt/penv/ml3.8/lib/python3.8/site-packages (from mushroom_rl) (1.2.0)\n",
      "Requirement already satisfied: pytest in /opt/penv/ml3.8/lib/python3.8/site-packages (from mushroom_rl) (7.2.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/penv/ml3.8/lib/python3.8/site-packages (from mushroom_rl) (1.1.2)\n",
      "Requirement already satisfied: Cython in /opt/penv/ml3.8/lib/python3.8/site-packages (from mushroom_rl) (0.29.33)\n",
      "Requirement already satisfied: matplotlib in /opt/penv/ml3.8/lib/python3.8/site-packages (from mushroom_rl) (3.6.0)\n",
      "Requirement already satisfied: numpy in /opt/penv/ml3.8/lib/python3.8/site-packages (from mushroom_rl) (1.22.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/penv/ml3.8/lib/python3.8/site-packages (from matplotlib->mushroom_rl) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/penv/ml3.8/lib/python3.8/site-packages (from matplotlib->mushroom_rl) (4.37.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/penv/ml3.8/lib/python3.8/site-packages (from matplotlib->mushroom_rl) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/penv/ml3.8/lib/python3.8/site-packages (from matplotlib->mushroom_rl) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/penv/ml3.8/lib/python3.8/site-packages (from matplotlib->mushroom_rl) (21.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/penv/ml3.8/lib/python3.8/site-packages (from matplotlib->mushroom_rl) (1.0.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/penv/ml3.8/lib/python3.8/site-packages (from matplotlib->mushroom_rl) (9.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/penv/ml3.8/lib/python3.8/site-packages (from matplotlib->mushroom_rl) (1.4.4)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/penv/ml3.8/lib/python3.8/site-packages (from pytest->mushroom_rl) (2.0.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /opt/penv/ml3.8/lib/python3.8/site-packages (from pytest->mushroom_rl) (1.0.0)\n",
      "Requirement already satisfied: iniconfig in /opt/penv/ml3.8/lib/python3.8/site-packages (from pytest->mushroom_rl) (2.0.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/penv/ml3.8/lib/python3.8/site-packages (from pytest->mushroom_rl) (22.1.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /opt/penv/ml3.8/lib/python3.8/site-packages (from pytest->mushroom_rl) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/penv/ml3.8/lib/python3.8/site-packages (from scikit-learn->mushroom_rl) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/penv/ml3.8/lib/python3.8/site-packages (from torch->mushroom_rl) (4.3.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/penv/ml3.8/lib/python3.8/site-packages (from torch->mushroom_rl) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/penv/ml3.8/lib/python3.8/site-packages (from torch->mushroom_rl) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/penv/ml3.8/lib/python3.8/site-packages (from torch->mushroom_rl) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/penv/ml3.8/lib/python3.8/site-packages (from torch->mushroom_rl) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /opt/penv/ml3.8/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->mushroom_rl) (56.0.0)\n",
      "Requirement already satisfied: wheel in /opt/penv/ml3.8/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->mushroom_rl) (0.37.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/penv/ml3.8/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->mushroom_rl) (1.16.0)\n",
      "Installing collected packages: mushroom_rl\n",
      "Successfully installed mushroom_rl-1.7.2\n"
     ]
    }
   ],
   "source": [
    "# installation des packages\n",
    "!pip install gym\n",
    "!pip install mushroom_rl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1. Exploration vs. Exploitation\n",
    "\n",
    "Ici, nous voulons tester l'effet de l'exploration/exploitation. \n",
    "Pour ce faire, nous allons tester avec des valeurs différentes de epsilon :\n",
    "- 0: exploitation (toujours)\n",
    "- 0.5: exploitation (50%) et exploration (50%)\n",
    "- 0.9: exploration (90%) et exploitation (10%).\n",
    "\n",
    "Le nombre max, par défaut, des étapes est fixé à 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/penv/ml3.8/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "  0%|                                                                       | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [20], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [collect_dataset]\n\u001b[1;32m     27\u001b[0m core \u001b[38;5;241m=\u001b[39m Core(agent, env, callbacks_fit\u001b[38;5;241m=\u001b[39mcallbacks)\n\u001b[0;32m---> 28\u001b[0m core\u001b[38;5;241m.\u001b[39mlearn(n_episodes\u001b[38;5;241m=\u001b[39mNBR_EPISODES, n_steps_per_fit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     30\u001b[0m res \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     31\u001b[0m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnbr_etapes\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m episodes_length(collect_dataset\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m/opt/penv/ml3.8/lib/python3.8/site-packages/mushroom_rl/core/core.py:75\u001b[0m, in \u001b[0;36mCore.learn\u001b[0;34m(self, n_steps, n_episodes, n_steps_per_fit, n_episodes_per_fit, render, quiet)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     fit_condition \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_episodes_counter\\\n\u001b[1;32m     73\u001b[0m                              \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_episodes_per_fit\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_condition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/penv/ml3.8/lib/python3.8/site-packages/mushroom_rl/core/core.py:125\u001b[0m, in \u001b[0;36mCore._run\u001b[0;34m(self, n_steps, n_episodes, fit_condition, render, quiet, initial_states)\u001b[0m\n\u001b[1;32m    120\u001b[0m     steps_progress_bar \u001b[38;5;241m=\u001b[39m tqdm(disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    121\u001b[0m     episodes_progress_bar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_episodes,\n\u001b[1;32m    122\u001b[0m                                  dynamic_ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, disable\u001b[38;5;241m=\u001b[39mquiet,\n\u001b[1;32m    123\u001b[0m                                  leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmove_condition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_condition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mepisodes_progress_bar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_states\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/penv/ml3.8/lib/python3.8/site-packages/mushroom_rl/core/core.py:141\u001b[0m, in \u001b[0;36mCore._run_impl\u001b[0;34m(self, move_condition, fit_condition, steps_progress_bar, episodes_progress_bar, render, initial_states)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m last:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset(initial_states)\n\u001b[0;32m--> 141\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_step([sample])\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_total_steps_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/penv/ml3.8/lib/python3.8/site-packages/mushroom_rl/core/core.py:189\u001b[0m, in \u001b[0;36mCore._step\u001b[0;34m(self, render)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03mSingle step.\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mdraw_action(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state)\n\u001b[0;32m--> 189\u001b[0m next_state, reward, absorbing, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmdp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_episode_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m render:\n",
      "File \u001b[0;32m/opt/penv/ml3.8/lib/python3.8/site-packages/mushroom_rl/environments/gym_env.py:95\u001b[0m, in \u001b[0;36mGym.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     94\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_action(action)\n\u001b[0;32m---> 95\u001b[0m     obs, reward, absorbing, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39matleast_1d(obs), reward, absorbing, info\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "from mushroom_rl.core import Environment\n",
    "from mushroom_rl.policy import EpsGreedy\n",
    "from mushroom_rl.algorithms.value import QLearning\n",
    "from mushroom_rl.utils.dataset import compute_J\n",
    "from mushroom_rl.utils.parameters import Parameter\n",
    "from mushroom_rl.core import Core\n",
    "from mushroom_rl.utils.callbacks import CollectDataset\n",
    "from mushroom_rl.utils.callbacks.callback import Callback\n",
    "from mushroom_rl.utils.dataset import parse_dataset, episodes_length\n",
    "\n",
    "NBR_EPISODES = 1000 # nombre des exécutions\n",
    "\n",
    "env = Environment.make('Gym', 'Taxi-v3')\n",
    "\n",
    "epsilons = [.0, .5, .9]\n",
    "\n",
    "tests = []\n",
    "\n",
    "for eps in epsilons:\n",
    "    epsilon = Parameter(value=eps)\n",
    "    pi = EpsGreedy(epsilon=epsilon)\n",
    "    agent = QLearning(env.info, pi, learning_rate=Parameter(value=.3))\n",
    "    \n",
    "    collect_dataset = CollectDataset()\n",
    "    callbacks = [collect_dataset]\n",
    "    \n",
    "    core = Core(agent, env, callbacks_fit=callbacks)\n",
    "    core.learn(n_episodes=NBR_EPISODES, n_steps_per_fit=1)\n",
    "    \n",
    "    res = {}\n",
    "    res['nbr_etapes'] = episodes_length(collect_dataset.get())\n",
    "    tests.append(res)\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "for i, test in enumerate(tests):\n",
    "    plt.plot(test['nbr_etapes'], label='epsilon=' + str(epsilons[i]))\n",
    "plt.xlabel('Episodes') \n",
    "plt.ylabel('Nombre des etapes') \n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Analyser les résultats** \n",
    "\n",
    "- Pourquoi l'algorithme avec plus d'exploration ne peut pas minimiser le nombre des étapes après plusieurs épisodes ?\n",
    "- Pourquoi il existe des épisodes qui ont un nombre minimal des étapes surtout dans les dernières épisodes (toujours dans l'algorithme avec plus d'exploration) ?\n",
    "- Pourquoi celui ui utilise seulement l'exploitation prend moins d'étapes à chaque épisode ?\n",
    "- Dans ce cas, quel est l'interêt de l'exploration ?\n",
    "\n",
    "**Réponse**\n",
    "\n",
    "- ...\n",
    "- ...\n",
    "- ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### II.2. Taux d'apprentissage\n",
    "\n",
    "Ici, nous voulons tester l'effet du taux d'apprentissage sur le nombre des itérations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBR_EPISODES = 1000 # nombre des exécutions\n",
    "\n",
    "env = Environment.make('Gym', 'Taxi-v3')\n",
    "\n",
    "lrs = [.1, .2, .3]\n",
    "\n",
    "tests = []\n",
    "\n",
    "for lr in lrs:\n",
    "    epsilon = Parameter(value=0.1)\n",
    "    pi = EpsGreedy(epsilon=epsilon)\n",
    "    agent = QLearning(env.info, pi, learning_rate=Parameter(value=lr))\n",
    "    \n",
    "    collect_dataset = CollectDataset()\n",
    "    callbacks = [collect_dataset]\n",
    "    \n",
    "    core = Core(agent, env, callbacks_fit=callbacks)\n",
    "    core.learn(n_episodes=NBR_EPISODES, n_steps_per_fit=1)\n",
    "    \n",
    "    res = {}\n",
    "    res['nbr_etapes'] = episodes_length(collect_dataset.get())\n",
    "    tests.append(res)\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "for i, test in enumerate(tests):\n",
    "    plt.plot(test['nbr_etapes'], label='Taux=' + str(lrs[i]))\n",
    "plt.xlabel('Episodes') \n",
    "plt.ylabel('Nombre des etapes') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Analyser les résultats** \n",
    "\n",
    "- Quel est l'effet de $\\alpha$ sur le nombre des étapes après chaque $n$ épisodes ?\n",
    "- En consultant ce diagramme et celui avant, nous pouvons dire que l'évolution avec $\\epsilon=0$ est presque comme celle avec $\\alpha=0.3$ et l'évolution avec $\\epsilon=0.5$ est presque comme celle avec $\\alpha=0.1$. Dans ce cas, est ce que nous pouvons dire qu'il y ait une relation directe entre les deux paramètres (un peut etre remplacé avec une fonction sur l'autre) ? Pourquoi ?\n",
    "\n",
    "**Réponse**\n",
    "\n",
    "- ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
