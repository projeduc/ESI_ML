{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2CSSID-TP09. Apprentissage par renforcement\n",
    "\n",
    "Dans ce TP, nous allons impl√©menter un algorithme bas√© sur l'apprentissage par renforcement pour le probl√®me du taxi et passager.\n",
    "Nous allons impl√©mener l'agent (g√©n√©rique) qui se base sur Q-Learning et l'environnement (sp√©cifique) qui attribue le r√©compenses en se basant sur les actions de l'agent.\n",
    "Ensuite, nous allons comparer entre l'exporation et l'exploitation.\n",
    "Aussi, nous allons tester l'effet du taux d'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.22.4', '1.5.0', '3.6.0')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.__version__, pd.__version__, matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Type, Dict, Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. R√©alisation des algorithmes\n",
    "\n",
    "Dans cette partie, nous allons impl√©menter un algorithme d'apprentissage par renforcement.\n",
    "Deux classes seront impl√©ment√©es : l'agent et l'environnement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1. Agent\n",
    "\n",
    "Ici, nous commen√ßons par r√©aliser les fonctions de l'agent.\n",
    "Nous allons imp√©menter Q-learning o√π l'agent poss√®de une matrice des √©tats et des actions.\n",
    "\n",
    "#### I.1.1. Cr√©ation de la table Q\n",
    "\n",
    "Etant donn√© $n$ √©tats et $m$ actions, nous devons cr√©er une matrice $Q[n, m]$ initialis√©e √† $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Cr√©ation de la table Q\n",
    "def creer_Q(nbr_etats: int, nbr_actions: int) -> np.ndarray:\n",
    "    return None\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[0., 0., 0.],\n",
    "#        [0., 0., 0.],\n",
    "#        [0., 0., 0.],\n",
    "#        [0., 0., 0.],\n",
    "#        [0., 0., 0.]])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "Q5_3 = creer_Q(5, 3)\n",
    "\n",
    "Q5_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.2. Exploration et Exploration de la table Q\n",
    "\n",
    "Dans les deux fonctions, il faut choisir un entier entre $0$ et $m$ (nombre des actions).\n",
    "Dans l'exploration, ce nombre est choisi al√©atoirement. \n",
    "Dans l'exploitation, ce nombre est l'action qui a une valeur max parmi celles de l'√©tat courant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Exploration\n",
    "def exploration(Q: np.ndarray) -> int:\n",
    "    return None\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# un nombre al√©atoire dans {0, 1, 2}\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "exploration(Q5_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 0, 1, 2, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Exploitation\n",
    "def exploitation(Q: np.ndarray, etat: int) -> int:\n",
    "    return None\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (2, 0, 1, 2, 1)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "Q_t = np.array([\n",
    "    [0.1, 0.2, 0.3],\n",
    "    [1.0, 0.5, 0.7],\n",
    "    [0.5, 1.0, 0.8],\n",
    "    [0.2, 0.8, 0.9],\n",
    "    [0.2, 1.0, 0.3]\n",
    "])\n",
    "\n",
    "exploitation(Q_t, 0), exploitation(Q_t, 1), exploitation(Q_t, 2), exploitation(Q_t, 3), exploitation(Q_t, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def choisir_action(Q: np.ndarray, etat: int, epsilon: float=0.2) -> int:\n",
    "    if np.random.random() < epsilon:\n",
    "        return exploration(Q)\n",
    "    else:\n",
    "        return exploitation(Q, etat)\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# Soit 2 soit un autre nombre dans {0, 1, 2}\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "choisir_action(Q_t, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.3. Mise √† jours de la table Q\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) = Q(s_t, a_t) + \\alpha * (r + \\gamma * \\max_a Q(s_{t+1}, a) - Q(s_t, a_t))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1 , 0.2 , 0.3 ],\n",
       "       [1.  , 0.5 , 1.58],\n",
       "       [0.5 , 1.  , 0.8 ],\n",
       "       [0.2 , 0.8 , 0.9 ],\n",
       "       [0.2 , 1.  , 0.3 ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Mise √† jours de la table Q\n",
    "def mettre_ajour_Q(Q: np.ndarray, etat: int, netat: int, action: int, alpha: float, r: float, gamma: float) -> np.ndarray:\n",
    "    new_Q = Q.copy()\n",
    "    return new_Q\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[0.1 , 0.2 , 0.3 ],\n",
    "#        [1.  , 0.5 , 1.58],\n",
    "#        [0.5 , 1.  , 0.8 ],\n",
    "#        [0.2 , 0.8 , 0.9 ],\n",
    "#        [0.2 , 1.  , 0.3 ]])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "mettre_ajour_Q(Q_t, 1, 2, 2, 0.2, 5, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.4. La classe Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.  ,  4.  ,  0.  ],\n",
       "       [-0.36, -0.2 , -2.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ],\n",
       "       [-0.2 ,  0.4 ,  0.  ],\n",
       "       [ 2.  ,  0.  ,  0.  ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Agent:\n",
    "    def __init__(self, nbr_etats: int, nbr_actions: int, alpha: float, epsilon=0.2):\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = creer_Q(nbr_etats, nbr_actions)\n",
    "    \n",
    "    def set_etat(self, etat: int):\n",
    "        self.etat = etat\n",
    "        self.action = 0\n",
    "        \n",
    "    def choisir_action(self):\n",
    "        self.action = choisir_action(self.Q, self.etat, self.epsilon)\n",
    "        return self.action\n",
    "    \n",
    "    def appliquer(self, netat: int, action: int, r: float, gamma: float):\n",
    "        self.Q = mettre_ajour_Q(self.Q, self.etat, netat, self.action, self.alpha, r, gamma)\n",
    "        self.etat = netat\n",
    "        \n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[-2.  ,  4.  ,  0.  ],\n",
    "#        [-0.36, -0.2 , -2.  ],\n",
    "#        [ 0.  ,  0.  ,  0.  ],\n",
    "#        [-0.2 ,  0.4 ,  0.  ],\n",
    "#        [ 2.  ,  0.  ,  0.  ]])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "netats_rs = [(0, -1), (1, -10), (3, -1), (1, 2), (4, -1), (1, 10), (1, -10), (0, -1), (0, 20)]\n",
    "\n",
    "agent = Agent(5, 3, 0.2, epsilon=0.) # exploitation: pour qu'il soit d√©terministe\n",
    "agent.set_etat(3) # etat initial = 3\n",
    "\n",
    "for netat, r in netats_rs:\n",
    "    action = agent.choisir_action()\n",
    "    # FeedBack de l'environnement (netat, r)\n",
    "    agent.appliquer(netat, action, r, gamma=0.5)\n",
    "    \n",
    "\n",
    "agent.Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2. Environnement\n",
    "\n",
    "Ici, nous allons impl√©menter le probl√®me du taxi et passager : (https://arxiv.org/pdf/cs/9905014.pdf).\n",
    "Notre environnement est un espace divis√© en $nb_l$ lignes et $nb_c$ colonnes pour indiquer la position.\n",
    "Il contient, aussi, un nombre d'arr√™ts $b_a$. \n",
    "La position du taxi est encod√©e en utilisant le num√©ro de la ligne et de la colonne (les coordonn√©es).\n",
    "La destination est le num√©ro de l'arr√™t ($0<= dst < nb_a$).\n",
    "La position du passager est repr√©sent√©e par le num√©ro de l'arr√™t ($0 <= psg < nb_a$) plus un num√©ro $psg = nb_a$ indiquant que le passager est √† l'int√©rieur du taxi.\n",
    "\n",
    "#### I.2.1. Encodage et d√©codage des √©tats\n",
    "\n",
    "**Rien √† programmer ici**\n",
    "\n",
    "Ici, nous avons deux fonctions : une qui encode l'√©tat en se basant sur la position du taxi, le num√©ro de l'arr√™t de d√©marrage, le num√©ro de l'arr√™t destinataire, le nombre des colonnes, des lignes et des arr√™ts.\n",
    "L'autre fonction d√©code un √©tat en position du taxi : ligne et colonne plus l'arr√™t du passager et l'arr√™t destinataire. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encoder_etat(pos: Tuple[int, int], psg: int, dst: int, nb_l: int, nb_c: int, nb_a: int) -> int:\n",
    "    return (pos[0] * nb_c + pos[1]) * (nb_a + 1) * nb_a + (psg * nb_a + dst)\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# 153\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "encoder_etat((1, 2), 3, 1, 5, 5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decoder_etat(etat: int, nb_l:int, nb_c: int, nb_a: int) -> Tuple[int, int, int, int]:\n",
    "    nb_pa = (nb_a + 1) * nb_a # nombre max des positions passager * arret par case\n",
    "    \n",
    "    pa = etat % nb_pa # position passager * position arret\n",
    "    dst = pa % nb_a\n",
    "    psg = pa// nb_a\n",
    "    \n",
    "    lc = etat // nb_pa # ligne * colonne\n",
    "    l = lc // nb_c\n",
    "    c = lc % nb_c\n",
    "    \n",
    "    return l, c, psg, dst\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (1, 2, 3, 1)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "decoder_etat(153, 5, 5, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.2. Calculer la r√©compense\n",
    "\n",
    "La fonction de r√©compense a comme entr√©e : \n",
    "\n",
    "- etat : l'√©tat courant de l'agent\n",
    "- action : num√©ro de l'action choisie par l'agent \n",
    "- nb_l, nb_c : nombre des lignes et des colonnes dans l'environnement\n",
    "- arrets : une liste des posittions des arrets. La position est encod√©e sous forme d'un tuple (x, y). P.S. Les tuples sont hashables ; donc on peut v√©rifier leur existance dans la liste en utilisant l'op√©rateur \"in\".\n",
    "- bar : un dictionnaire {pos: list entiers}. Si une position existe, on aura une liste des actions interdites (actions de positionnement).\n",
    "\n",
    "La fonction doit retourner : la r√©compense, l'√©tat suivant et un bool√©an qui indique la fin.\n",
    "\n",
    "**La r√©compense**\n",
    "\n",
    "- Pour chaque action ex√©cut√©e, une r√©compense de -1 est attribu√©e\n",
    "- Si l'agent essaye de d√©poser ou prendre un passager ill√©galement, une r√©compense de -10 est attribu√©e en plus. L'action \"d√©poser\" est consid√©r√©e ill√©gale si le passager n'est pas dans le taxi ou si le lieu de d√©pot n'est pas l'arr√™t destinataire. L'action \"prendre\" est consid√©r√©e ill√©gale si le passager n'est pas dans la position actuelle ou il est d√©j√† dans la voiture.\n",
    "- Si l'agent d√©pose le passager dans l'arr√™t destinaire, il aura une r√©compense de +20 en plus.\n",
    "\n",
    "**L'√©tat suivant**\n",
    "\n",
    "- L'√©tat suivant est celui actuel sauf dans les cas suivants\n",
    "- Si le passager monte dans le taxi, l'index du passager sera \"nb_a\" (dans la voiture). \n",
    "- Si le passager arrive √† sa destination en sortant du taxi, l'index du passager sera \"dst\". La fin sera True.\n",
    "- Dans le cas d'une action de postionnement ({0, 1, 2, 3}), si la position n'est pas dans celles du barri√®re \"bar\" ou elle existe mais l'action n'existe pas dans la liste des actions interdites, la nouvelle position et le nouveau √©tat sont calcul√©s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 2), (1, 2), (1, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# En se basant sur l'action et la position, retourner la nouvelle position.\n",
    "# Cette fonction ne prend pas en consid√©ration les contraintes \n",
    "def repositionner(pos: Tuple[int, int], action: int) -> Tuple[int, int]:  \n",
    "    if action == 0: # gauche\n",
    "        return pos[0], pos[1]  - 1\n",
    "    if action == 1: # droit\n",
    "        return pos[0], pos[1]  + 1\n",
    "    if action == 2: # avant \n",
    "        return pos[0] + 1, pos[1]\n",
    "    if action == 3: # arri√®re\n",
    "        return pos[0] - 1, pos[1]\n",
    "    \n",
    "    return pos # action > 3\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# ((2, 2), (1, 2), (1, 1))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "repositionner((1, 2), 2), repositionner((1, 2), 4), repositionner((1, 2), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-11, 56, False),\n",
       " (-11, 44, False),\n",
       " (-1, 96, False),\n",
       " (19, 0, True),\n",
       " (-11, 4, False),\n",
       " (-11, 56, False),\n",
       " (-11, 44, False),\n",
       " (-1, 44, False),\n",
       " (-1, 284, False),\n",
       " (-1, 224, False)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: R√©compense\n",
    "def calculer_recompense(etat: int, action: int, \n",
    "                        nb_l:int, nb_c: int, \n",
    "                        arrets: List[Tuple[int, int]],\n",
    "                        bar: Dict[Tuple[int, int], Set[int]]) -> Tuple[float, int, bool]:\n",
    "    \n",
    "    recompense = -1 # Toujours on applique cette r√©compense\n",
    "    netat = etat\n",
    "    fin = False\n",
    "    nb_a = len(arrets)\n",
    "    l, c, psg, dst = decoder_etat(etat, nb_l, nb_c, nb_a)\n",
    "    pos = (l, c)\n",
    "    # Compl√©ter ici\n",
    "\n",
    "    return recompense, netat, fin\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# [(-11, 56, False),\n",
    "#  (-11, 44, False),\n",
    "#  (-1, 96, False),\n",
    "#  (19, 0, True),\n",
    "#  (-11, 4, False),\n",
    "#  (-11, 56, False),\n",
    "#  (-11, 44, False),\n",
    "#  (-1, 44, False),\n",
    "#  (-1, 284, False),\n",
    "#  (-1, 224, False)]\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "nb_l, nb_c = 5, 5\n",
    "arrets = [(0,0), (0,4), (4,0), (4,3)]\n",
    "nb_a = len(arrets)\n",
    "barrieres = {\n",
    "    (0, 1): set([1]), # barri√®re √† droit\n",
    "    (0, 2): set([0]), # barri√®re √† gauche\n",
    "    (3, 0): set([1]), # barri√®re √† droit\n",
    "    (4, 0): set([1]), # barri√®re √† droit\n",
    "    (3, 1): set([0]), # barri√®re √† gauche\n",
    "    (4, 1): set([0]), # barri√®re √† gauche\n",
    "    (3, 2): set([1]), # barri√®re √† droit\n",
    "    (4, 2): set([1]), # barri√®re √† droit\n",
    "    (3, 3): set([0]), # barri√®re √† gauche\n",
    "    (4, 3): set([0]), # barri√®re √† gauche\n",
    "}\n",
    "\n",
    "resultats = []\n",
    "\n",
    "tests = [# (etat, action)\n",
    "    # action = 4 (prendre un passager)\n",
    "    (encoder_etat((0, 2), 4, 0, nb_l, nb_c, nb_a), 4), # pos=(0,2); psg=dans la voiture; dst=arr√™t0(0, 0)\n",
    "    (encoder_etat((0, 2), 1, 0, nb_l, nb_c, nb_a), 4), # pos=(0,2); psg=arr√™t1; dst=arr√™t0(0, 0)\n",
    "    (encoder_etat((0, 4), 1, 0, nb_l, nb_c, nb_a), 4), # pos=arr√™t1; psg=arr√™t1; dst=arr√™t0(0, 0)\n",
    "    # action = 5 (d√©poser un passager)\n",
    "    (encoder_etat((0, 0), 4, 0, nb_l, nb_c, nb_a), 5), # pos=arr√™t0; psg=dans la voiture; dst=arr√™t0(0, 0)\n",
    "    (encoder_etat((0, 0), 1, 0, nb_l, nb_c, nb_a), 5), # pos=arr√™t0; psg=arr√™t1; dst=arr√™t0(0, 0)\n",
    "    (encoder_etat((0, 2), 4, 0, nb_l, nb_c, nb_a), 5), # pos=(0, 2); psg=dans la voiture; dst=arr√™t0(0, 0)\n",
    "    (encoder_etat((0, 2), 1, 0, nb_l, nb_c, nb_a), 5), # pos=(0, 2); psg=arr√™t1; dst=arr√™t0(0, 0)\n",
    "    # action = 0 (aller √† gauche)\n",
    "    (encoder_etat((0, 2), 1, 0, nb_l, nb_c, nb_a), 0), # il existe une barri√®re √† gauche\n",
    "    (encoder_etat((3, 0), 1, 0, nb_l, nb_c, nb_a), 0), # il existe une barri√®re √† droite\n",
    "    (encoder_etat((2, 2), 1, 0, nb_l, nb_c, nb_a), 0), # il n'existe aucune barri√®re\n",
    "]\n",
    "\n",
    "for etat, action in tests:\n",
    "    resultats.append(calculer_recompense(etat, action, nb_l, nb_c, arrets, barrieres))\n",
    "\n",
    "resultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.3. La classe Envinonnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIN\n"
     ]
    }
   ],
   "source": [
    "import time, sys\n",
    "from IPython.display import HTML, display, clear_output\n",
    "\n",
    "class TaxiEnv():\n",
    "    def __init__(self, nb_l:int, nb_c: int, \n",
    "                 arrets: List[Tuple[int, int]], \n",
    "                 bar: Dict[Tuple[int, int], Set[int]], gamma: float = 0.5):\n",
    "        self.actions = ['gauche', 'droit', 'avant', 'arriere', 'prendre', 'deposer']\n",
    "        self.arrets = arrets\n",
    "        self.nb_l = nb_l\n",
    "        self.nb_c = nb_c\n",
    "        self.nb_etats = nb_l * nb_c * (len(arrets) + 1) * len(arrets)\n",
    "        self.bar = bar\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        for i in range(nb_l):\n",
    "            pos = (i, 0)\n",
    "            if pos not in bar:\n",
    "                bar[pos] = set()\n",
    "            bar[pos].add(0) # on ne peut pas aller √† gauche\n",
    "            pos = (i, nb_c-1)\n",
    "            if pos not in bar:\n",
    "                bar[pos] = set()\n",
    "            bar[pos].add(1) # on ne peut pas aller √† droit\n",
    "        for j in range(nb_c):\n",
    "            pos = (0, j)\n",
    "            if pos not in bar:\n",
    "                bar[pos] = set()\n",
    "            bar[pos].add(3) # on ne peut pas aller en avant\n",
    "            pos = (nb_l-1, j)\n",
    "            if pos not in bar:\n",
    "                bar[pos] = set()\n",
    "            bar[pos].add(2) # on ne peut pas aller en arri√®re\n",
    "            \n",
    "        \n",
    "    def ajouter_agent(self, alpha: float, epsilon=0.2):\n",
    "        self.agent = Agent(self.nb_etats, len(self.actions), alpha, epsilon=epsilon)\n",
    "    \n",
    "    def encoder_etat(self, pos: Tuple[int, int], psg: int, dst: int):\n",
    "        return encoder_etat(pos, psg, dst, self.nb_l, self.nb_c, len(self.arrets))\n",
    "    \n",
    "    def decoder_etat(self, etat: int) -> Tuple[int, int, int]:\n",
    "        return decoder_etat(etat, self.nb_l, self.nb_c, len(self.arrets))\n",
    "    \n",
    "    def initialiser(self, pos: Tuple[int, int], psg: int, dst: int):\n",
    "        etat = self.encoder_etat(pos, psg, dst)\n",
    "        self.agent.set_etat(etat)\n",
    "    \n",
    "    def transporter(self, plot=False):\n",
    "        \n",
    "        nb_l = self.nb_l\n",
    "        nb_c = self.nb_c\n",
    "        arrets = self.arrets\n",
    "        bar = self.bar\n",
    "        nb_a = len(arrets)\n",
    "        actions = self.actions\n",
    "        \n",
    "        etat = self.agent.etat\n",
    "        \n",
    "        etapes = []\n",
    "        fin = False\n",
    "        rt = 0\n",
    "        \n",
    "        while not fin:\n",
    "            action = self.agent.choisir_action()\n",
    "            r, netat, fin = calculer_recompense(etat, action, nb_l, nb_c, arrets, bar)\n",
    "            etapes.append((self.decoder_etat(etat), actions[action], r, fin))\n",
    "            self.agent.appliquer(netat, action, r, self.gamma)\n",
    "            if plot:\n",
    "                rt += r\n",
    "                html = self.dessiner()\n",
    "                html += '<div class=\"cont\">'\n",
    "                html += f'<p>Etape: {len(etapes)}</p>'\n",
    "                html += f'<p>Etat: {etat}</p>'\n",
    "                html += f'<p>Action: {actions[action]}</p>'\n",
    "                html += f'<p>R√©compense: {r}</p>'\n",
    "                html += f'<p>R√©compense totale: {rt}</p>'\n",
    "                html += '</div>'\n",
    "                time.sleep(0.5)\n",
    "                clear_output(wait=True)\n",
    "                display(HTML(html))\n",
    "                sys.stdout.flush()\n",
    "            etat = netat\n",
    " \n",
    "        return etapes\n",
    "    \n",
    "    def dessiner(self):\n",
    "        bordures = ['l', 'r', 'b', 't']\n",
    "        \n",
    "        nb_a = len(self.arrets)\n",
    "        \n",
    "        if hasattr(self.agent, 'etat'):\n",
    "            l, c, psg, dst = decoder_etat(self.agent.etat, self.nb_l, self.nb_c, nb_a)\n",
    "        else:\n",
    "            l, c, psg, dst = None, None, None, None\n",
    "        \n",
    "        html = \"\"\"<style>\n",
    "                div.cont {display:inline-block; margin:5px; vertical-align: top;}\n",
    "                table#t, table#t td, table#t tr {border: 1px dotted black; \n",
    "                                                background: white; padding: 1px;}\n",
    "                \n",
    "                table#t td {width: 1cm; height:1cm; text-align: center;}\n",
    "                table#t tr td.l {border-left: 2px solid red ;}\n",
    "                table#t tr td.r {border-right: 2px solid red ;}\n",
    "                table#t tr td.b {border-bottom: 2px solid red ;}\n",
    "                table#t tr td.t {border-top: 2px solid red ;}\n",
    "                table#t tr td.arret {background: yellow;}\n",
    "                </style>\n",
    "                <div class=\"cont\">\n",
    "                <table id=\"t\">\n",
    "                \"\"\"\n",
    "        for i in range(self.nb_l):\n",
    "            html += \"<tr>\"\n",
    "            for j in range(self.nb_c):\n",
    "                cls = None\n",
    "                html += \"<td \"\n",
    "                if (i, j) in self.bar:\n",
    "                    cls = 'class=\"'\n",
    "                    bl = self.bar[(i, j)]\n",
    "                    for b in bl:\n",
    "                        cls += bordures[b] + ' '\n",
    "                if (i, j) in self.arrets:\n",
    "                    if not cls:\n",
    "                        cls = 'class=\"'\n",
    "                    cls += 'arret'\n",
    "                if cls:\n",
    "                    cls += '\"'\n",
    "                    html += cls\n",
    "                html += '>'\n",
    "                cont = ''\n",
    "                if dst != None and self.arrets[dst] == (i, j):\n",
    "                    cont = 'üè≤'\n",
    "                if psg != None and psg != nb_a and self.arrets[psg] == (i, j):\n",
    "                    cont += 'üëΩ'\n",
    "                if (l, c) == (i, j):\n",
    "                    if psg != None and psg != nb_a:\n",
    "                        cont += 'üöñ'\n",
    "                    else:\n",
    "                        cont += 'üöç'\n",
    "                if not cont:\n",
    "                    cont = ':'\n",
    "                html += cont + '</td>'\n",
    "            html += '</tr>'\n",
    "            \n",
    "        html += '</table></div>'\n",
    "        \n",
    "        return html\n",
    "        \n",
    "print('FIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "                div.cont {display:inline-block; margin:5px; vertical-align: top;}\n",
       "                table#t, table#t td, table#t tr {border: 1px dotted black; \n",
       "                                                background: white; padding: 1px;}\n",
       "                \n",
       "                table#t td {width: 1cm; height:1cm; text-align: center;}\n",
       "                table#t tr td.l {border-left: 2px solid red ;}\n",
       "                table#t tr td.r {border-right: 2px solid red ;}\n",
       "                table#t tr td.b {border-bottom: 2px solid red ;}\n",
       "                table#t tr td.t {border-top: 2px solid red ;}\n",
       "                table#t tr td.arret {background: yellow;}\n",
       "                </style>\n",
       "                <div class=\"cont\">\n",
       "                <table id=\"t\">\n",
       "                <tr><td class=\"l t arret\">üè≤üëΩüöñ</td><td class=\"r t \">:</td><td class=\"l t \">:</td><td class=\"t \">:</td><td class=\"r t arret\">:</td></tr><tr><td class=\"l \">:</td><td >:</td><td >:</td><td >:</td><td class=\"r \">:</td></tr><tr><td class=\"l \">:</td><td >:</td><td >:</td><td >:</td><td class=\"r \">:</td></tr><tr><td class=\"l r \">:</td><td class=\"l \">:</td><td class=\"r \">:</td><td class=\"l \">:</td><td class=\"r \">:</td></tr><tr><td class=\"l r b arret\">:</td><td class=\"l b \">:</td><td class=\"r b \">:</td><td class=\"l b arret\">:</td><td class=\"r b \">:</td></tr></table></div><div class=\"cont\"><p>Etape: 398</p><p>Etat: 16</p><p>Action: deposer</p><p>R√©compense: 19</p><p>R√©compense totale: -1068</p></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "arrets = [(0,0), (0,4), (4,0), (4,3)]\n",
    "barrieres = {\n",
    "    (0, 1): set([1]), # barri√®re √† droit\n",
    "    (0, 2): set([0]), # barri√®re √† gauche\n",
    "    (3, 0): set([1]), # barri√®re √† droit\n",
    "    (4, 0): set([1]), # barri√®re √† droit\n",
    "    (3, 1): set([0]), # barri√®re √† gauche\n",
    "    (4, 1): set([0]), # barri√®re √† gauche\n",
    "    (3, 2): set([1]), # barri√®re √† droit\n",
    "    (4, 2): set([1]), # barri√®re √† droit\n",
    "    (3, 3): set([0]), # barri√®re √† gauche\n",
    "    (4, 3): set([0]), # barri√®re √† gauche\n",
    "}\n",
    "\n",
    "taxi = TaxiEnv(5, 5, arrets, barrieres)\n",
    "taxi.ajouter_agent(0.1, 0.1)\n",
    "taxi.initialiser((3, 1), 2, 0)\n",
    "\n",
    "\n",
    "html = taxi.dessiner()\n",
    "display(HTML(html))\n",
    "hist = taxi.transporter(plot=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "                div.cont {display:inline-block; margin:5px; vertical-align: top;}\n",
       "                table#t, table#t td, table#t tr {border: 1px dotted black; \n",
       "                                                background: white; padding: 1px;}\n",
       "                \n",
       "                table#t td {width: 1cm; height:1cm; text-align: center;}\n",
       "                table#t tr td.l {border-left: 2px solid red ;}\n",
       "                table#t tr td.r {border-right: 2px solid red ;}\n",
       "                table#t tr td.b {border-bottom: 2px solid red ;}\n",
       "                table#t tr td.t {border-top: 2px solid red ;}\n",
       "                table#t tr td.arret {background: yellow;}\n",
       "                </style>\n",
       "                <div class=\"cont\">\n",
       "                <table id=\"t\">\n",
       "                <tr><td class=\"l t arret\">üè≤üëΩüöñ</td><td class=\"r t \">:</td><td class=\"l t \">:</td><td class=\"t \">:</td><td class=\"r t arret\">:</td></tr><tr><td class=\"l \">:</td><td >:</td><td >:</td><td >:</td><td class=\"r \">:</td></tr><tr><td class=\"l \">:</td><td >:</td><td >:</td><td >:</td><td class=\"r \">:</td></tr><tr><td class=\"l r \">:</td><td class=\"l \">:</td><td class=\"r \">:</td><td class=\"l \">:</td><td class=\"r \">:</td></tr><tr><td class=\"l r b arret\">:</td><td class=\"l b \">:</td><td class=\"r b \">:</td><td class=\"l b arret\">:</td><td class=\"r b \">:</td></tr></table></div><div class=\"cont\"><p>Etape: 11</p><p>Etat: 16</p><p>Action: deposer</p><p>R√©compense: 19</p><p>R√©compense totale: 9</p></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tester apr√®s l'ex√©cution de la m√™me initialisation plusieurs fois\n",
    "for i in range(1000):\n",
    "    taxi.initialiser((3, 1), 2, 0)\n",
    "    taxi.transporter() \n",
    "\n",
    "taxi.initialiser((3, 1), 2, 0)\n",
    "hist = taxi.transporter(plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((3, 1, 2, 0), 'arriere', -1, False),\n",
       " ((2, 1, 2, 0), 'gauche', -1, False),\n",
       " ((2, 0, 2, 0), 'avant', -1, False),\n",
       " ((3, 0, 2, 0), 'avant', -1, False),\n",
       " ((4, 0, 2, 0), 'prendre', -1, False),\n",
       " ((4, 0, 4, 0), 'gauche', -1, False),\n",
       " ((4, 0, 4, 0), 'arriere', -1, False),\n",
       " ((3, 0, 4, 0), 'arriere', -1, False),\n",
       " ((2, 0, 4, 0), 'arriere', -1, False),\n",
       " ((1, 0, 4, 0), 'arriere', -1, False),\n",
       " ((0, 0, 4, 0), 'deposer', 19, True)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# historique des √©tapes\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fin\n"
     ]
    }
   ],
   "source": [
    "# tester l'appentissage avec des initialisations al√©atoires\n",
    "arrets2 = [(0,0), (0,4), (4,0), (4,3)]\n",
    "barrieres2 = {\n",
    "    (0, 1): set([1]), # barri√®re √† droit\n",
    "    (0, 2): set([0]), # barri√®re √† gauche\n",
    "    (3, 0): set([1]), # barri√®re √† droit\n",
    "    (4, 0): set([1]), # barri√®re √† droit\n",
    "    (3, 1): set([0]), # barri√®re √† gauche\n",
    "    (4, 1): set([0]), # barri√®re √† gauche\n",
    "    (3, 2): set([1]), # barri√®re √† droit\n",
    "    (4, 2): set([1]), # barri√®re √† droit\n",
    "    (3, 3): set([0]), # barri√®re √† gauche\n",
    "    (4, 3): set([0]), # barri√®re √† gauche\n",
    "}\n",
    "\n",
    "taxi2 = TaxiEnv(5, 5, arrets2, barrieres2)\n",
    "taxi2.ajouter_agent(0.1, 0.1)\n",
    "\n",
    "def exec_aleatoire(taxi_env, plot=False):\n",
    "    pos = np.random.randint(5), np.random.randint(5)\n",
    "    psg, dst = np.random.randint(len(arrets2)), np.random.randint(len(arrets2))\n",
    "    taxi_env.initialiser(pos, psg, dst)\n",
    "    return taxi_env.transporter(plot=plot) \n",
    "\n",
    "for i in range(10000):\n",
    "    exec_aleatoire(taxi2, plot=False)\n",
    "    \n",
    "print('fin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "                div.cont {display:inline-block; margin:5px; vertical-align: top;}\n",
       "                table#t, table#t td, table#t tr {border: 1px dotted black; \n",
       "                                                background: white; padding: 1px;}\n",
       "                \n",
       "                table#t td {width: 1cm; height:1cm; text-align: center;}\n",
       "                table#t tr td.l {border-left: 2px solid red ;}\n",
       "                table#t tr td.r {border-right: 2px solid red ;}\n",
       "                table#t tr td.b {border-bottom: 2px solid red ;}\n",
       "                table#t tr td.t {border-top: 2px solid red ;}\n",
       "                table#t tr td.arret {background: yellow;}\n",
       "                </style>\n",
       "                <div class=\"cont\">\n",
       "                <table id=\"t\">\n",
       "                <tr><td class=\"l t arret\">:</td><td class=\"r t \">:</td><td class=\"l t \">:</td><td class=\"t \">:</td><td class=\"r t arret\">üè≤üëΩüöñ</td></tr><tr><td class=\"l \">:</td><td >:</td><td >:</td><td >:</td><td class=\"r \">:</td></tr><tr><td class=\"l \">:</td><td >:</td><td >:</td><td >:</td><td class=\"r \">:</td></tr><tr><td class=\"l r \">:</td><td class=\"l \">:</td><td class=\"r \">:</td><td class=\"l \">:</td><td class=\"r \">:</td></tr><tr><td class=\"l r b arret\">:</td><td class=\"l b \">:</td><td class=\"r b \">:</td><td class=\"l b arret\">:</td><td class=\"r b \">:</td></tr></table></div><div class=\"cont\"><p>Etape: 14</p><p>Etat: 97</p><p>Action: deposer</p><p>R√©compense: 19</p><p>R√©compense totale: -14</p></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist = exec_aleatoire(taxi2, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Application et analyse\n",
    "\n",
    "Voici quelques outils pour tester l'apprentissage par renforcement :\n",
    "\n",
    "- OpenAI Baselines: https://github.com/openai/baselines\n",
    "- Intel Coach: https://github.com/IntelLabs/coach\n",
    "- Stable Baselines: https://github.com/DLR-RM/stable-baselines3\n",
    "- TF-Agents: https://github.com/tensorflow/agents\n",
    "- Keras-RL: https://github.com/keras-rl/keras-rl\n",
    "- Tensorforce: https://github.com/tensorforce/tensorforce\n",
    "- Chainer RL: https://github.com/chainer/chainerrl\n",
    "- Mushroom RL: https://github.com/MushroomRL/mushroom-rl\n",
    "- Acme: https://github.com/deepmind/acme\n",
    "- Dopamine: https://github.com/google/dopamine\n",
    "- RAY: https://github.com/ray-project/ray\n",
    "\n",
    "Environnements :\n",
    "\n",
    "- Gym: https://gym.openai.com/\n",
    "- iGibson: http://svl.stanford.edu/igibson/\n",
    "\n",
    "On va utiliser \"MushroomRL\" puisque l'outil impl√©mente les m√©thodes traditionnelles.\n",
    "Aussi, on va utiliser \"Gym\" pour g√©n√©rer les environnements. \n",
    "L'environnement Taxi sera utilis√© puisqu'il ne consomme pas beacoup de ressources (m√©moire et calcul).\n",
    "Vous pouvez consulter \"Gym\" pour des environements plus complexes comme les jeux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /opt/penv/ml3.8/lib/python3.8/site-packages (0.26.2)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /opt/penv/ml3.8/lib/python3.8/site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/penv/ml3.8/lib/python3.8/site-packages (from gym) (1.22.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/penv/ml3.8/lib/python3.8/site-packages (from gym) (2.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /opt/penv/ml3.8/lib/python3.8/site-packages (from gym) (4.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/penv/ml3.8/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym) (3.8.1)\n",
      "Collecting mushroom_rl\n",
      "  Using cached mushroom_rl-1.7.2-cp38-cp38-linux_x86_64.whl\n",
      "Requirement already satisfied: tqdm in /opt/penv/ml3.8/lib/python3.8/site-packages (from mushroom_rl) (4.64.1)\n",
      "Requirement already satisfied: pygame in /opt/penv/ml3.8/lib/python3.8/site-packages (from mushroom_rl) (2.1.2)\n",
      "Requirement already satisfied: opencv-python in /opt/penv/ml3.8/lib/python3.8/site-packages (from mushroom_rl) (4.7.0.68)\n",
      "Requirement already satisfied: numpy-ml in /opt/penv/ml3.8/lib/python3.8/site-packages (from mushroom_rl) (0.1.2)\n",
      "Requirement already satisfied: scipy in /opt/penv/ml3.8/lib/python3.8/site-packages (from mushroom_rl) (1.9.1)\n",
      "Requirement already satisfied: torch in /opt/penv/ml3.8/lib/python3.8/site-packages (from mushroom_rl) (1.13.1)\n",
      "Requirement already satisfied: joblib in /opt/penv/ml3.8/lib/python3.8/site-packages (from mushroom_rl) (1.2.0)\n",
      "Requirement already satisfied: pytest in /opt/penv/ml3.8/lib/python3.8/site-packages (from mushroom_rl) (7.2.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/penv/ml3.8/lib/python3.8/site-packages (from mushroom_rl) (1.1.2)\n",
      "Requirement already satisfied: Cython in /opt/penv/ml3.8/lib/python3.8/site-packages (from mushroom_rl) (0.29.33)\n",
      "Requirement already satisfied: matplotlib in /opt/penv/ml3.8/lib/python3.8/site-packages (from mushroom_rl) (3.6.0)\n",
      "Requirement already satisfied: numpy in /opt/penv/ml3.8/lib/python3.8/site-packages (from mushroom_rl) (1.22.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/penv/ml3.8/lib/python3.8/site-packages (from matplotlib->mushroom_rl) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/penv/ml3.8/lib/python3.8/site-packages (from matplotlib->mushroom_rl) (4.37.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/penv/ml3.8/lib/python3.8/site-packages (from matplotlib->mushroom_rl) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/penv/ml3.8/lib/python3.8/site-packages (from matplotlib->mushroom_rl) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/penv/ml3.8/lib/python3.8/site-packages (from matplotlib->mushroom_rl) (21.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/penv/ml3.8/lib/python3.8/site-packages (from matplotlib->mushroom_rl) (1.0.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/penv/ml3.8/lib/python3.8/site-packages (from matplotlib->mushroom_rl) (9.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/penv/ml3.8/lib/python3.8/site-packages (from matplotlib->mushroom_rl) (1.4.4)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/penv/ml3.8/lib/python3.8/site-packages (from pytest->mushroom_rl) (2.0.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /opt/penv/ml3.8/lib/python3.8/site-packages (from pytest->mushroom_rl) (1.0.0)\n",
      "Requirement already satisfied: iniconfig in /opt/penv/ml3.8/lib/python3.8/site-packages (from pytest->mushroom_rl) (2.0.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/penv/ml3.8/lib/python3.8/site-packages (from pytest->mushroom_rl) (22.1.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /opt/penv/ml3.8/lib/python3.8/site-packages (from pytest->mushroom_rl) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/penv/ml3.8/lib/python3.8/site-packages (from scikit-learn->mushroom_rl) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/penv/ml3.8/lib/python3.8/site-packages (from torch->mushroom_rl) (4.3.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/penv/ml3.8/lib/python3.8/site-packages (from torch->mushroom_rl) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/penv/ml3.8/lib/python3.8/site-packages (from torch->mushroom_rl) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/penv/ml3.8/lib/python3.8/site-packages (from torch->mushroom_rl) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/penv/ml3.8/lib/python3.8/site-packages (from torch->mushroom_rl) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /opt/penv/ml3.8/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->mushroom_rl) (56.0.0)\n",
      "Requirement already satisfied: wheel in /opt/penv/ml3.8/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->mushroom_rl) (0.37.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/penv/ml3.8/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->mushroom_rl) (1.16.0)\n",
      "Installing collected packages: mushroom_rl\n",
      "Successfully installed mushroom_rl-1.7.2\n"
     ]
    }
   ],
   "source": [
    "# installation des packages\n",
    "!pip install gym\n",
    "!pip install mushroom_rl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1. Exploration vs. Exploitation\n",
    "\n",
    "Ici, nous voulons tester l'effet de l'exploration/exploitation. \n",
    "Pour ce faire, nous allons tester avec des valeurs diff√©rentes de epsilon :\n",
    "- 0: exploitation (toujours)\n",
    "- 0.5: exploitation (50%) et exploration (50%)\n",
    "- 0.9: exploration (90%) et exploitation (10%).\n",
    "\n",
    "Le nombre max, par d√©faut, des √©tapes est fix√© √† 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/penv/ml3.8/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "  0%|                                                                       | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [20], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [collect_dataset]\n\u001b[1;32m     27\u001b[0m core \u001b[38;5;241m=\u001b[39m Core(agent, env, callbacks_fit\u001b[38;5;241m=\u001b[39mcallbacks)\n\u001b[0;32m---> 28\u001b[0m core\u001b[38;5;241m.\u001b[39mlearn(n_episodes\u001b[38;5;241m=\u001b[39mNBR_EPISODES, n_steps_per_fit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     30\u001b[0m res \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     31\u001b[0m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnbr_etapes\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m episodes_length(collect_dataset\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m/opt/penv/ml3.8/lib/python3.8/site-packages/mushroom_rl/core/core.py:75\u001b[0m, in \u001b[0;36mCore.learn\u001b[0;34m(self, n_steps, n_episodes, n_steps_per_fit, n_episodes_per_fit, render, quiet)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     fit_condition \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_episodes_counter\\\n\u001b[1;32m     73\u001b[0m                              \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_episodes_per_fit\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_condition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/penv/ml3.8/lib/python3.8/site-packages/mushroom_rl/core/core.py:125\u001b[0m, in \u001b[0;36mCore._run\u001b[0;34m(self, n_steps, n_episodes, fit_condition, render, quiet, initial_states)\u001b[0m\n\u001b[1;32m    120\u001b[0m     steps_progress_bar \u001b[38;5;241m=\u001b[39m tqdm(disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    121\u001b[0m     episodes_progress_bar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_episodes,\n\u001b[1;32m    122\u001b[0m                                  dynamic_ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, disable\u001b[38;5;241m=\u001b[39mquiet,\n\u001b[1;32m    123\u001b[0m                                  leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmove_condition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_condition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mepisodes_progress_bar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_states\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/penv/ml3.8/lib/python3.8/site-packages/mushroom_rl/core/core.py:141\u001b[0m, in \u001b[0;36mCore._run_impl\u001b[0;34m(self, move_condition, fit_condition, steps_progress_bar, episodes_progress_bar, render, initial_states)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m last:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset(initial_states)\n\u001b[0;32m--> 141\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_step([sample])\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_total_steps_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/penv/ml3.8/lib/python3.8/site-packages/mushroom_rl/core/core.py:189\u001b[0m, in \u001b[0;36mCore._step\u001b[0;34m(self, render)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03mSingle step.\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mdraw_action(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state)\n\u001b[0;32m--> 189\u001b[0m next_state, reward, absorbing, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmdp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_episode_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m render:\n",
      "File \u001b[0;32m/opt/penv/ml3.8/lib/python3.8/site-packages/mushroom_rl/environments/gym_env.py:95\u001b[0m, in \u001b[0;36mGym.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     94\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_action(action)\n\u001b[0;32m---> 95\u001b[0m     obs, reward, absorbing, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39matleast_1d(obs), reward, absorbing, info\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "from mushroom_rl.core import Environment\n",
    "from mushroom_rl.policy import EpsGreedy\n",
    "from mushroom_rl.algorithms.value import QLearning\n",
    "from mushroom_rl.utils.dataset import compute_J\n",
    "from mushroom_rl.utils.parameters import Parameter\n",
    "from mushroom_rl.core import Core\n",
    "from mushroom_rl.utils.callbacks import CollectDataset\n",
    "from mushroom_rl.utils.callbacks.callback import Callback\n",
    "from mushroom_rl.utils.dataset import parse_dataset, episodes_length\n",
    "\n",
    "NBR_EPISODES = 1000 # nombre des ex√©cutions\n",
    "\n",
    "env = Environment.make('Gym', 'Taxi-v3')\n",
    "\n",
    "epsilons = [.0, .5, .9]\n",
    "\n",
    "tests = []\n",
    "\n",
    "for eps in epsilons:\n",
    "    epsilon = Parameter(value=eps)\n",
    "    pi = EpsGreedy(epsilon=epsilon)\n",
    "    agent = QLearning(env.info, pi, learning_rate=Parameter(value=.3))\n",
    "    \n",
    "    collect_dataset = CollectDataset()\n",
    "    callbacks = [collect_dataset]\n",
    "    \n",
    "    core = Core(agent, env, callbacks_fit=callbacks)\n",
    "    core.learn(n_episodes=NBR_EPISODES, n_steps_per_fit=1)\n",
    "    \n",
    "    res = {}\n",
    "    res['nbr_etapes'] = episodes_length(collect_dataset.get())\n",
    "    tests.append(res)\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "for i, test in enumerate(tests):\n",
    "    plt.plot(test['nbr_etapes'], label='epsilon=' + str(epsilons[i]))\n",
    "plt.xlabel('Episodes') \n",
    "plt.ylabel('Nombre des etapes') \n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Analyser les r√©sultats** \n",
    "\n",
    "- Pourquoi l'algorithme avec plus d'exploration ne peut pas minimiser le nombre des √©tapes apr√®s plusieurs √©pisodes ?\n",
    "- Pourquoi il existe des √©pisodes qui ont un nombre minimal des √©tapes surtout dans les derni√®res √©pisodes (toujours dans l'algorithme avec plus d'exploration) ?\n",
    "- Pourquoi celui ui utilise seulement l'exploitation prend moins d'√©tapes √† chaque √©pisode ?\n",
    "- Dans ce cas, quel est l'inter√™t de l'exploration ?\n",
    "\n",
    "**R√©ponse**\n",
    "\n",
    "- ...\n",
    "- ...\n",
    "- ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### II.2. Taux d'apprentissage\n",
    "\n",
    "Ici, nous voulons tester l'effet du taux d'apprentissage sur le nombre des it√©rations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBR_EPISODES = 1000 # nombre des ex√©cutions\n",
    "\n",
    "env = Environment.make('Gym', 'Taxi-v3')\n",
    "\n",
    "lrs = [.1, .2, .3]\n",
    "\n",
    "tests = []\n",
    "\n",
    "for lr in lrs:\n",
    "    epsilon = Parameter(value=0.1)\n",
    "    pi = EpsGreedy(epsilon=epsilon)\n",
    "    agent = QLearning(env.info, pi, learning_rate=Parameter(value=lr))\n",
    "    \n",
    "    collect_dataset = CollectDataset()\n",
    "    callbacks = [collect_dataset]\n",
    "    \n",
    "    core = Core(agent, env, callbacks_fit=callbacks)\n",
    "    core.learn(n_episodes=NBR_EPISODES, n_steps_per_fit=1)\n",
    "    \n",
    "    res = {}\n",
    "    res['nbr_etapes'] = episodes_length(collect_dataset.get())\n",
    "    tests.append(res)\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "for i, test in enumerate(tests):\n",
    "    plt.plot(test['nbr_etapes'], label='Taux=' + str(lrs[i]))\n",
    "plt.xlabel('Episodes') \n",
    "plt.ylabel('Nombre des etapes') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Analyser les r√©sultats** \n",
    "\n",
    "- Quel est l'effet de $\\alpha$ sur le nombre des √©tapes apr√®s chaque $n$ √©pisodes ?\n",
    "- En consultant ce diagramme et celui avant, nous pouvons dire que l'√©volution avec $\\epsilon=0$ est presque comme celle avec $\\alpha=0.3$ et l'√©volution avec $\\epsilon=0.5$ est presque comme celle avec $\\alpha=0.1$. Dans ce cas, est ce que nous pouvons dire qu'il y ait une relation directe entre les deux param√®tres (un peut etre remplac√© avec une fonction sur l'autre) ? Pourquoi ?\n",
    "\n",
    "**R√©ponse**\n",
    "\n",
    "- ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
