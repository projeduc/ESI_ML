% !TEX TS-program = pdflatex
% !TeX program = pdflatex
% !TEX encoding = UTF-8
% !TEX spellcheck = en_US

\documentclass[xcolor=table]{beamer}

\usepackage{../extra/beamer/karimml}

\input{options}

\subtitle[DT \& Ensemble]{Decision trees and Ensemble learning}

\changegraphpath{../img/arbres/}

\begin{document}
	
\begin{frame}
	\frametitle{\inserttitle}
	\framesubtitle{\insertshortsubtitle: Introduction}
	
	\begin{itemize}
		\item Decision trees
		\begin{itemize}
			\item how to create a decision tree using ID3
			\item how to create a decision tree using CART
		\end{itemize}
		\item Random Forests
		\begin{itemize}
			\item Ensemble learning
			\item How a random forest works
		\end{itemize}
	\end{itemize}

\end{frame}


\begin{frame}
	\frametitle{\inserttitle}
	\framesubtitle{\insertshortsubtitle: Plan}
	
	\begin{multicols}{2}
		%	\small
		\tableofcontents
	\end{multicols}
\end{frame}

\section{Decision trees}

\begin{frame}
	\frametitle{\insertsection}
	
	\begin{itemize}
		\item \optword{ID3 (Iterative Dichotomiser 3)}: developed in 1986 by Ross Quinlan. It can be applied only to nominal characteristics. It is used for ranking.
		\item \optword{C4.5}: an extension of ID3 by Ross Quinlan. It can be applied on all types of features. It is used for ranking.
		\item \optword{C5.0}: a commercial extension of C4.5, again by Ross Quinlan.
		\item \optword{CART (Classification and Regression Trees)}: like C4.5 but uses other metrics. Also, the algorithm supports regression.
	\end{itemize}
	
\end{frame}

\subsection{Algorithms}

\begin{frame}
	\frametitle{\insertsection}
	\framesubtitle{\insertsubsection: Construction (Training)}
	
	\begin{algorithm}[H]\scriptsize
		\SetKwFunction{FConst}{build}
		\SetKwProg{Fn}{function}{}{end}
		
		\KwData{$ X, Y$}
		\KwResult{A decision tree} 
		\Return \FConst{$X, Y$};\tcp{return the tree's root}
		
		\Fn{\FConst{$X', Y'$}}{
			n $\leftarrow$ new Node()\;
			\eIf{$Y'$ is homogeneous or stop criteria is reached}{
				$s.class \leftarrow \arg\max_{k}{|\{y \in Y' / y = k \}|}$ \tcp{$n$ is a leaf}
			}{
				determine the feature $j$ of $ X' $ which better divides $Y'$\;
				split $(X, Y)$ into $(X_1, Y_1), \ldots, (X_K, Y_K)$ based on the $ K $ values of $X'_j$\;
				$n.feature = j$\;
				\lForEach{$ k \in \{1, \ldots, K\}$}{
					$ n.children[k] \leftarrow$ \FConst{$ X_k, Y_k $})
				}
			}
			
			\Return n\;
		}
		\caption{Generating a decision tree (General algorithm)}
	\end{algorithm}
	
\end{frame}

\begin{frame}
	\frametitle{\insertsection}
	\framesubtitle{\insertsubsection: Search (prediction)}
	
	\begin{algorithm}[H]\scriptsize
		\SetKwFunction{FSearch}{Search}
		\SetKwProg{Fn}{function}{}{end}
		
		\KwData{$x$: a sample; $T = (V, E)$: a decision tree}
		\KwResult{$ y $: the result class} 
		
		Let $n_{root}$ be the root node of $T$\;
		\Return \FSearch($n_{root};\ x$)\;
		
	
		\Fn{\FSearch{$n$: a node; $x$: a sample}}{
			
			\eIf{$n$ is a leaf \tcp{it has no children}}{
				\Return $n.class$\;
			}{
				$k = x[n.feature]$\; 
				\Return \FSearch($n.children[k];\ x$)\;
			}
			
			\Return n\;
		}
	
		\caption{Traversing a decision tree (general algorithm)}
	\end{algorithm}
	
\end{frame}

\subsection{Stop conditions}

\begin{frame}
	\frametitle{\insertsection}
	\framesubtitle{\insertsubsection}
	
	\begin{itemize}
		\item \optword{Homogeneity}: the observations in the node have the same class;
		\item \optword{Minimum impurity}: the impurity or classification/regression error of observations in the node is less than or equal to this threshold;
		\item \optword{Minimum number of observations}: the number of observations in a node is less than or equal to this threshold;
		\item \optword{Depth}: the depth of the node in the tree is less than this threshold.
	\end{itemize}
	
\end{frame}

\subsection{Review}

\begin{frame}
	\frametitle{\insertsection}
	\framesubtitle{\insertsubsection: Advantages}
	
	\begin{itemize}
		\item simple to understand and interpret. We can visualize the trees. Also, we can easily explain the obtained results.
		\item can work on data with little preparation. For example, they do not need data normalization.
		\item accept numeric and nominal data. Other learning algorithms are specialized in a single type of data.
		\item perform well even if their assumptions are somewhat violated by the actual model from which the data was generated.
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{\insertsection}
	\framesubtitle{\insertsubsection: Limits}
	
	\begin{itemize}
		\item can be too complex to not generalize well (overfitting). This can be adjusted by setting the minimum number of samples in the leaves or by setting the maximum depth of the tree.
		\item may be unstable due to data variations.
		\item there are problems that are a bit difficult to learn by decision trees. They are not easy to express, for example: XOR.
		\item may be biased to the dominant class. So, you have to balance the data before training the system.
		\item it is not guaranteed to fall on the optimal decision tree.
	\end{itemize}
	
\end{frame}

\section{ID3}

\begin{frame}
	\frametitle{\insertsection}
	
	\begin{itemize}
		\item \keyword{Iterative Dichotomize 3};
		\item only accepts nominal characteristics;
		\item only for classification (no regression);
		\item the training stops if the sets of classes in the leafs are homogeneous.
	\end{itemize}
	
\end{frame}

\subsection{Homogeneity of a set}

\begin{frame}
	\frametitle{\insertsection}
	\framesubtitle{\insertsubsection}
	
	\begin{itemize}
		\item Shannon's entropy $H(Y)$ to measure the uncertainty of a set $Y$;
		\item $H(Y)=0$: $Y$ contains the same values (classes);
		\item $H(Y)=1$: $Y$ contains totally different values;
		\item given $V_y$ the vocabulary set of $Y$ (unique values).
	\end{itemize}

	\[H(Y) = - \sum\limits_{v \in V_y} p(v/Y) \log_2 p(v/Y)\]
	
	\[p(v/Y) = \frac{|\{y / y \in Y \wedge y = v\}|}{|S|}\]
	
\end{frame}

\subsection{Set's split}

\begin{frame}
	\frametitle{\insertsection}
	\framesubtitle{\insertsubsection}
	
	\begin{itemize}
		\item $Y$: a set of predictions;
		\item $X_j$: the values of the feature $j$;
		\item $v$: one value out of the possible values of $X_j$ (vocabulary $V_j$);
		\item for each value $v \in V_j$, we create a set $Y_{j, v}$;
		\item if $|V_j| = K$, we will have $K$ sets $Y_1, \cdots, Y_K$;
	\end{itemize}
	
	\[Y_{j,v} = \{y^{(i)} \in Y / X_j^{(i)} \in X_j \wedge X_j^{(i)} = v\}\]

	
\end{frame}

\subsection{Choice of split feature}

\begin{frame}
	\frametitle{\insertsection}
	\framesubtitle{\insertsubsection}
	
	\begin{itemize}
		\item Entropy gain (information gain) $IG(Y, X_j)$ is used;
		\item $j = \arg\max_{j'} IG(Y, X_{j'})$;
		\item This measure calculates how much uncertainty in $Y$ was reduced after it was split using the feature $j$;
		\item $ IG $ is the difference between the entropy of $Y$ and the weighted entropy of the split sets;
	\end{itemize}

	\[IG(Y, X_j) = H(Y) - \sum_{v \in V_j} p(v/X_j) H(Y_{j, v})\]
	
	
\end{frame}

\section{CART}

\begin{frame}
	\frametitle{\insertsection}
	
	\begin{itemize}
		\item \keyword{Classification and Regression Trees};
		\item supports regression;
		\item tries to minimize a cost function;
		\item uses pre-pruning based on a stopping criterion;
		\item creates binary trees.
	\end{itemize}
	
\end{frame}

\subsection{Homogeneity of a set}

\begin{frame}
	\frametitle{\insertsection}
	\framesubtitle{\insertsubsection}
	
	\begin{itemize}
		\item diversity index $Gini(Y)$ to measure the classification error of $Y$;
		\item $Gini(Y)=0$ represents the best division;
		\item $Gini(Y)=0.5$ represents the worst division;
		\item given $V_y$ the vocabulary of $Y$ (unique values or classes);
		\item In the case of regression, \keyword{MSE} is used;
	\end{itemize}
	
	\[Gini(S) = \sum\limits_{v \in V_y} p(v/Y) (1-p(v/Y)) = 1 - \sum\limits_{v \in V_y} p(v/Y)^2 \]
	
	\[p(v/Y) = \frac{|\{y / y \in Y \wedge y = v\}|}{|S|}\]
	
\end{frame}

\subsection{Set's split}

\begin{frame}
	\frametitle{\insertsection}
	\framesubtitle{\insertsubsection}
	
	\begin{itemize}
		\item $Y$: a set of predictions;
		\item $X_j$: the values of the feature $j$;
		\item $v$: a value out of the possible values of $X_j$ (vocabulary $V_j$)
		\item for each value $v \in V_j$, two sets ($Y_G$ and $Y_D$) are created;
		\item $Y_G$ (with values $X_j > v$) and $Y_D$ (with values $X_j \le v$);
	\end{itemize}
	
	\[Y_G = \{y^{(i)} \in Y / X_j^{(i)} \in X_j \wedge X_j^{(i)} > v\}\]
	\[Y_D = \{y^{(i)} \in Y / X_j^{(i)} \in X_j \wedge X_j^{(i)} \le v\}\]
	
\end{frame}

\subsection{Choice of split feature}

\begin{frame}
	\frametitle{\insertsection}
	\framesubtitle{\insertsubsection}
	
	\begin{itemize}
		\item Gini diversity of the division $Gini_{div}(Y_G, Y_D)$ is used;
		\item $j, v = \arg\min_{j', v'} Gini_{div}(Y_G, Y_D)$;
		\item Here, not only the split feature $j$ is explored, but also the value $v$ which minimizes the Gini diversity of the division;
		\item $|S| = |S_G| + |S_D|$;
		\item for regression, \keyword{MSE} is used instead of \keyword{Gini};
		\item the estimate in the case of regression is the average of the leaf's $Y$;
	\end{itemize}
	
	\[Gini_{div}(S_G, S_D) = \frac{|S_G|}{|S|} Gini(S_G) + \frac{|S_D|}{|S|} Gini(S_D)\]
	
	
\end{frame}

\section{Random forests}

\begin{frame}
	\frametitle{\insertsection}
	
	\begin{itemize}
		\item ensemble method;
		\item uses decision trees for estimation;
		\item final estimation is done by majority vote;
		\item uses Bootstraps (sets of random observations) for tree learning;
		\item uses random features to train each tree: by using fewer attributes in each tree, overfitting issues can be prevented.
	\end{itemize}
	
\end{frame}

\subsection{Ensemble learning}

\begin{frame}
	\frametitle{\insertsection}
	\framesubtitle{\insertsubsection}
	
	\begin{itemize}
		\item \optword{Bootstrap aggregating (Bagging)}
		\begin{itemize}
			\item Concurrently train different estimators on random data subsets.
		\end{itemize}
		\item \optword{Boosting}
		\begin{itemize}
			\item Sequentially train estimators on the same data; each one improves the performance of the others.
		\end{itemize}
		\item \optword{Stacking}
		\begin{itemize}
			\item Concurrently train estimators on the same data; Train an estimator that combines the predictions of the other estimators.
		\end{itemize}
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{\insertsection}
	\framesubtitle{\insertsubsection: Bootstrap aggregating (Bagging)}
	
	\begin{itemize}
		\item \textbf{Training}
		\begin{itemize}
			\item Create \textbf{K} Bootstraps (random sets) from the training dataset
			\item A bootstrap can have a subset of features
			\item Train a model for each Bootstrap
		\end{itemize}
		\item \textbf{Estimation}
		\begin{itemize}
			\item Use the models to obtain \textbf{K} estimations
			\item Final estimation (Classification): by majority vote
			\item Final estimation (Regression): by mathematical average
		\end{itemize}
		\item \textbf{Examples}
		\begin{itemize}
			\item \expword{Random Forests}
		\end{itemize}
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{\insertsection}
	\framesubtitle{\insertsubsection: Boosting}
	
	\begin{itemize}
		\item \textbf{Training}
		\begin{itemize}
			\item Train an estimator on the training dataset
			\item Make predictions on this dataset to extract samples that are not well-estimated
			\item Train another estimator on the same dataset but with more weight on poorly estimated samples
			\item Repeat the same operation until generating \textbf{K} estimators
		\end{itemize}
		\item \textbf{Estimation}
		\begin{itemize}
			\item Use the models to obtain \textbf{K} estimations
			\item Final estimation (Classification): by majority vote
			\item Final estimation (Regression): by mathematical average
			\item \textbf{Examples}
			\begin{itemize}
				\item \expword{AdaBoost}
			\end{itemize}
		\end{itemize}
	\end{itemize}
	
\end{frame}


\begin{frame}
	\frametitle{\insertsection}
	\framesubtitle{\insertsubsection: Stacking}
	
	\begin{itemize}
		\item \textbf{Training}
		\begin{itemize}
			\item Train \textbf{K} estimators on the same training dataset
			\item These estimators should have different hyperparameters or different training algorithms
			\item Train an estimator that takes the outputs of the other \textbf{K} estimators as input
			\item This final estimator will learn to merge the estimations of the other estimators to produce a better one
		\end{itemize}
		\item \textbf{Estimation}
		\begin{itemize}
			\item Use the \textbf{K} estimators to obtain initial estimations
			\item Use the final estimator to combine these estimations
		\end{itemize}
	\end{itemize}
	
\end{frame}

\subsection{Parameters of a Forest}

\begin{frame}
	\frametitle{\insertsection}
	\framesubtitle{\insertsubsection}
	
	\begin{itemize}
		\item \optword{Bootstrap}
		\begin{itemize}
			\item Do we use the dataset as it is, or do we use \keyword{bootstrapping}?
			\item What is the size of a Bootstrap?
			\item How much randomness do we use to generate the Bootstrap?
		\end{itemize}
		\item \optword{Feature}
		\begin{itemize}
			\item How many features should we use per tree?
			\item How do we choose these features (amount of randomness)?
		\end{itemize}
		\item \optword{Tree}
		\begin{itemize}
			\item How many trees do we use for estimation?
			\item Plus other parameters related to the trees
		\end{itemize}
	\end{itemize}
	
\end{frame}


%\insertbibliography{ML_Lab-DecisionTrees-Ensemble}{*}

\end{document}



