% !TEX TS-program = pdflatex
% !TeX program = pdflatex
% !TEX encoding = UTF-8
% !TeX spellcheck = en_US

\documentclass[xcolor=table]{beamer}%, aspectratio = 169

\usepackage{../extra/beamer/karimml}

\input{options}

\usepackage{cancel}

\subtitle[Regularization and feature selection]{Regularization and feature selection} 

\changegraphpath{../img/regsel/}

\begin{document}
	
\begin{frame}
	\frametitle{\inserttitle}
	\framesubtitle{\insertshortsubtitle: Introduction}
	
	\begin{itemize}
		\item You saw in the lecture ...
		\begin{itemize}
			\item the overfitting problem;
			\item regularization by penalty (L2 as an example);
			\item the different approaches to feature selection.
		\end{itemize}
		\item In this lab support, we will present ...
		\begin{itemize}
			\item some regularization approaches;
			\item three penalty regularization techniques;
			\item a reminder about feature selection approaches
			\item detailing the techniques of each approach (with example from scikit-learn)
		\end{itemize}
	\end{itemize}
	
\end{frame}


\begin{frame}
	\frametitle{\inserttitle}
	\framesubtitle{\insertshortsubtitle: Plan}
	
	\begin{multicols}{2}
		%	\small
		\tableofcontents
	\end{multicols}

\end{frame}

\section{Regularization}

\begin{frame}
	\frametitle{\insertshortsubtitle}
	\framesubtitle{\insertsection}

	\begin{itemize}
		\item used to reduce overfitting
		\item can cause faster convergence
		\item \optword{By augmentation}: add more data
		\begin{itemize}
			\item automatic data generation
			\item Ex. \expword{Generate new images by rotation}
		\end{itemize}
		\item \optword{Early stopping}: use validation in the stopping decision
		\begin{itemize}
			\item train on one dataset and use another for validation in each iteration
			\item when the validation error increases, stop
		\end{itemize}
		\item \optword{By penalty}: add a penalty to the objective function
		\begin{itemize}
			\item reduce model complexity
			\item by adding another constraint on the parameters (penalty)
		\end{itemize}
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{\insertshortsubtitle}
	\framesubtitle{\insertsection: By penalty}
	
	\[
	\begin{cases}
	\min J_\text{cost}(\theta) & \\
	\wedge  & \Rightarrow \min J_\text{cost}(\theta) + J_\text{complexity}(\theta)\\
	\min J_\text{complexity}(\theta) \\
	\end{cases}
	\]

	\begin{itemize}
		\item minimize the complexity by increasing the bias: $\theta_0$ has no constraint, so the model gives it more importance during training
		\item the complexity penalty uses a hyper-parameter $\lambda$
		\begin{itemize}
			\item if $\lambda$ is too big, the model will be too simple (not dependent on attributes). So, \keyword{underfitting}.
			\item if $\lambda$ is too small, the model will be too complex (too dependent on attributes). So, \keyword{overfitting}.
		\end{itemize}
	\end{itemize}
	
\end{frame}

\subsection{L2 Loss}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection: Tikhonov regularization, L2 loss}
	
	\begin{columns}
		\begin{column}{.64\textwidth}
			\[J_{L2} = \frac{\lambda}{2M} \sum_{j=1}^{N} \theta_j^2\]
	
			\begin{itemize}
				\item $\theta_0$ is not affected by regularization
				\item $\theta_j$ converges to $0$, but does not equal $0$
				\item $\lambda \rightarrow \infty \Rightarrow \theta_j \rightarrow 0$
				\item $L2$ tries to pull the values of $\theta$ inside a sphere
				\item the size of the sphere is inversely proportional to $\lambda$
			\end{itemize}
		\end{column}
		\begin{column}{.35\textwidth}
			\hgraphpage{L1L2comp.png}
		\end{column}
	\end{columns}
	
\end{frame}

\subsection{L1 Loss}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection: L1 loss}
	
	\begin{columns}
		\begin{column}{.64\textwidth}
			\[J_{L1} = \frac{\lambda}{M} \sum_{j=1}^{N} |\theta_j|\]
	
			\begin{itemize}
				\item least absolute shrinkage and selection operator
				\item $\theta_0$ is not affected by regularization
				\item $\theta_j$ is canceled after a few iterations (it will last depending on the importance of its attribute)
				\item $it \rightarrow \infty \Rightarrow \theta_j = 0$
				\item $L1$ tries to pull the values of $\theta$ inside a cube
				\item the size of the cube is inversely proportional to $\lambda$
			\end{itemize}
		\end{column}
		\begin{column}{.35\textwidth}
			\hgraphpage{L1L2comp.png}
		\end{column}
	\end{columns}
	
\end{frame}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection: Optimization}

	\begin{itemize}
		\item L1 is not differential in $0$
		\item Several techniques have been proposed to find the solution path $L1$
		\begin{itemize}
			\item Subgradient methods; ex. \expword{least-angle regression (LARS)}
			\item Coordinate descent
			\item Proximal gradient methods (by operator \keyword{soft thresholding})
		\end{itemize}
	\end{itemize}

	\[S_\frac{\lambda}{M} (\theta_j) = 
	\begin{cases}
		\theta - \frac{\lambda}{M} & \theta_j > \frac{\lambda}{M} \\
		0 & \theta_j \in [-\frac{\lambda}{M}, \frac{\lambda}{M}] \\
		\theta + \frac{\lambda}{M} & \theta_j < -\frac{\lambda}{M} \\
	\end{cases}
	\]
	
	\begin{itemize}
		\item In case of the proximal gradient, the parameters are updated as follows: 
		\item $\theta = S_\frac{\lambda}{M} (\theta - \alpha \frac{\partial}{\partial \theta} J_\text{cost}(\theta)) $
	\end{itemize}
	
\end{frame}

\subsection{ElasticNet}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection}
	
	\[J_{EN} = \frac{\lambda}{M} \sum_{j=1}^{N} \left( r  |\theta_j| + \frac{(r-1)}{2} \theta_j^2\right)\]
	

	\begin{itemize}
		\item $\theta_0$ is not affected by regularization
		\item $r \in [0, 1]$ controls the percentage of $L1$ regularization
	\end{itemize}
	
\end{frame}

\section{Feature selection}

\begin{frame}
	\frametitle{\insertshortsubtitle}
	\framesubtitle{\insertsection}
	
	\begin{itemize}
		\item can reduce overfitting: the model can overfit over noise features 
		\item can improve system accuracy
		\item reduce training time
		\item \optword{Filter}: independently of the estimator, the best features are selected based on univariate statistical tests
		\item \optword{Embedded}: the best features are selected during training
		\item \optword{Wrapper}: the best features are selected for a given estimator before training
	\end{itemize}
	
\end{frame}

\subsection{Filter}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection: Score}
	
	\begin{tabular}{p{0.2\textwidth}p{0.3\textwidth}p{0.3\textwidth}}
%	\hline
	& \multicolumn{2}{c}{Input} \\
	\cline{2-3}
	Output	& Numerical & Categorical \\
	\hline
	Numerical \linebreak(Regression) & Pearson & Mutual information \\
	\hline
	Categorical \linebreak(Classification) & ANOVA, \linebreak Mutual information & Chi-2, \linebreak Mutual information\\
	\hline
	\end{tabular}

	\begin{itemize}
		\item \optword{Pearson}: \expword{sklearn.feature\_selection.f\_regression}
		\item \optword{ANOVA}: \expword{sklearn.feature\_selection.f\_classif}
		\item \optword{Chi2}: \expword{sklearn.feature\_selection.chi2}
		\item \optword{Mutual information}: \expword{feature\_selection.mutual\_info\_classif}
		\item \optword{Mutual information}: \expword{feature\_selection.mutual\_info\_regression}
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection: Selection}
	
	\begin{itemize}
		\item Based on previous scores, the most important features can be selected in several ways
		\item \optword{Number}: number of features to select
		\begin{itemize}
			\item \expword{sklearn.feature\_selection.SelectKBest}
		\end{itemize}
		\item \optword{Percentile}: percentile of highest scores.
		\begin{itemize}
			\item \expword{sklearn.feature\_selection.SelectPercentile}
		\end{itemize}
		\item \optword{P-value}: max threshold of accepted p-values
		\begin{itemize}
			\item \expword{sklearn.feature\_selection.SelectFpr}
		\end{itemize}
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection: One-Way ANOVA (logic)}
	
	\begin{itemize}
		\item Given a feature $A$ ($M$ samples) with numerical values
		\item the values are split on sets $A_j$ where $j$ is a class among $N$ classes. We call them \keyword{Treatments}
		\item an attribute is representative (well correlated) of the output classes, if ...
		\begin{itemize}
			\item values of the same class have less variance (intra-class variance)
			\item class values have more variance with the rest (inter-class variance)
			\item \textbf{SO}, the ratio (inter-class variance)/(intra-class variance) must be large
			\item We call this ratio: \keyword{F-value} of ANOVA
		\end{itemize}
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection: One-Way ANOVA (math)}
	
	\begin{itemize}
		\item Correlation factor: $CF = \frac{(\sum_{ij} A_{ij})^2}{M} $
		\item Total Sum of Squares: $TotalSS = \sum_{ij} A_{ij}^2 - CF$
		\item Treatment Sum of Squares: $TreatmentSS = \sum_j \frac{(\sum_{i} A_{ij})^2}{|A_j|} - CF$
		\item Error Sum of Squares: $ErrorSS = TotalSS - TreatmentSS$
		\item Mean of Squares Between: $MSB = \frac{TreatmentSS}{(N - 1)}$
		\item Mean of Squares Within: $MSW =  \frac{ErrorSS}{(M - N)}$
		\item $Fvalue = \frac{MSB}{MSW}$
	\end{itemize}
	
\end{frame}

\subsection{Embedded}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection}
	
	\begin{itemize}
		\item \optword{Decision trees}
		\begin{itemize}
			\item In each node, the best split feature is chosen.
			\item Certain features will not be considered.
			\item Finally, the tree will only use features with high separation capacity.
		\end{itemize}
		\item \optword{L1 regularization}
		\begin{itemize}
			\item It forces parameters to have small values.
			\item In L1, certain parameters are set to 0 after convergence.
			\item In this case, during the estimation the model does not take into account features with parameters of zero.
		\end{itemize}
	\end{itemize}
	
\end{frame}

\subsection{Wrapper}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection}

	\begin{itemize}
		\item use an estimator and try to find feature combination giving more performance
		\expword{sklearn.feature\_selection.SequentialFeatureSelector}
		\item \optword{Ascending (Forward selection)}
		\begin{itemize}
			\item the initial set of features is empty
			\item start by selecting a single feature that maximizes cross-validation
			\item add more features in the same way until reaching the desired number
			\item \expword{SequentialFeatureSelector(direction="forward")}
		\end{itemize}
		\item \optword{Backward elimination}
		\begin{itemize}
			\item the initial set of features equals to $N$
			\item start by eliminating a single feature which minimizes cross-validation
			\item eliminate more features in the same way until reaching the desired number
			\item \expword{SequentialFeatureSelector(direction="backward")}
		\end{itemize}
	\end{itemize}
	
\end{frame}


\begin{frame}[plain]
	
	\huge 
	\[
	\hspace{-4cm}\text{The end of the } \cancelto{\text{presentation}}{\text{world}}
	\]
	
\end{frame}

\begin{frame}[plain]
	
	\begin{center}
		\Huge 
		Stop scrolling
		
		... 
		
		It is the end!
	\end{center}
	
\end{frame}



%\insertbibliography{ML-preparation}{*}

\end{document}



