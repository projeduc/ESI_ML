% !TEX TS-program = pdflatex
% !TeX program = pdflatex
% !TEX encoding = UTF-8
% !TeX spellcheck = en_US

\documentclass[xcolor=table]{beamer}

\usepackage{../extra/beamer/karimml}

\input{options}

\usepackage{cancel}

\subtitle[Regularization and feature selection] %
{Regularization and feature selection} 

\changegraphpath{../img/regsel/}

\begin{document}
	
\begin{frame}
	\frametitle{Machine Learning}
	\framesubtitle{Régularisation et sélection d'attributs  (support TP) : Introduction}
	
	\begin{itemize}
		\item Vous avez vu en cours ...
		\begin{itemize}
			\item le problème de sur-apprentissage ;
			\item la régularisation par pénalité (L2 comme exemple) ;
			\item les différentes approches de sélection d'attributs.
		\end{itemize}
		\item Dans ce support de TP, on va présenter ...
		\begin{itemize}
			\item quelques approches de régularisation ;
			\item trois techniques de régularisation par pénalité ;
			\item un rappel sur les approches de sélection d'attributs
			\item du détail des techniques de chaque approche (avec exemple de scikit-learn)
		\end{itemize}
	\end{itemize}
\end{frame}


\begin{frame}
	\frametitle{Machine Learning}
	\framesubtitle{Régularisation et sélection d'attributs (support TP) : Plan}
	
	\begin{multicols}{2}
		%	\small
		\tableofcontents
	\end{multicols}
\end{frame}

\section{Régularisation}

\begin{frame}
	\frametitle{Régularisation}
	
	\begin{itemize}
		\item utilisée pour réduire le sur-apprentissage 
		\item peut causer une convergence plus rapide
		\item \optword{Par augmentation} : ajouter plus de données
		\begin{itemize}
			\item génération automatique des données 
			\item Ex. \expword{Générer de nouvelles images par rotation}
		\end{itemize}
		\item \optword{Early stopping} : utiliser la validation dans la décision d'arrêt
		\begin{itemize}
			\item entrainer sur un dataset et utiliser un autre pour la validation dans chaque itération
			\item lorsque l'erreur de validation augmente, on s'arrête
		\end{itemize}
		\item \optword{Par pénalité} : ajouter une pénalité à la fonction objective
		\begin{itemize}
			\item réduire la complexité du modèle
			\item en ajoutant une autre contrainte sur les paramètres (pénalité)
		\end{itemize}
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Régularisation (par pénalité)}
	
	\[
	\begin{cases}
	\min J_\text{erreur}(\theta) & \\
	\wedge  & \Rightarrow \min J_\text{erreur}(\theta) + J_\text{complexité}(\theta)\\
	\min J_\text{complexité}(\theta) \\
	\end{cases}
	\]
	
	\begin{itemize}
		\item minimiser la complexité par augmentation du biais : $\theta_0$ n'a pas de contrainte, donc le modèle lui donne plus d'importance lors de l'entrainement
		\item la pénalité sur la complexité utilise un hyper-paramètre $\lambda$
		\begin{itemize}
			\item si $\lambda$ est trop grand, le modèle sera trop simple (non dépendant aux attributs). Donc, \keyword{sous-apprentissage}.
			\item si $\lambda$ est trop petit, le modèle sera trop complexe (trop dépendant aux attributs). Donc, \keyword{sur-apprentissage}.
		\end{itemize}
	\end{itemize}
	
\end{frame}

\subsection{Ridge}

\begin{frame}
	\frametitle{Régularisation et sélection d'attributs (support TP)}
	\framesubtitle{Ridge (Régularisation de Tikhonov, L2 loss)}
	
	\begin{columns}
		\begin{column}{.6\textwidth}
			\[J_{L2} = \frac{\lambda}{2M} \sum_{j=1}^{N} \theta_j^2\]
	
			\begin{itemize}
				\item $\theta_0$ n'est pas concerné par la régularisation
				\item $\theta_j$ converge vers $0$, mais n'égale pas $0$
				\item $\lambda \rightarrow \infty \Rightarrow \theta_j \rightarrow 0$
				\item $L2$ essaye de tirer les valeurs des $\theta$ à l'intérieur d'un sphère
				\item la taille du sphère est inversement proportionnelle à $\lambda$
			\end{itemize}
		\end{column}
		\begin{column}{.4\textwidth}
			\hgraphpage{L1L2comp.png}
		\end{column}
	\end{columns}
	
\end{frame}

\subsection{Lasso}

\begin{frame}
	\frametitle{Régularisation et sélection d'attributs (support TP)}
	\framesubtitle{Lasso (L1 loss)}
	
	\begin{columns}
		\begin{column}{.6\textwidth}
			\[J_{L1} = \frac{\lambda}{M} \sum_{j=1}^{N} |\theta_j|\]
	
			\begin{itemize}
				\item least absolute shrinkage and selection operator
				\item $\theta_0$ n'est pas concerné par la régularisation
				\item $\theta_j$ s'annule après quelques itérations (il va durer selon l'importance de son attribut)
				\item $it \rightarrow \infty \Rightarrow \theta_j = 0$
				\item $L1$ essaye de tirer les valeurs des $\theta$ à l'intérieur d'un cube
				\item la taille du cube est inversement proportionnelle à $\lambda$
			\end{itemize}
		\end{column}
		\begin{column}{.4\textwidth}
			\hgraphpage{L1L2comp.png}
		\end{column}
	\end{columns}
	
\end{frame}

\begin{frame}
	\frametitle{Régularisation et sélection d'attributs (support TP)}
	\framesubtitle{Lasso : optimisation}
	
	\begin{itemize}
		\item L1 n'est pas différentielle dans $0$
		\item Plusieurs techniques ont été proposées pour trouver le chemin de solution $L1$
		\begin{itemize}
			\item méthodes de sous-gradients; ex. \expword{least-angle regression (LARS)}
			\item descente par coordonnée
			\item méthodes de gradient proximal (par opérateur \keyword{soft thresholding})
		\end{itemize}
	\end{itemize}

	\[S_\frac{\lambda}{M} (\theta_j) = 
	\begin{cases}
		\theta - \frac{\lambda}{M} & \theta_j > \frac{\lambda}{M} \\
		0 & \theta_j \in [-\frac{\lambda}{M}, \frac{\lambda}{M}] \\
		\theta + \frac{\lambda}{M} & \theta_j < -\frac{\lambda}{M} \\
	\end{cases}
	\]
	
	\begin{itemize}
		\item Dans le cas du gradient proximal, la mise à jour des paramètres se fait comme suit : 
		\item $\theta = S_\frac{\lambda}{M} (\theta - \alpha \frac{\partial}{\partial \theta} J_\text{erreur}(\theta)) $
	\end{itemize}
	
\end{frame}

\subsection{ElasticNet}

\begin{frame}
	\frametitle{Régularisation et sélection d'attributs (support TP)}
	\framesubtitle{ElasticNet}
	
	\[J_{EN} = \frac{\lambda}{M} \sum_{j=1}^{N} \left( r  |\theta_j| + \frac{(r-1)}{2} \theta_j^2\right)\]
	
	\begin{itemize}
		\item $\theta_0$ n'est pas concerné par la régularisation
		\item $r \in [0, 1]$ contrôle le pourcentage de la régularisation $L1$
	\end{itemize}
	
\end{frame}

\section{Sélection d'attributs}

\begin{frame}
	\frametitle{Sélection d'attributs}
	
	\begin{itemize}
		\item réduire le sur-apprentissage 
		\item réduire le temps d'entraînement
		\item améliorer l'exactitude du système
		\item \optword{Filtrage} : indépendamment de l'estimateur, on sélectionne les meilleurs attributs en se basant sur des tests statistiques uni-variées
		\item \optword{Intégrées (embedded)} : les attributs sont sélectionnés pendant l'entraînement
		\item \optword{Enveloppante (Wrapper)} : on sélectionne les meilleurs attributs pour un estimateur donné avant d'entraîner
	\end{itemize}
	
\end{frame}

\subsection{Par filtrage}

\begin{frame}
	\frametitle{Sélection d'attributs}
	\framesubtitle{Par filtrage : score}
	
	\begin{tabular}{|p{0.4\textwidth}|p{0.2\textwidth}|p{0.3\textwidth}|}
	\hline
	\multirow{2}{*}{Attribut de sortie}	& \multicolumn{2}{c|}{Attribut d'entrée} \\
	\cline{2-3}
		& Numérique & Nominal \\
	\hline
	Numérique (Régression) & Pearson & Information mutuelle \\
	\hline
	Nominal (Classification) & ANOVA, Information mutuelle & Chi-2, Information mutuelle\\
	\hline
	\end{tabular}

	\begin{itemize}
		\item \optword{Pearson} : \expword{sklearn.feature\_selection.f\_regression}
		\item \optword{ANOVA} : \expword{sklearn.feature\_selection.f\_classif}
		\item \optword{Chi2} : \expword{sklearn.feature\_selection.chi2}
		\item \optword{Information mutuelle} : \expword{feature\_selection.mutual\_info\_classif}
		\item \optword{Information mutuelle} : \expword{feature\_selection.mutual\_info\_regression}
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Sélection d'attributs}
	\framesubtitle{Par filtrage : sélection}
	
	\begin{itemize}
		\item En se basant sur les scores précédents, on peut sélectionner les attributs les plus importants de plusieurs façons
		\item \optword{Nombre} : nombre des attributs à sélectionner
		\begin{itemize}
			\item \expword{sklearn.feature\_selection.SelectKBest}
		\end{itemize}
		\item \optword{Percentile} : centile des scores les plus élevés.
		\begin{itemize}
			\item \expword{sklearn.feature\_selection.SelectPercentile}
		\end{itemize}
		\item \optword{P-value} : seuil max des p-values acceptées
		\begin{itemize}
			\item \expword{sklearn.feature\_selection.SelectFpr}
		\end{itemize}
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Sélection d'attributs}
	\framesubtitle{Par filtrage : One-Way ANOVA (raisonnement)}
	
	\begin{itemize}
		\item COMPLETELY RANDOM DESIGN (CRD) 
		\item Étant donné un attribut $A$ ($M$ échantillons) avec des valeurs continues (réelles)
		\item on divise ses valeurs sur des ensembles $A_j$ où $j$ est une classe parmi les $N$ classes. On les appelles \keyword{Treatements}
		\item un attribut est représentatif (bien corrélé) des classes de sortie, si ..
		\begin{itemize}
			\item les valeurs de la même classe ont moins de variance (variance intra-classe)
			\item les valeurs des classes ont plus de variance avec le reste (variance inter-classes)
			\item \textbf{DONC}, le ratio (variance inter-classes)/(variance intra-classe) doit être grand
			\item On appelle ce ratio : \keyword{F-value} d'ANOVA
		\end{itemize}
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Sélection d'attributs}
	\framesubtitle{Par filtrage : One-Way ANOVA (Calcul)}
	
	\begin{itemize}
		\item Correlation factor : $CF = \frac{(\sum_{ij} A_{ij})^2}{M} $
		\item Total Sum of Squares : $TotalSS = \sum_{ij} A_{ij}^2 - CF$
		\item Treatment Sum of Squares : $TreatmentSS = \sum_j \frac{(\sum_{i} A_{ij})^2}{|A_j|} - CF$
		\item Error Sum of Squares : $ErrorSS = TotalSS - TreatmentSS$
		\item Mean of Squares Between : $MSB = \frac{TreatmentSS}{(N - 1)}$
		\item Mean of Squares Within : $MSW =  \frac{ErrorSS}{(M - N)}$
		\item $Fvalue = \frac{MSB}{MSW}$
	\end{itemize}
	
\end{frame}

\subsection{Intégrées (Embedded)}

\begin{frame}
	\frametitle{Sélection de données}
	\framesubtitle{Intégrées (Embedded)}
	
	\begin{itemize}
		\item \optword{Arbres de décision}
		\begin{itemize}
			\item Dans chaque nœud, on choisi le meilleur attribut pour éclater l'ensemble de sortie.
			\item Certains attributs ne vont pas être considérés.
			\item Finalement, l'arbre n'utilisera que les attributs avec grande capacité de séparation.
		\end{itemize}
		\item \optword{Régularisation L1}
		\begin{itemize}
			\item Elle force les paramètres thétas à avoir des petites valeurs.
			\item Dans L1, certains paramètres sont mises à 0 après convergence.
			\item Dans ce cas, lors de l'estimation le modèle ne tient as compte les attributs avec des paramètres nuls.
		\end{itemize}
	\end{itemize}
	
\end{frame}

\subsection{Enveloppante (Wrapper)}

\begin{frame}
	\frametitle{Sélection de données}
	\framesubtitle{Enveloppante (Wrapper)}
	
	\begin{itemize}
		\item utiliser un estimateur et essayer de trouver la combinaison des attributs avec plus de performance
		\expword{sklearn.feature\_selection.SequentialFeatureSelector}
		\item \optword{Ascendante (Forward selection)}
		\begin{itemize}
			\item l'ensemble initial des attribut est vide
			\item commence par sélectionner un seul attribut qui maximise la cross-validation
			\item ajoute des attributs de la même manière jusqu'à arriver au nombre voulu
			\item \expword{SequentialFeatureSelector(direction="forward")}
		\end{itemize}
		\item \optword{Descendante (Backward elimination)}
		\begin{itemize}
			\item l'ensemble initial des attribut égale à $N$
			\item commence par éliminer un seul attribut qui minimise la cross-validation
			\item élimine des attributs de la même manière jusqu'à arriver au nombre voulu
			\item \expword{SequentialFeatureSelector(direction="backward")}
		\end{itemize}
	\end{itemize}
	
\end{frame}


\begin{frame}
	
	\Huge 
	\[
	\text{The end of the } \cancelto{\text{presentation}}{\text{world}}
	\]
	
\end{frame}

\begin{frame}
	
	\begin{center}
		\Huge 
		arrêter le défilement 
		
		... 
		
		c'est la fin !
	\end{center}
	
\end{frame}



%\insertbibliography{ML-preparation}{*}

\end{document}



