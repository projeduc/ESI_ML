{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5c3f313",
   "metadata": {},
   "source": [
    "# Workshop. Neural networks' tools (Pytorch)\n",
    "\n",
    "<p style='text-align: right;font-style: italic; color: red;'>Designed by: Mr. Abdelkrime Aries</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5823642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1+cu121'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn, optim\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbfd29ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-lightning\n",
    "# import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3919a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas     as pd\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a79a7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce92c408",
   "metadata": {},
   "source": [
    "## I. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a2fcff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4435, 36]), torch.Size([4435, 6]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('data/sat.trn', delimiter=' ', header=None)\n",
    "\n",
    "X_train = train.iloc[:, :-1].values\n",
    "Y_train = train.iloc[:,  -1].values\n",
    "\n",
    "lbin = LabelBinarizer()\n",
    "\n",
    "X_train = X_train / 255.\n",
    "Y_train = lbin.fit_transform(Y_train)\n",
    "\n",
    "X_train = Tensor(X_train)\n",
    "Y_train = Tensor(Y_train)\n",
    "\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6356a3fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2000, 36]), (2000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('data/sat.tst', delimiter=' ', header=None)\n",
    "\n",
    "X_test = test.iloc[:, :-1].values\n",
    "Y_test = test.iloc[:,  -1].values\n",
    "\n",
    "X_test = X_test / 255.\n",
    "# Y_test = lbin.transform(Y_test)\n",
    "\n",
    "X_test = Tensor(X_test)\n",
    "# Y_test = Tensor(Y_test)\n",
    "\n",
    "X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba6d6e5",
   "metadata": {},
   "source": [
    "## II. High level\n",
    "\n",
    "### II.1. Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "715ce675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=36, out_features=10, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=10, out_features=6, bias=True)\n",
       "  (5): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn1 = nn.Sequential(\n",
    "   nn.Linear(X_train.shape[1], 10),\n",
    "   nn.ReLU(),\n",
    "   nn.Linear(10, 10),\n",
    "   nn.ReLU(),\n",
    "   nn.Linear(10, Y_train.shape[1]),\n",
    "   nn.Softmax(dim=1)\n",
    "   )\n",
    "\n",
    "nn1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b8ada7",
   "metadata": {},
   "source": [
    "### II.2. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6434ac36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t= 0 , loss =  1.7860535\n",
      "t= 100 , loss =  1.4005458\n",
      "t= 200 , loss =  1.3745211\n",
      "t= 300 , loss =  1.3695382\n",
      "t= 400 , loss =  1.3673468\n",
      "t= 500 , loss =  1.3660754\n",
      "t= 600 , loss =  1.3653035\n",
      "t= 700 , loss =  1.3109933\n",
      "t= 800 , loss =  1.2944173\n",
      "t= 900 , loss =  1.2906961\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(nn1.parameters(), lr=0.01)\n",
    "\n",
    "for t in range(1000):\n",
    "    Y_pred = nn1(X_train)\n",
    "    loss = loss_fn(Y_pred, Y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if not t%100: #print every 100 iterations\n",
    "        print('t=', t, ', loss = ', loss.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c00065f",
   "metadata": {},
   "source": [
    "### II.3. Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06522f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.99      0.88       461\n",
      "           2       0.00      0.00      0.00       224\n",
      "           3       0.81      0.96      0.88       397\n",
      "           4       0.00      0.00      0.00       211\n",
      "           5       0.52      0.75      0.61       237\n",
      "           7       0.71      0.93      0.81       470\n",
      "\n",
      "    accuracy                           0.73      2000\n",
      "   macro avg       0.47      0.60      0.53      2000\n",
      "weighted avg       0.57      0.73      0.64      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, lbin.inverse_transform(nn1(X_test)), zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb730a6",
   "metadata": {},
   "source": [
    "## III. High level with a custom class\n",
    "\n",
    "### III.1. Custom Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f09d715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyLayer(in_features=3, out_features=2, bias=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MyLayer in here\n",
    "class MyLayer(nn.Linear):\n",
    "    def __init__(self, \n",
    "                 nb_in: int, nb_out: int, \n",
    "                 bias: bool = True, act: Literal['relu', 'sigmoid', 'linear'] = 'linear'):\n",
    "        assert nb_in   > 0\n",
    "        assert nb_out  > 0\n",
    "        super().__init__(nb_in, nb_out, bias=bias)\n",
    "\n",
    "        self.act = lambda x: x\n",
    "        if act == 'relu':\n",
    "            self.act = nn.ReLU()\n",
    "        elif act == 'sigmoid':\n",
    "            self.act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.act(super().forward(X))\n",
    "\n",
    "\n",
    "MyLayer(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b1b1f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AssertionError()\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "# Must print an 'Exception' or 'AssertionError'\n",
    "\n",
    "try:\n",
    "    ml1 = MyLayer(0, 2)\n",
    "except Exception as e:\n",
    "    print(repr(e))\n",
    "\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19b7fe99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "MyLayer(\n",
      "  in_features=3, out_features=2, bias=False\n",
      "  (act): ReLU()\n",
      ")\n",
      "-------------------------------\n",
      "bias= None\n",
      "output= tensor([[1.2344, 0.6513],\n",
      "        [3.2725, 2.1922]], grad_fn=<ReluBackward0>)\n",
      "===============================\n",
      "MyLayer(\n",
      "  in_features=3, out_features=2, bias=True\n",
      "  (act): Sigmoid()\n",
      ")\n",
      "-------------------------------\n",
      "bias= Parameter containing:\n",
      "tensor([ 0.0439, -0.5674], requires_grad=True)\n",
      "output= tensor([[0.7324, 0.0676],\n",
      "        [0.8473, 0.0082]], grad_fn=<SigmoidBackward0>)\n",
      "===============================\n",
      "MyLayer(in_features=3, out_features=1, bias=True)\n",
      "-------------------------------\n",
      "bias= Parameter containing:\n",
      "tensor([-0.3595], requires_grad=True)\n",
      "output= tensor([[0.0351],\n",
      "        [0.0075]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "l2ts = [\n",
    "    MyLayer(3, 2, bias=False, act='relu'),\n",
    "    MyLayer(3, 2, bias=True, act='sigmoid'),\n",
    "    MyLayer(3, 1)\n",
    "    ]\n",
    "\n",
    "XX = Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "for l in l2ts:\n",
    "    print('===============================')\n",
    "    print(l)\n",
    "    print('-------------------------------')\n",
    "    print('bias=', l.bias)\n",
    "    weight = Tensor(l.weight)\n",
    "    print('output=', l(XX))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd89a6f1",
   "metadata": {},
   "source": [
    "### III.2. Custom Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adde9856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.locked = False\n",
    "    \n",
    "    def add_layer(self, layer: MyLayer):\n",
    "        if self.locked:\n",
    "            raise Exception('You cannot add more layers')\n",
    "        out_nbr = None\n",
    "        if len(self.layers):\n",
    "            out_nbr = self.layers[-1].weight.shape[0]\n",
    "        in_nbr = layer.weight.shape[1]\n",
    "        if out_nbr is not None and out_nbr != in_nbr:\n",
    "            raise Exception(f'The last layer outputs ({out_nbr}) must be the same as this layer input {in_nbr}')\n",
    "        self.layers.append(layer)\n",
    "        return self\n",
    "        \n",
    "    def compile(self, nb_in=1, nb_out=1, bias=True, multiclass=False, lr=1.):\n",
    "        if len(self.layers):\n",
    "            nb_in = self.layers[-1].weight.shape[0]\n",
    "        self.layers.append(MyLayer(nb_in, nb_out, bias=bias, act='sigmoid'))\n",
    "\n",
    "        self.loss = nn.BCELoss()\n",
    "        if multiclass and nb_out > 1:\n",
    "            self.layers[-1].act = nn.Softmax(dim=1)\n",
    "            self.loss = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.locked = True\n",
    "\n",
    "    def forward(self, X):\n",
    "        Z = X \n",
    "        for layer in self.layers:\n",
    "            Z = layer(Z)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, X, Y):\n",
    "        Y_pred = self.forward(X)\n",
    "        loss = self.loss(Y_pred, Y)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.detach().numpy()\n",
    "\n",
    "    def fit(self, X, Y, epochs=20, pr:int = 100):\n",
    "        for epoch in range(epochs):\n",
    "            loss = self.backward(X, Y)\n",
    "            if not epoch%pr: # print every pr\n",
    "                print('epoch', epoch, ', loss =', loss)\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.forward(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84994fdd",
   "metadata": {},
   "source": [
    "### III.3. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b1ff947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyMLP(\n",
       "  (layers): ModuleList(\n",
       "    (0): MyLayer(\n",
       "      in_features=36, out_features=10, bias=True\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (1): MyLayer(\n",
       "      in_features=10, out_features=10, bias=True\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (2): MyLayer(\n",
       "      in_features=10, out_features=6, bias=True\n",
       "      (act): Softmax(dim=1)\n",
       "    )\n",
       "  )\n",
       "  (loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn2 = MyMLP()\n",
    "nn2.add_layer(MyLayer(X_train.shape[1], 10, act='relu'))\\\n",
    "   .add_layer(MyLayer(10, 10, act='relu'))\\\n",
    "   .compile(nb_out=Y_train.shape[1], lr=0.01, multiclass=True)\n",
    "\n",
    "nn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15f42cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 , loss = 1.8015405\n",
      "epoch 100 , loss = 1.3338568\n",
      "epoch 200 , loss = 1.276798\n",
      "epoch 300 , loss = 1.2276777\n",
      "epoch 400 , loss = 1.209966\n",
      "epoch 500 , loss = 1.2046121\n",
      "epoch 600 , loss = 1.20211\n",
      "epoch 700 , loss = 1.1997224\n",
      "epoch 800 , loss = 1.1977218\n",
      "epoch 900 , loss = 1.1965315\n"
     ]
    }
   ],
   "source": [
    "nn2.fit(X_train, Y_train, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc07c7fb",
   "metadata": {},
   "source": [
    "### III.4. Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06871b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.99      0.95       461\n",
      "           2       0.95      0.90      0.93       224\n",
      "           3       0.81      0.96      0.88       397\n",
      "           4       0.00      0.00      0.00       211\n",
      "           5       0.83      0.72      0.77       237\n",
      "           7       0.71      0.93      0.80       470\n",
      "\n",
      "    accuracy                           0.82      2000\n",
      "   macro avg       0.70      0.75      0.72      2000\n",
      "weighted avg       0.74      0.82      0.78      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, lbin.inverse_transform(nn2(X_test)), zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca87fb3",
   "metadata": {},
   "source": [
    "## IV. Low level\n",
    "\n",
    "### IV.1. Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0525616",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSigmoid(nn.Module):\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        return 1/(1+torch.exp(-X))\n",
    "\n",
    "class SimpleReLU(nn.Module):\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        return torch.where(X > 0., X, 0.)\n",
    "    \n",
    "\n",
    "class SimpleSoftmax(nn.Module):\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        H = torch.exp(X)\n",
    "        return H/H.sum(axis=1).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "699879d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7311, 0.2689, 0.5000],\n",
      "        [0.3775, 0.5498, 0.9933]])\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2000, 5.0000]])\n",
      "tensor([[0.6652, 0.0900, 0.2447],\n",
      "        [0.0040, 0.0081, 0.9878]])\n"
     ]
    }
   ],
   "source": [
    "XX = Tensor([[1., -1., 0.], [-0.5, 0.2, 5]])\n",
    "sigmoid = SimpleSigmoid()\n",
    "print(sigmoid(XX))\n",
    "relu = SimpleReLU()\n",
    "print(relu(XX))\n",
    "softmax = SimpleSoftmax()\n",
    "print(softmax(XX))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110e9570",
   "metadata": {},
   "source": [
    "### IV.2. Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3709a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBCE(nn.Module):\n",
    "    def forward(self, H: Tensor, Y: Tensor) -> Tensor:\n",
    "        return torch.mean(- Y * torch.log(H) - (1-Y) * torch.log(1-H))\n",
    "    \n",
    "class SimpleCE(nn.Module):\n",
    "    def forward(self, H: Tensor, Y: Tensor) -> Tensor:\n",
    "        return torch.mean(- Y * torch.log(H))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02b51e4",
   "metadata": {},
   "source": [
    "### IV.3. Optimization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3645425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGD(optim.Optimizer):\n",
    "    def __init__(self, params, lr=0.001):\n",
    "        super().__init__(params, defaults={'lr': lr})\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups: \n",
    "            for p in group['params']:  \n",
    "                p.data -= group['lr'] * p.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a238453d",
   "metadata": {},
   "source": [
    "### IV.4. Custom Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a757d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleLayer()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SimpleLayer in here\n",
    "class SimpleLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 nb_in: int, nb_out: int, \n",
    "                 bias: bool = True, act: Literal['relu', 'sigmoid', 'linear'] = 'linear'):\n",
    "        assert nb_in   > 0\n",
    "        assert nb_out  > 0\n",
    "        super().__init__()\n",
    "\n",
    "        self.W = nn.parameter.Parameter(torch.zeros([nb_in, nb_out]))\n",
    "        self.b = torch.zeros([1   , nb_out])\n",
    "        if bias:\n",
    "            self.b = nn.parameter.Parameter(self.b)\n",
    "\n",
    "        self.act = lambda x: x\n",
    "        if act == 'relu':\n",
    "            self.act = SimpleReLU()\n",
    "        elif act == 'sigmoid':\n",
    "            self.act = SimpleSigmoid()\n",
    "\n",
    "    def randomize(self):\n",
    "        self.W.data = torch.normal(torch.zeros(self.W.shape), 1.0)\n",
    "        if self.b.requires_grad:\n",
    "            self.b.data = torch.normal(torch.zeros(self.b.shape), 1.0)\n",
    "            \n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.act(torch.matmul(X, self.W) + self.b)\n",
    "\n",
    "\n",
    "SimpleLayer(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e6c2644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0.]]),\n",
       " Parameter containing:\n",
       " tensor([[-1.3187, -0.8510],\n",
       "         [-1.2403, -1.0491],\n",
       "         [ 1.1244, -0.0342]], requires_grad=True),\n",
       " [Parameter containing:\n",
       "  tensor([[-1.3187, -0.8510],\n",
       "          [-1.2403, -1.0491],\n",
       "          [ 1.1244, -0.0342]], requires_grad=True)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl = SimpleLayer(3, 2, bias=False)\n",
    "\n",
    "sl.randomize()\n",
    "sl.b, sl.W, list(sl.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4146479",
   "metadata": {},
   "source": [
    "### IV.5. Custom Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d381678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.locked = False\n",
    "    \n",
    "    def add_layer(self, layer: SimpleLayer):\n",
    "        if self.locked:\n",
    "            raise Exception('You cannot add more layers')\n",
    "        out_nbr = None\n",
    "        if len(self.layers):\n",
    "            out_nbr = self.layers[-1].W.shape[1]\n",
    "        in_nbr = layer.W.shape[0]\n",
    "        if out_nbr is not None and out_nbr != in_nbr:\n",
    "            raise Exception(f'The last layer outputs ({out_nbr}) must be the same as this layer input {in_nbr}')\n",
    "        self.layers.append(layer)\n",
    "        return self\n",
    "        \n",
    "    def compile(self, nb_in=1, nb_out=1, bias=True, multiclass=False, lr=1.):\n",
    "        if len(self.layers):\n",
    "            nb_in = self.layers[-1].W.shape[1]\n",
    "        self.layers.append(SimpleLayer(nb_in, nb_out, bias=bias, act='sigmoid'))\n",
    "\n",
    "        self.loss = SimpleBCE()\n",
    "        if multiclass and nb_out > 1:\n",
    "            self.layers[-1].act = SimpleSoftmax()\n",
    "            self.loss = SimpleCE()\n",
    "        self.optimizer = SimpleGD(self.parameters(), lr=lr)\n",
    "        self.locked = True\n",
    "\n",
    "    def forward(self, X):\n",
    "        Z = X \n",
    "        for layer in self.layers:\n",
    "            Z = layer(Z)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, X, Y):\n",
    "        Y_pred = self.forward(X)\n",
    "        loss = self.loss(Y_pred, Y)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.detach().numpy()\n",
    "\n",
    "    def fit(self, X, Y, epochs=20, pr:int = 100):\n",
    "        for epoch in range(epochs):\n",
    "            loss = self.backward(X, Y)\n",
    "            if not epoch%pr: # print every pr\n",
    "                print('epoch', epoch, ', loss =', loss)\n",
    "\n",
    "    def randomize(self):\n",
    "        for layer in self.layers:\n",
    "            layer.randomize()\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9faa772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8401],\n",
      "        [0.8428]], grad_fn=<MulBackward0>)\n",
      "1.0020916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.5149],\n",
       "        [0.5659]], requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Result:\n",
    "# tensor([[0.8401],\n",
    "#         [0.8428]], grad_fn=<MulBackward0>)\n",
    "# 1.0020916\n",
    "# Parameter containing:\n",
    "# tensor([[0.5149],\n",
    "#         [0.5659]], requires_grad=True)\n",
    "\n",
    "nn3t = SimpleMLP()\n",
    "nn3t.add_layer(SimpleLayer(2, 2, act='sigmoid'))\\\n",
    "    .add_layer(SimpleLayer(2, 2, act='sigmoid'))\\\n",
    "    .compile()\n",
    "\n",
    "# print(nn3)\n",
    "\n",
    "with torch.no_grad():\n",
    "    nn3t.layers[0].W += torch.Tensor([[0.5, 0.3], [0.2, 0.4]])\n",
    "    nn3t.layers[0].b += torch.Tensor([[-0.3, 0.5]])\n",
    "    nn3t.layers[1].W += torch.Tensor([[0.3, -0.1], [0.5, -0.3]])\n",
    "    nn3t.layers[1].b += torch.Tensor([[-0.3, -0.2]])\n",
    "    nn3t.layers[2].W  += torch.Tensor([[0.7], [0.7]])\n",
    "    nn3t.layers[2].b  += torch.Tensor([[1.]])\n",
    "\n",
    "XX = Tensor([[2, -1], [3, 5]])\n",
    "YY = Tensor([[0], [1]])\n",
    "\n",
    "print(nn3t.forward(XX))\n",
    "\n",
    "loss = nn3t.backward(XX, YY)\n",
    "\n",
    "print(loss)\n",
    "\n",
    "nn3t.layers[2].W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251617ac",
   "metadata": {},
   "source": [
    "### IV.6. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4d472a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-1.0324e+00,  6.2962e-02,  1.7376e+00, -1.0988e+00, -5.4936e-01,\n",
       "           9.8362e-01, -6.1377e-01, -1.1112e-01,  9.5957e-01,  1.1741e+00],\n",
       "         [-2.3289e-01,  9.5027e-01,  5.2685e-01, -5.6911e-01,  1.0022e+00,\n",
       "          -3.1141e-01,  9.8217e-01,  1.6803e-02,  1.0775e+00,  3.2616e-01],\n",
       "         [-6.1324e-01, -6.5029e-01, -5.3696e-01, -1.3392e-01,  7.8460e-01,\n",
       "          -7.0812e-01,  2.8266e-01,  4.3198e-02, -4.9156e-01,  1.2444e+00],\n",
       "         [-1.8798e+00, -3.5944e-01, -1.8772e+00, -3.4254e-02,  1.6008e+00,\n",
       "           2.0627e+00,  1.2528e+00,  1.6610e+00,  1.2891e+00, -1.1635e+00],\n",
       "         [ 2.8728e-01, -5.6467e-01,  1.2480e+00, -2.5756e-03, -1.5971e+00,\n",
       "          -5.7729e-01, -3.5158e-01,  1.1920e+00, -9.4963e-01,  7.0126e-01],\n",
       "         [-5.2769e-01,  5.0802e-01,  1.1877e+00,  2.2736e-01, -8.0788e-01,\n",
       "           3.0933e-01, -1.8719e+00, -9.1211e-01,  2.2809e-01,  4.5248e-01],\n",
       "         [-3.0328e-01, -9.5507e-01,  6.4631e-01, -7.5942e-01, -8.6839e-01,\n",
       "          -9.3197e-01,  8.6038e-01,  2.3728e-01,  1.4765e+00, -2.1698e+00],\n",
       "         [-1.1630e+00, -8.4062e-01,  5.6441e-01, -2.5009e-01, -2.1973e+00,\n",
       "           2.8342e-01, -5.2404e-01, -8.2577e-01, -1.7603e+00,  1.4967e+00],\n",
       "         [-6.2792e-01,  1.3024e+00, -1.3177e+00, -1.3746e+00, -2.1010e+00,\n",
       "           2.5168e-01,  8.3305e-01, -5.2173e-01,  7.5916e-02, -4.0959e-01],\n",
       "         [-4.2723e-01,  1.5470e-01, -6.4194e-01,  5.8679e-02, -2.0265e+00,\n",
       "           8.8146e-02,  2.4860e-01,  5.3176e-01, -1.2571e+00, -4.4142e-01],\n",
       "         [-1.7346e-01, -9.4361e-01, -2.0652e+00, -7.1890e-01, -6.2839e-01,\n",
       "           5.9203e-01, -6.5193e-01,  2.6317e-01,  9.7734e-01,  7.1247e-01],\n",
       "         [ 3.3172e-01,  1.4665e+00,  9.5621e-01, -3.4792e-01,  1.9463e-01,\n",
       "          -1.1148e+00, -6.0467e-01,  8.5538e-01,  2.2826e-01,  2.0342e+00],\n",
       "         [ 1.0376e-01, -7.8401e-01,  5.6078e-02, -3.3274e-01,  1.5998e+00,\n",
       "           1.7325e-01,  2.6347e+00, -2.7200e-01, -1.2074e+00, -3.3016e-01],\n",
       "         [ 4.6359e-02, -5.7394e-01, -8.5983e-01,  5.4883e-01, -9.9630e-01,\n",
       "          -9.6617e-02,  1.1401e+00,  1.0781e+00,  1.4771e+00,  1.5510e+00],\n",
       "         [ 4.3292e-01, -8.3605e-01,  6.3105e-01,  2.7489e+00, -1.5757e+00,\n",
       "           1.4940e+00, -8.4775e-03, -8.9772e-01, -3.6463e-01, -1.2739e-01],\n",
       "         [ 6.5225e-01,  8.1355e-01,  4.9217e-01, -8.9628e-01, -1.2702e-01,\n",
       "          -4.8614e-01, -9.7615e-02,  1.5198e+00, -1.2190e+00, -1.0514e-01],\n",
       "         [-9.5954e-01, -8.3221e-01,  7.5279e-01, -4.1019e-01, -1.6512e+00,\n",
       "          -5.8478e-01,  1.0591e+00,  8.4154e-02, -1.2632e+00, -3.5796e-01],\n",
       "         [-1.5702e+00,  3.0189e+00,  1.0307e+00, -1.0163e+00,  8.3715e-01,\n",
       "           9.4812e-01, -1.5891e-01,  7.8965e-02, -4.2962e-01,  3.3971e-01],\n",
       "         [-1.3304e+00, -2.0816e-01, -7.4896e-01, -2.5620e-02,  3.2771e-01,\n",
       "           6.5033e-01, -4.3130e-02,  6.1586e-01, -1.1076e+00, -3.1002e-01],\n",
       "         [-3.7541e-01, -3.4213e-01,  1.1299e+00, -8.1116e-01,  1.2120e+00,\n",
       "           3.5789e+00, -1.5484e+00, -8.7979e-02,  9.4740e-01,  4.6771e-01],\n",
       "         [-3.0986e+00, -5.7318e-01,  1.7742e-01,  1.4526e-01, -3.1492e-01,\n",
       "           2.4142e+00,  1.2280e+00,  4.8918e-01, -1.2668e+00, -2.4811e+00],\n",
       "         [ 1.8306e+00, -2.5175e+00,  9.9315e-01,  4.1536e-01,  4.2910e-01,\n",
       "          -5.0440e-02, -6.2754e-01,  5.7198e-01, -1.1163e+00,  4.9263e-01],\n",
       "         [ 6.2922e-01, -1.3806e+00, -5.7170e-01,  2.6022e-02,  9.6382e-01,\n",
       "          -3.7933e-01, -7.8774e-01, -8.1554e-01, -2.6785e-02,  1.3576e-01],\n",
       "         [ 1.2580e+00, -1.0156e-01,  5.0829e-01, -1.3127e-01, -2.5572e-01,\n",
       "          -1.3382e+00,  2.9727e-01, -8.1508e-01,  3.5383e-01,  4.0002e-01],\n",
       "         [-8.2152e-01, -4.7799e-01, -8.2055e-02,  3.7703e-01,  4.1773e-01,\n",
       "           1.4641e+00, -6.2217e-01, -1.7643e+00,  9.0686e-01, -2.2694e-01],\n",
       "         [ 1.6532e+00, -1.9848e+00, -6.2700e-01, -3.6278e-01,  2.0299e-01,\n",
       "           4.3220e-02, -1.0194e+00, -1.7244e+00,  1.0107e+00, -1.8653e-01],\n",
       "         [ 1.1459e+00,  1.7224e-01, -4.6327e-01, -1.1695e+00, -9.6558e-01,\n",
       "           6.7641e-01,  7.9436e-01, -3.1834e-01,  1.7895e-01,  8.7632e-01],\n",
       "         [ 3.1653e-01,  5.3237e-01, -7.3664e-01, -1.2196e-01,  1.5715e-01,\n",
       "           9.5235e-01,  8.8341e-02,  6.8423e-01, -1.0812e+00,  1.3677e+00],\n",
       "         [ 6.2664e-01,  1.2245e+00,  2.8901e-01,  1.8743e-01, -8.8068e-01,\n",
       "          -1.1433e+00,  1.2674e+00,  6.2852e-01,  1.8258e-01,  7.8443e-01],\n",
       "         [-1.0499e+00,  1.0374e+00,  4.0015e-01, -1.6543e+00, -4.4087e-02,\n",
       "          -1.4144e+00,  1.9177e-01, -1.1748e+00, -1.5748e-01, -3.3097e-01],\n",
       "         [ 8.4072e-02, -5.5064e-01, -2.9175e-01, -1.1543e+00, -6.3813e-01,\n",
       "           5.7827e-01,  3.6505e-01, -1.8666e+00, -7.0174e-03, -7.5821e-01],\n",
       "         [-4.2585e-01,  1.4211e+00,  1.0785e+00, -6.1874e-01,  2.0940e+00,\n",
       "          -3.2709e-02,  1.9889e+00, -2.8153e-01,  2.4981e+00,  4.3570e-01],\n",
       "         [ 6.4628e-01,  2.6348e-01,  1.1058e-01,  4.5875e-01, -2.3369e-01,\n",
       "          -4.7216e-01,  5.8541e-01, -1.9112e+00, -7.4665e-01,  5.2117e-01],\n",
       "         [-1.5549e+00, -4.3878e-01,  2.8743e+00, -1.4031e+00, -1.1327e+00,\n",
       "          -8.7378e-01,  1.2577e+00,  5.7568e-01,  3.6113e-01,  1.3217e+00],\n",
       "         [-2.8410e-01,  3.7150e-01, -8.4039e-01, -1.3514e+00,  1.2427e+00,\n",
       "           7.1945e-01,  7.1213e-02,  1.6988e+00,  7.6500e-01,  1.3247e+00],\n",
       "         [-5.2892e-01,  7.2867e-01, -4.8904e-01, -7.6063e-01, -1.8832e+00,\n",
       "          -8.6106e-01, -1.4626e-02, -9.7024e-01,  2.5963e-01, -9.1677e-02]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.1113, -2.0483,  0.7349, -0.9866,  1.5767,  2.1968,  0.3704, -1.8469,\n",
       "          -0.0143,  0.4079]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.3454,  0.5781, -1.4284, -0.0769, -0.4066,  1.0097, -0.6329,  0.1433,\n",
       "          -0.4943, -1.0986],\n",
       "         [-0.2087,  1.4652,  0.4663, -0.9569, -2.6270, -0.1754, -1.3454, -0.3878,\n",
       "          -2.1055,  0.2702],\n",
       "         [-0.2913,  0.7595, -0.8456, -1.3113,  0.4985, -0.7335,  1.2444,  0.4303,\n",
       "           0.2240,  0.2678],\n",
       "         [-1.0169,  0.8706, -0.2058, -0.1042,  0.3967,  0.7362, -0.0137, -0.1014,\n",
       "          -1.4078, -0.0681],\n",
       "         [-1.7374, -1.4811,  0.0059,  0.3318, -0.4737,  0.3668,  0.0043, -1.5368,\n",
       "          -0.0945, -0.4971],\n",
       "         [ 0.0527,  0.6849,  2.1500, -0.6065,  0.3615,  0.6179, -0.6422,  0.0832,\n",
       "          -0.2533,  0.8039],\n",
       "         [-1.6292,  0.3127,  0.2226, -0.6686,  0.2565, -0.6889, -1.9856,  0.4653,\n",
       "           0.0827, -1.5999],\n",
       "         [ 0.1414,  1.4065, -0.8410, -0.1885,  0.3894,  0.3281, -0.4627,  0.1501,\n",
       "          -0.6272, -0.8009],\n",
       "         [-2.2040,  0.5915, -0.2506,  0.9146,  0.8689, -0.8131,  1.3121, -0.6823,\n",
       "           1.7718, -1.6015],\n",
       "         [ 1.0171,  0.8191, -1.7283,  0.5523,  1.7065,  0.7820,  0.6400,  0.1479,\n",
       "           0.6943, -0.1707]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.4536, -0.2058, -0.4933, -0.2040, -1.7590, -0.9609, -0.4866,  0.0827,\n",
       "           0.0422,  0.7977]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.2529, -1.3143, -1.6442,  0.3745, -0.4758,  2.4474],\n",
       "         [ 0.5451, -0.3402, -2.3370,  1.2084,  1.5769, -0.1174],\n",
       "         [ 0.3080,  0.3453,  0.6800, -0.1961,  0.7639, -2.0238],\n",
       "         [ 1.7133, -0.2587,  0.5212, -1.3643,  0.5756, -0.3141],\n",
       "         [ 1.1072,  0.7750, -0.9527, -0.4737,  0.2177,  0.2486],\n",
       "         [-1.3290,  0.3306,  1.2158,  1.7976, -0.4416,  1.4211],\n",
       "         [ 0.3313,  0.4359, -0.5097, -1.0535, -1.2488,  0.5612],\n",
       "         [-0.5094, -1.2663, -0.5485,  0.9306, -0.6931, -0.5844],\n",
       "         [-0.0101, -0.9774,  0.2934,  1.1481,  0.0090, -0.5361],\n",
       "         [ 0.5799,  0.1631,  0.4570, -1.4435, -1.4625,  0.4963]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0370,  1.0784,  0.7486, -0.2463, -0.1050,  0.4015]],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn3 = SimpleMLP()\n",
    "nn3.add_layer(SimpleLayer(X_train.shape[1], 10, act='relu'))\\\n",
    "   .add_layer(SimpleLayer(10, 10, act='relu'))\\\n",
    "   .compile(nb_out=Y_train.shape[1], lr=0.01, multiclass=True)\n",
    "\n",
    "nn3.randomize()\n",
    "\n",
    "list(nn3.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6132a19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 , loss = 2.7024145\n",
      "epoch 100 , loss = 0.528085\n",
      "epoch 200 , loss = 0.39478248\n",
      "epoch 300 , loss = 0.35853994\n",
      "epoch 400 , loss = 0.3345934\n",
      "epoch 500 , loss = 0.3168753\n",
      "epoch 600 , loss = 0.30264002\n",
      "epoch 700 , loss = 0.29065415\n",
      "epoch 800 , loss = 0.28024605\n",
      "epoch 900 , loss = 0.27096522\n"
     ]
    }
   ],
   "source": [
    "nn3.fit(X_train, Y_train, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e255711c",
   "metadata": {},
   "source": [
    "### IV.7. Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22c82465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      0.89      0.76       461\n",
      "           2       0.99      0.55      0.71       224\n",
      "           3       0.10      0.18      0.13       397\n",
      "           4       0.00      0.00      0.00       211\n",
      "           5       0.05      0.01      0.01       237\n",
      "           7       0.20      0.20      0.20       470\n",
      "\n",
      "    accuracy                           0.35      2000\n",
      "   macro avg       0.33      0.30      0.30      2000\n",
      "weighted avg       0.34      0.35      0.33      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, lbin.inverse_transform(nn3(X_test)), zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0326f29d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
