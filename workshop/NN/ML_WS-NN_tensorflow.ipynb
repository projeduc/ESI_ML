{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5c3f313",
   "metadata": {},
   "source": [
    "# Workshop. Neural networks' tools (Tensorflow)\n",
    "\n",
    "<p style='text-align: right;font-style: italic; color: red;'>Designed by: Mr. Abdelkrime Aries</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc360b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes, TensorFlow throws errors when there is no GPU. \n",
    "# To stop these messages, we can use this code:\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5823642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.17.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee63e37f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas     as pd\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "110b428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce92c408",
   "metadata": {},
   "source": [
    "## I. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a2fcff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([4435, 36]), TensorShape([4435, 6]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('data/sat.trn', delimiter=' ', header=None)\n",
    "\n",
    "X_train = train.iloc[:, :-1].values\n",
    "Y_train = train.iloc[:,  -1].values\n",
    "\n",
    "lbin = LabelBinarizer()\n",
    "\n",
    "X_train = X_train / 255.\n",
    "Y_train = lbin.fit_transform(Y_train)\n",
    "\n",
    "X_train = tf.constant(X_train, dtype=tf.float32)\n",
    "Y_train = tf.constant(Y_train, dtype=tf.float32)\n",
    "\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6356a3fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([2000, 36]), (2000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('data/sat.tst', delimiter=' ', header=None)\n",
    "\n",
    "X_test = test.iloc[:, :-1].values\n",
    "Y_test = test.iloc[:,  -1].values\n",
    "\n",
    "X_test = X_test / 255.\n",
    "# Y_test = lbin.transform(Y_test)\n",
    "\n",
    "X_test = tf.constant(X_test, dtype=tf.float32)\n",
    "\n",
    "X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca87fb3",
   "metadata": {},
   "source": [
    "## II. Keras\n",
    "\n",
    "### II.1. Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "239d8d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">370</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m370\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m110\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │            \u001b[38;5;34m66\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">546</span> (2.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m546\u001b[0m (2.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">546</span> (2.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m546\u001b[0m (2.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn1 = keras.Sequential()\n",
    "nn1.add(keras.Input(shape=(X_train.shape[1],)))\n",
    "nn1.add(keras.layers.Dense(10, activation='relu'))\n",
    "nn1.add(keras.layers.Dense(10, activation='relu'))\n",
    "nn1.add(keras.layers.Dense(Y_train.shape[1], activation='softmax'))\n",
    "\n",
    "\n",
    "nn1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575d0056",
   "metadata": {},
   "source": [
    "### II.2. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6d208ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just to print every 10 iterations\n",
    "class PrintEveryPrEpochs(keras.callbacks.Callback):\n",
    "    def __init__(self, pr:int=10):\n",
    "        self.pr = pr\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if not epoch%self.pr:\n",
    "            print('epoch =', epoch, ', loss=', logs['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "294a902f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0 , loss= 1.2577202320098877\n",
      "epoch = 10 , loss= 0.4249204695224762\n",
      "epoch = 20 , loss= 0.3979230523109436\n",
      "epoch = 30 , loss= 0.37447887659072876\n",
      "epoch = 40 , loss= 0.36111515760421753\n",
      "epoch = 50 , loss= 0.36925023794174194\n",
      "epoch = 60 , loss= 0.3617419898509979\n",
      "epoch = 70 , loss= 0.35329291224479675\n",
      "epoch = 80 , loss= 0.3386724293231964\n",
      "epoch = 90 , loss= 0.3415277898311615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x77ef418e53f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn1.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.01), \n",
    "    loss=keras.losses.CategoricalCrossentropy()\n",
    "    )\n",
    "nn1.fit(X_train, Y_train, epochs=100, callbacks=[PrintEveryPrEpochs()], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66de4b5a",
   "metadata": {},
   "source": [
    "### II.3. Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bd3b894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.99      0.97       461\n",
      "           2       0.92      0.98      0.95       224\n",
      "           3       0.91      0.90      0.90       397\n",
      "           4       0.55      0.65      0.59       211\n",
      "           5       0.87      0.80      0.83       237\n",
      "           7       0.88      0.79      0.83       470\n",
      "\n",
      "    accuracy                           0.86      2000\n",
      "   macro avg       0.85      0.85      0.85      2000\n",
      "weighted avg       0.87      0.86      0.87      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, lbin.inverse_transform(nn1(X_test).numpy()), zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d7b644",
   "metadata": {},
   "source": [
    "## III. High level with a custom class\n",
    "\n",
    "### III.1. Custom Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b92c4ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MyLayer name=my_layer, built=True>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MyLayer in here\n",
    "class MyLayer(keras.layers.Dense):\n",
    "    def __init__(self, \n",
    "                 nb_in:int, nb_out: int, \n",
    "                 bias: bool = True, act: Literal['relu', 'sigmoid', 'linear'] = 'linear'):\n",
    "        assert nb_in   > 0\n",
    "        assert nb_out  > 0\n",
    "        super().__init__(nb_out, use_bias=bias, activation=act)\n",
    "\n",
    "        self.build((nb_in,))\n",
    "\n",
    "\n",
    "MyLayer(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4391be86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AssertionError()\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "# Must print an 'Exception' or 'AssertionError'\n",
    "\n",
    "try:\n",
    "    ml1 = MyLayer(0, 2)\n",
    "except Exception as e:\n",
    "    print(repr(e))\n",
    "\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd403141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "<MyLayer name=my_layer_1, built=True>\n",
      "-------------------------------\n",
      "bias= None\n",
      "output= tf.Tensor(\n",
      "[[2.9173596 2.136314 ]\n",
      " [6.4983883 3.548113 ]], shape=(2, 2), dtype=float32)\n",
      "===============================\n",
      "<MyLayer name=my_layer_2, built=True>\n",
      "-------------------------------\n",
      "bias= <KerasVariable shape=(2,), dtype=float32, path=my_layer_2/bias>\n",
      "output= tf.Tensor(\n",
      "[[0.9942501  0.08129059]\n",
      " [0.9999962  0.00186742]], shape=(2, 2), dtype=float32)\n",
      "===============================\n",
      "<MyLayer name=my_layer_3, built=True>\n",
      "-------------------------------\n",
      "bias= <KerasVariable shape=(1,), dtype=float32, path=my_layer_3/bias>\n",
      "output= tf.Tensor(\n",
      "[[-0.9461639]\n",
      " [-1.1059246]], shape=(2, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "l2ts = [\n",
    "    MyLayer(3, 2, bias=False, act='relu'),\n",
    "    MyLayer(3, 2, bias=True, act='sigmoid'),\n",
    "    MyLayer(3, 1)\n",
    "    ]\n",
    "\n",
    "XX = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "for l in l2ts:\n",
    "    print('===============================')\n",
    "    print(l)\n",
    "    print('-------------------------------')\n",
    "    print('bias=', l.bias)\n",
    "    weight = l.kernel\n",
    "    print('output=', l(XX))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51d2b91",
   "metadata": {},
   "source": [
    "### III.2. Custom Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef537647",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers_list = []\n",
    "        self.locked = False\n",
    "    \n",
    "    def add_layer(self, layer: MyLayer):\n",
    "        if self.locked:\n",
    "            raise Exception('You cannot add more layers')\n",
    "        out_nbr = None\n",
    "        if len(self.layers_list):\n",
    "            out_nbr = self.layers_list[-1].kernel.shape[1]\n",
    "        in_nbr = layer.kernel.shape[0]\n",
    "        if out_nbr is not None and out_nbr != in_nbr:\n",
    "            raise Exception(f'The last layer outputs ({out_nbr}) must be the same as this layer input {in_nbr}')\n",
    "        self.layers_list.append(layer)\n",
    "        return self\n",
    "        \n",
    "    def compile(self, nb_in=1, nb_out=1, bias=True, multiclass=False, lr=1.):\n",
    "        if len(self.layers_list):\n",
    "            nb_in = self.layers_list[-1].kernel.shape[1]\n",
    "\n",
    "        loss = keras.losses.BinaryCrossentropy()\n",
    "        act = 'sigmoid'\n",
    "        if multiclass and nb_out > 1:\n",
    "            act='softmax'\n",
    "            loss = keras.losses.CategoricalCrossentropy()\n",
    "        self.layers_list.append(MyLayer(nb_in, nb_out, bias=bias, act=act))\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "        self.locked = True\n",
    "        super().compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Z = X \n",
    "        for layer in self.layers_list:\n",
    "            Z = layer(Z)\n",
    "        return Z\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edbb878",
   "metadata": {},
   "source": [
    "### III.3. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b29c1bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"my_mlp\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"my_mlp\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ my_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MyLayer</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">370</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ my_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MyLayer</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ my_layer_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MyLayer</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)                    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ my_layer_4 (\u001b[38;5;33mMyLayer\u001b[0m)            │ (\u001b[38;5;34m10\u001b[0m)                   │           \u001b[38;5;34m370\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ my_layer_5 (\u001b[38;5;33mMyLayer\u001b[0m)            │ (\u001b[38;5;34m10\u001b[0m)                   │           \u001b[38;5;34m110\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ my_layer_6 (\u001b[38;5;33mMyLayer\u001b[0m)            │ (\u001b[38;5;34m6\u001b[0m)                    │            \u001b[38;5;34m66\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">546</span> (2.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m546\u001b[0m (2.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">546</span> (2.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m546\u001b[0m (2.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn2 = MyMLP()\n",
    "nn2.add_layer(MyLayer(X_train.shape[1], 10, act='relu'))\\\n",
    "   .add_layer(MyLayer(10, 10, act='relu'))\\\n",
    "   .compile(nb_out=Y_train.shape[1], lr=0.01, multiclass=True)\n",
    "\n",
    "nn2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0dec320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0 , loss= 1.3791676759719849\n",
      "epoch = 10 , loss= 0.4372740387916565\n",
      "epoch = 20 , loss= 0.40486615896224976\n",
      "epoch = 30 , loss= 0.3750939965248108\n",
      "epoch = 40 , loss= 0.36997494101524353\n",
      "epoch = 50 , loss= 0.3551260530948639\n",
      "epoch = 60 , loss= 0.35428428649902344\n",
      "epoch = 70 , loss= 0.3431343138217926\n",
      "epoch = 80 , loss= 0.33802530169487\n",
      "epoch = 90 , loss= 0.3494139611721039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x77ef405bf8e0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn2.fit(X_train, Y_train, epochs=100, callbacks=[PrintEveryPrEpochs()], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8796250",
   "metadata": {},
   "source": [
    "### III.4. Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccfc12ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.99      0.94      0.96       461\n",
      "           2       0.96      0.93      0.95       224\n",
      "           3       0.86      0.91      0.89       397\n",
      "           4       0.43      0.24      0.31       211\n",
      "           5       0.85      0.73      0.78       237\n",
      "           7       0.71      0.90      0.80       470\n",
      "\n",
      "    accuracy                           0.83      2000\n",
      "   macro avg       0.80      0.78      0.78      2000\n",
      "weighted avg       0.82      0.83      0.82      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, lbin.inverse_transform(nn2(X_test).numpy()), zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d456c2d9",
   "metadata": {},
   "source": [
    "## IV. Low level\n",
    "\n",
    "### IV.1. Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b79a6727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_sigmoid(X):\n",
    "    return 1/(1+tf.math.exp(-X))\n",
    "\n",
    "def simple_ReLU(X):\n",
    "    return tf.where(X > 0., X, 0.)\n",
    "    \n",
    "def simple_softmax(X):\n",
    "    H = tf.math.exp(X)\n",
    "    # return H/tf.math.reduce_sum(H, axis=0)\n",
    "    return H/tf.reshape(tf.math.reduce_sum(H, axis=1), (-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e11914a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.7310586  0.26894143 0.5       ]\n",
      " [0.37754068 0.54983395 0.9933072 ]], shape=(2, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.  0.  0. ]\n",
      " [0.  0.2 5. ]], shape=(2, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.66524094 0.09003058 0.24472848]\n",
      " [0.00403705 0.00812962 0.9878334 ]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "XX = tf.constant([[1., -1., 0.], [-0.5, 0.2, 5]])\n",
    "print(simple_sigmoid(XX))\n",
    "print(simple_ReLU(XX))\n",
    "print(simple_softmax(XX))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d4e2bd",
   "metadata": {},
   "source": [
    "### IV.2. Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5375e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBCE(keras.Loss):\n",
    "    def call(self, H, Y):\n",
    "        return tf.reduce_mean(- Y * tf.math.log(H) - (1-Y) * tf.math.log(1-H))\n",
    "    \n",
    "class SimpleCE(keras.Loss):\n",
    "    def call(self, H, Y):\n",
    "        return tf.reduce_mean(- Y * tf.math.log(H))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5064e6a",
   "metadata": {},
   "source": [
    "### IV.3. Optimization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0d98b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGD(keras.Optimizer):\n",
    "    def __init__(self, learning_rate=0.001):\n",
    "        super().__init__(learning_rate=learning_rate)\n",
    "    def apply_gradients(self, grads_and_vars):\n",
    "        for grads, vars in grads_and_vars:\n",
    "            vars.assign_sub(self.learning_rate * grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3556b894",
   "metadata": {},
   "source": [
    "### IV.4. Custom Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f99791ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SimpleLayer at 0x77ef405705b0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SimpleLayer in here\n",
    "class SimpleLayer(object):\n",
    "    def __init__(self, \n",
    "                 nb_in: int, nb_out: int, \n",
    "                 bias: bool = True, act: Literal['relu', 'sigmoid', 'linear'] = 'linear'):\n",
    "        assert nb_in   > 0\n",
    "        assert nb_out  > 0\n",
    "        super().__init__()\n",
    "\n",
    "        self.W = tf.Variable(tf.zeros([nb_in, nb_out]))\n",
    "        self.trainable_weights = [self.W]\n",
    "        self.b = tf.zeros([1   , nb_out])\n",
    "        if bias:\n",
    "            self.b = tf.Variable(self.b)\n",
    "            self.trainable_weights.append(self.b)\n",
    "\n",
    "        self.act = lambda x: x\n",
    "        if act == 'relu':\n",
    "            self.act = simple_ReLU\n",
    "        elif act == 'sigmoid':\n",
    "            self.act = simple_sigmoid\n",
    "\n",
    "    def randomize(self):\n",
    "        self.W.assign(tf.random.normal(self.W.shape, mean=0.0, stddev=0.1))\n",
    "        if isinstance(self.b, tf.Variable):\n",
    "            self.b.assign(tf.random.normal(self.b.shape, mean=0.0, stddev=0.1))\n",
    "            \n",
    "    def forward(self, X):\n",
    "        return self.act(tf.matmul(X, self.W) + self.b)\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "\n",
    "SimpleLayer(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f158e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0., 0.]], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(3, 2) dtype=float32, numpy=\n",
       " array([[ 0.07582887,  0.08336047],\n",
       "        [-0.15432923,  0.02495772],\n",
       "        [ 0.18534607, -0.06679763]], dtype=float32)>,\n",
       " [<tf.Variable 'Variable:0' shape=(3, 2) dtype=float32, numpy=\n",
       "  array([[ 0.07582887,  0.08336047],\n",
       "         [-0.15432923,  0.02495772],\n",
       "         [ 0.18534607, -0.06679763]], dtype=float32)>])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl = SimpleLayer(3, 2, bias=False)\n",
    "\n",
    "sl.randomize()\n",
    "sl.b, sl.W, sl.trainable_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b104bb",
   "metadata": {},
   "source": [
    "### IV.5. Custom Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec8a6d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(object):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        self.locked = False\n",
    "        self.trainable_weights = []\n",
    "    \n",
    "    def add_layer(self, layer: SimpleLayer):\n",
    "        if self.locked:\n",
    "            raise Exception('You cannot add more layers')\n",
    "        out_nbr = None\n",
    "        if len(self.layers):\n",
    "            out_nbr = self.layers[-1].W.shape[1]\n",
    "        in_nbr = layer.W.shape[0]\n",
    "        if out_nbr is not None and out_nbr != in_nbr:\n",
    "            raise Exception(f'The last layer outputs ({out_nbr}) must be the same as this layer input {in_nbr}')\n",
    "        self.layers.append(layer)\n",
    "        self.trainable_weights.extend(layer.trainable_weights)\n",
    "        return self\n",
    "        \n",
    "    def compile(self, nb_in=1, nb_out=1, bias=True, multiclass=False, lr=1.):\n",
    "        if len(self.layers):\n",
    "            nb_in = self.layers[-1].W.shape[1]\n",
    "        out_layer = SimpleLayer(nb_in, nb_out, bias=bias, act='sigmoid')\n",
    "        self.loss = SimpleBCE()\n",
    "        if multiclass and nb_out > 1:\n",
    "            out_layer.act = simple_softmax\n",
    "            self.loss = SimpleCE()\n",
    "        self.layers.append(out_layer)\n",
    "        self.trainable_weights.extend(self.layers[-1].trainable_weights)\n",
    "        self.optimizer = SimpleGD(learning_rate=lr)\n",
    "        self.locked = True\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        Z = X \n",
    "        for layer in self.layers:\n",
    "            Z = layer(Z)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, X, Y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            Y_pred = self.forward(X)\n",
    "            loss   = self.loss(Y_pred, Y)\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        return loss.numpy()\n",
    "    \n",
    "    def fit(self, X, Y, epochs=20, pr: int=100):\n",
    "        for epoch in range(epochs):\n",
    "            loss = self.backward(X, Y)\n",
    "            if not epoch%pr:\n",
    "                print('epoch', epoch, ' loss =', loss)\n",
    "    \n",
    "    def randomize(self):\n",
    "        for layer in self.layers:\n",
    "            layer.randomize()\n",
    "            \n",
    "    def __call__(self, X):\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0f33ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.8400944]\n",
      " [0.8428117]], shape=(2, 1), dtype=float32)\n",
      "1.0020916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 1) dtype=float32, numpy=\n",
       "array([[0.51494634],\n",
       "       [0.5659208 ]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Result:\n",
    "# tf.Tensor(\n",
    "# [[0.8400944]\n",
    "#  [0.8428117]], shape=(2, 1), dtype=float32)\n",
    "# learning_rate 1.0\n",
    "# 1.0020916\n",
    "# <tf.Variable 'Variable:0' shape=(2, 1) dtype=float32, numpy=\n",
    "# array([[0.51494634],\n",
    "#        [0.5659208 ]], dtype=float32)>\n",
    "\n",
    "nn3t = SimpleMLP()\n",
    "nn3t.add_layer(SimpleLayer(2, 2, act='sigmoid'))\\\n",
    "    .add_layer(SimpleLayer(2, 2, act='sigmoid'))\\\n",
    "    .compile()\n",
    "\n",
    "\n",
    "nn3t.layers[0].W.assign_add(tf.constant([[0.5, 0.3], [0.2, 0.4]]))\n",
    "nn3t.layers[0].b.assign_add(tf.constant([[-0.3, 0.5]]))\n",
    "nn3t.layers[1].W.assign_add(tf.constant([[0.3, -0.1], [0.5, -0.3]]))\n",
    "nn3t.layers[1].b.assign_add(tf.constant([[-0.3, -0.2]]))\n",
    "nn3t.layers[2].W.assign_add(tf.constant([[0.7], [0.7]]))\n",
    "nn3t.layers[2].b.assign_add(tf.constant([[1.]]))\n",
    "\n",
    "XX = tf.constant([[2, -1], [3, 5]], dtype=tf.float32)\n",
    "YY = tf.constant([[0], [1]], dtype=tf.float32)\n",
    "\n",
    "print(nn3t.forward(XX))\n",
    "\n",
    "loss = nn3t.backward(XX, YY)\n",
    "\n",
    "print(loss)\n",
    "\n",
    "nn3t.layers[2].W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a6f2e4",
   "metadata": {},
   "source": [
    "### IV.6. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09c2d695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(36, 10) dtype=float32, numpy=\n",
       " array([[ 0.18425314, -0.11320397,  0.04093604,  0.03161915,  0.07488319,\n",
       "         -0.09128364,  0.0349329 , -0.07464761, -0.02969515, -0.00922264],\n",
       "        [ 0.21519005,  0.03331221,  0.11816205, -0.10681549,  0.00058334,\n",
       "          0.09853227, -0.13442342,  0.13662682,  0.2041689 ,  0.1015119 ],\n",
       "        [ 0.07625891, -0.00866228,  0.02954775,  0.02247149,  0.07250103,\n",
       "          0.08763894,  0.12947802,  0.01101634, -0.0978509 , -0.07510026],\n",
       "        [-0.11247783,  0.09608839, -0.1563275 , -0.00083876,  0.10808287,\n",
       "         -0.07713726,  0.16384551,  0.0305205 ,  0.03858315,  0.00707863],\n",
       "        [ 0.06424464,  0.09899114, -0.22343753,  0.08916724,  0.0366289 ,\n",
       "         -0.19540405, -0.02137367,  0.13679968, -0.10654328, -0.13445829],\n",
       "        [ 0.18460286, -0.01618042,  0.00453827,  0.00556519, -0.04105875,\n",
       "          0.01501153,  0.02578078, -0.05715035,  0.06434871, -0.06879394],\n",
       "        [ 0.07068237,  0.00382259, -0.11242542, -0.13434444,  0.06617597,\n",
       "          0.14263082,  0.16180058,  0.02329234,  0.15605009, -0.17884748],\n",
       "        [ 0.02108465, -0.18013996,  0.0023407 ,  0.15627168, -0.12183034,\n",
       "         -0.10362422,  0.0498807 ,  0.18067151,  0.10681313,  0.14220166],\n",
       "        [-0.05680269,  0.16712421, -0.01277643, -0.02825421,  0.04050505,\n",
       "          0.20312372,  0.1773542 , -0.03329064, -0.01700579,  0.17320774],\n",
       "        [ 0.2448281 , -0.04361978, -0.0017193 , -0.09372608, -0.09665281,\n",
       "          0.08795527, -0.13284306, -0.04768683, -0.00622267, -0.08503928],\n",
       "        [-0.06191008,  0.0546637 ,  0.01627261, -0.08820605,  0.04816813,\n",
       "         -0.01301795, -0.21901087,  0.04202215, -0.11171229,  0.036934  ],\n",
       "        [-0.15676065,  0.01600633,  0.04592276, -0.0290621 ,  0.23964308,\n",
       "          0.00677473,  0.07714389, -0.01563371,  0.01587839, -0.00909435],\n",
       "        [-0.23752753,  0.00245833,  0.21806426, -0.11436949, -0.05863144,\n",
       "         -0.04993393, -0.03836662,  0.1307105 ,  0.1563773 , -0.01801096],\n",
       "        [ 0.00519242,  0.05638676,  0.06477258, -0.07108206,  0.16724442,\n",
       "         -0.04648796, -0.06363463, -0.07551315,  0.01336951, -0.06573403],\n",
       "        [ 0.02129737, -0.01541572, -0.02087117,  0.17092444,  0.07735225,\n",
       "         -0.01432376,  0.08024904, -0.07539823, -0.04955636,  0.28293517],\n",
       "        [ 0.09750043,  0.1489527 , -0.02839381, -0.00193329, -0.00352989,\n",
       "          0.07316665,  0.02563702, -0.04940342,  0.02832232, -0.01891305],\n",
       "        [-0.01569355, -0.03647764,  0.19781636,  0.12621947,  0.05924902,\n",
       "          0.02298527,  0.06062579,  0.02417288, -0.08961754, -0.01399303],\n",
       "        [ 0.04364935, -0.03592655, -0.20201194, -0.06869604,  0.066532  ,\n",
       "         -0.1415987 , -0.07154413,  0.02453282,  0.01233836, -0.07726786],\n",
       "        [ 0.14755186, -0.0800713 ,  0.14873277,  0.06729977, -0.00998188,\n",
       "         -0.09681743, -0.12959091, -0.0368623 , -0.08479442,  0.11579099],\n",
       "        [ 0.04652151, -0.09483797,  0.01630112,  0.12917373, -0.07489584,\n",
       "          0.0967841 , -0.07880455, -0.00525007,  0.01480667, -0.01951371],\n",
       "        [-0.02692948, -0.11115644,  0.05324535,  0.16297154,  0.00767484,\n",
       "         -0.05486707,  0.19373545,  0.04418997, -0.12524335, -0.0312334 ],\n",
       "        [-0.09177398, -0.01854586, -0.10864544,  0.09305369,  0.0855576 ,\n",
       "         -0.00236382,  0.05298335, -0.03726643,  0.02070965,  0.00330857],\n",
       "        [ 0.04084307, -0.01602779,  0.00476386,  0.04409583, -0.01822494,\n",
       "          0.09203507, -0.11772402,  0.12864791, -0.03467774, -0.02936   ],\n",
       "        [-0.11542546, -0.16319314, -0.30468372, -0.15711103,  0.0360967 ,\n",
       "          0.05573396, -0.09897184, -0.15963076,  0.0335924 ,  0.06107928],\n",
       "        [-0.14141549,  0.03951572,  0.14090359, -0.1462354 , -0.09043421,\n",
       "          0.14082241, -0.0419824 ,  0.11189512,  0.00279366, -0.10520002],\n",
       "        [ 0.2334405 , -0.11010158,  0.04650821, -0.09560426,  0.06139514,\n",
       "         -0.1311657 ,  0.02119363, -0.10782512, -0.24042547, -0.21163993],\n",
       "        [-0.07597067,  0.15191162, -0.06339741, -0.10245037, -0.12177428,\n",
       "         -0.06025845, -0.0430361 , -0.04477213,  0.03485153, -0.17978273],\n",
       "        [ 0.03284866,  0.12260365, -0.08109628, -0.19680697, -0.03468158,\n",
       "          0.11112531,  0.06842881, -0.05170802, -0.06179519, -0.0105101 ],\n",
       "        [ 0.00600117, -0.10329919,  0.07680257,  0.02273351,  0.02710036,\n",
       "         -0.08556562, -0.04814065,  0.13223563, -0.06844535, -0.02025972],\n",
       "        [ 0.09718634, -0.04538556, -0.02353512, -0.08742486,  0.00123571,\n",
       "          0.02258532,  0.08930865, -0.06009196,  0.19134288, -0.11061243],\n",
       "        [-0.0005398 , -0.18695442,  0.02869339,  0.02959169, -0.11720672,\n",
       "         -0.07268663,  0.09371737, -0.01301922, -0.09026939,  0.07899661],\n",
       "        [-0.04716844, -0.11621489, -0.08833543,  0.15018375, -0.02939028,\n",
       "         -0.02516134,  0.06163571, -0.12809712,  0.04459813, -0.10812795],\n",
       "        [ 0.04837777,  0.18855165, -0.08605324,  0.01718238, -0.08929636,\n",
       "          0.02744385,  0.16000657, -0.01960547,  0.19176698, -0.01757748],\n",
       "        [-0.14931533, -0.24251902, -0.05744055,  0.03779892,  0.06563422,\n",
       "         -0.07242972, -0.06718922, -0.00920521, -0.02807859,  0.08495042],\n",
       "        [ 0.00547   , -0.01548854,  0.07401334, -0.11010162, -0.01517368,\n",
       "          0.0632561 ,  0.0958105 , -0.11939674, -0.22686128, -0.01348298],\n",
       "        [ 0.14688224,  0.03938819, -0.03948493,  0.13568896, -0.10936844,\n",
       "          0.03651028, -0.21001752,  0.16419467,  0.06150991,  0.02822709]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(1, 10) dtype=float32, numpy=\n",
       " array([[-0.11003884,  0.13421999, -0.15260285,  0.12578958,  0.03038909,\n",
       "         -0.00325764, -0.00455848, -0.03738527, -0.00496812,  0.04311648]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(10, 10) dtype=float32, numpy=\n",
       " array([[-2.61513330e-02,  9.79176685e-02,  2.22621094e-02,\n",
       "          7.99793378e-02,  7.71168619e-02, -3.41232307e-02,\n",
       "         -2.06005387e-02, -1.59689467e-02, -1.13489173e-01,\n",
       "         -1.43026531e-01],\n",
       "        [-3.02515812e-02, -5.76372817e-03,  3.35339382e-02,\n",
       "          1.10612353e-02,  4.77039851e-02,  9.76394024e-03,\n",
       "         -4.25831713e-02,  1.28403351e-01,  2.02009827e-01,\n",
       "         -1.42150491e-01],\n",
       "        [ 1.33316979e-01, -2.64025666e-02, -1.19688787e-01,\n",
       "          1.01297997e-01,  7.18109533e-02, -6.87478632e-02,\n",
       "         -7.73387626e-02, -1.04688242e-01,  1.18019164e-01,\n",
       "          8.58896300e-02],\n",
       "        [-1.34139806e-01,  1.22398315e-02,  1.70977116e-01,\n",
       "         -2.65845004e-03,  1.66675687e-01,  2.68574096e-02,\n",
       "         -7.92961493e-02, -1.13068121e-02,  1.43968329e-01,\n",
       "          4.20101546e-02],\n",
       "        [ 8.26496109e-02,  5.28382249e-02,  7.66942576e-02,\n",
       "         -6.22523203e-02, -2.58833971e-02, -2.60881986e-02,\n",
       "         -7.50772888e-03, -1.21683799e-01,  1.34267882e-01,\n",
       "          9.87658277e-02],\n",
       "        [ 5.37694804e-02, -2.16479003e-02,  7.40942312e-03,\n",
       "          7.65508935e-02, -4.88788150e-02, -3.30987051e-02,\n",
       "         -4.50702906e-02,  1.18467724e-02, -9.97415707e-02,\n",
       "          1.85106823e-04],\n",
       "        [ 1.16477810e-01,  8.04216415e-02, -1.17599726e-01,\n",
       "         -1.13347702e-01,  3.81743349e-02, -8.13075081e-02,\n",
       "          1.54661879e-01, -1.77718382e-02, -1.65693946e-02,\n",
       "          6.44752234e-02],\n",
       "        [-9.53274220e-02,  1.29086554e-01, -7.48881027e-02,\n",
       "         -1.08405367e-01,  2.26341248e-01,  3.68429273e-02,\n",
       "          1.09365582e-01, -1.43134221e-01,  1.31827697e-01,\n",
       "          6.59660026e-02],\n",
       "        [-1.54505998e-01,  1.18187420e-01,  5.30842058e-02,\n",
       "          2.22553253e-01, -1.09449081e-01, -8.25492367e-02,\n",
       "          6.70750672e-03,  1.94624782e-01, -4.45977822e-02,\n",
       "          6.96538156e-03],\n",
       "        [-1.36491641e-01, -2.08724767e-01, -1.81806069e-02,\n",
       "         -1.82092056e-01, -1.74484085e-02,  6.52063265e-02,\n",
       "          3.55961844e-02,  1.36687607e-01,  1.35917544e-01,\n",
       "         -5.97843248e-03]], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(1, 10) dtype=float32, numpy=\n",
       " array([[ 0.07183394, -0.01409981, -0.09477546,  0.04557202, -0.2179976 ,\n",
       "         -0.00459139, -0.05379222, -0.209554  ,  0.13661693, -0.01496782]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(10, 6) dtype=float32, numpy=\n",
       " array([[-1.6914438e-01, -4.4571973e-02,  6.9870427e-02,  1.0022759e-01,\n",
       "         -6.8617709e-02,  8.7516002e-02],\n",
       "        [-9.7997554e-02, -9.5480047e-02, -8.6708786e-03, -1.7964593e-01,\n",
       "         -8.8131957e-02, -1.3293286e-02],\n",
       "        [-6.2513761e-02, -1.0074971e-02,  1.0331303e-01, -9.4945496e-03,\n",
       "         -4.1570727e-02, -1.9150648e-01],\n",
       "        [ 2.5094494e-01, -5.3085990e-02, -5.7398450e-02,  4.5741495e-02,\n",
       "          1.1566495e-01, -2.0598693e-01],\n",
       "        [-6.0985584e-02,  2.9508892e-02,  5.2725766e-03,  5.2556962e-02,\n",
       "          7.2750553e-02, -3.1355661e-03],\n",
       "        [-1.0328249e-01,  1.3478033e-01, -1.3156934e-01,  3.1202290e-02,\n",
       "          2.0187439e-02,  8.4356070e-02],\n",
       "        [-1.0920348e-01, -1.0287275e-01,  1.8678246e-01, -6.8906866e-02,\n",
       "         -1.0639737e-02,  5.1873229e-03],\n",
       "        [-3.5049248e-02, -5.3972777e-02, -1.2822612e-02, -1.0851001e-01,\n",
       "          2.2714915e-02, -5.9782889e-02],\n",
       "        [-1.5863565e-01, -7.5540625e-02,  1.9222671e-04,  2.2592962e-02,\n",
       "         -7.2082944e-02,  2.0636009e-01],\n",
       "        [-3.2822564e-02, -4.0733211e-02, -1.3937189e-01, -8.1145056e-02,\n",
       "          1.3347554e-01,  2.7768914e-02]], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(1, 6) dtype=float32, numpy=\n",
       " array([[ 0.0326675 ,  0.11405053, -0.02302905,  0.25484103, -0.10448255,\n",
       "          0.1762774 ]], dtype=float32)>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn3 = SimpleMLP()\n",
    "nn3.add_layer(SimpleLayer(X_train.shape[1], 10, act='relu'))\\\n",
    "   .add_layer(SimpleLayer(10, 10, act='relu'))\\\n",
    "   .compile(nb_out=Y_train.shape[1], lr=0.01, multiclass=True)\n",
    "\n",
    "nn3.randomize()\n",
    "\n",
    "list(nn3.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f01a5809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  loss = 0.30071974\n",
      "epoch 100  loss = 0.2998662\n",
      "epoch 200  loss = 0.2990634\n",
      "epoch 300  loss = 0.29830793\n",
      "epoch 400  loss = 0.2975965\n",
      "epoch 500  loss = 0.29692635\n",
      "epoch 600  loss = 0.29629487\n",
      "epoch 700  loss = 0.2956996\n",
      "epoch 800  loss = 0.29513833\n",
      "epoch 900  loss = 0.29460898\n"
     ]
    }
   ],
   "source": [
    "nn3.fit(X_train, Y_train, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251617ac",
   "metadata": {},
   "source": [
    "### IV.7. Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79461405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00       461\n",
      "           2       0.00      0.00      0.00       224\n",
      "           3       0.00      0.00      0.00       397\n",
      "           4       0.00      0.00      0.00       211\n",
      "           5       0.00      0.00      0.00       237\n",
      "           7       0.23      1.00      0.38       470\n",
      "\n",
      "    accuracy                           0.23      2000\n",
      "   macro avg       0.04      0.17      0.06      2000\n",
      "weighted avg       0.06      0.23      0.09      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, lbin.inverse_transform(nn3(X_test).numpy()), zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e307da61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
