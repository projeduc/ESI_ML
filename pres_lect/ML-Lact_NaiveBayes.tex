% !TEX TS-program = pdflatex
% !TeX program = pdflatex
% !TEX encoding = UTF-8
% !TEX spellcheck = fr

\documentclass[xcolor=table]{beamer}

\input{options}

\title[ML: Naïve Bayes] %
{Machine Learning \\Naïve Bayes}  

\changegraphpath{../img/naive-bayes/}

\begin{document}

\begin{frame}
\frametitle{Naïve Bayes}
\framesubtitle{Rappel}
\huge
\[
\overbrace{P(A|B)}^\text{Postérieure} = \frac{\overbrace{P(A)}^\text{Antérieure} \overbrace{P(B|A)}^{\text{Vraisemblance}}}{\underbrace{P(B)}_\text{Évidence}}
\]
\end{frame}

\begin{frame}
\frametitle{Naïve Bayes}
\framesubtitle{Plan}

\begin{multicols}{2}
	%	\small
	\tableofcontents
\end{multicols}
\end{frame}

\section{Modèles de classification}

\subsection{Modèle discriminatif}

\begin{frame}
	\frametitle{Naïve Bayes}
	\framesubtitle{Modèles de classification : modèle discriminatif}
	
	\begin{itemize}
		\item Le modèle apprend la probabilité conditionnelle $P(Y|X=x)$
		\item apprendre les frontières entre les classes
		\item Exemples des classificateurs 
		\begin{itemize}
			\item Régression logistique
			\item SVM
			\item Arbres de décision 
			\item Réseau de neurones 
			\item ...
		\end{itemize}
	\end{itemize}
	
\end{frame}

\subsection{Modèle génératif}

\begin{frame}
	\frametitle{Naïve Bayes}
	\framesubtitle{Modèles de classification : modèle génératif}
	
	\begin{itemize}
		\item Le modèle apprend la probabilité conditionnelle $P(X|Y=y)$
		\item apprendre la distribution des classes individuelles
		\item Exemples des classificateurs 
		\begin{itemize}
			\item Naïve Bayes
			\item Modèle de Markov caché
			\item Auto-encodeur variationnel
			\item ...
		\end{itemize}
	\end{itemize}
	
\end{frame}

\section{Théorie}

\subsection{Classification}

\begin{frame}
	\frametitle{Naïve Bayes}
	\framesubtitle{Théorie : Classification}
	
	\[P(Y_k|X) = \frac{P(Y_k) P(X|Y_k)}{P(X)}\]
	
	\[P(Y_k|X) \propto P(Y_k) P(X|Y_k)\]
	
	\[P(Y_k|X) \propto P(Y_k) \prod_{j=1}^{N} P(X_j|Y_k)\]
	
	\[\hat{Y} = \arg\max_{Y_k} P(Y_k) \prod_{j=1}^{N} P(X_j|Y_k)\]
	
	\[\hat{Y} = \arg\max_{Y_k} \log P(Y_k) + \sum_{j=1}^{N} \log P(X_j|Y_k)\]
	
\end{frame}

\subsection{Apprentissage}

\begin{frame}
	\frametitle{Naïve Bayes}
	\framesubtitle{Théorie : Apprentissage (probabilité antérieure)}
	
	\[P(Y_k) = \frac{|\{y / y \in Y \wedge y = k\}|}{M}\]
	
	\begin{itemize}
		\item $|\{y / y \in Y \wedge y = k\}|$ est le nombre des échantillons appartenant à la classe $k$ dans le dataset d'entraînement
		\item $M$ est le nombre des échantillons dans le dataset d'entraînement
		\item Si distribution des échantillons d'entraînement est uniforme sur les classes, cette probabilité est négligées 
		\item Si on veut donner le même poids aux classes, on néglige cette probabilité
	\end{itemize}
	
	
\end{frame}


\begin{frame}
	\frametitle{Naïve Bayes}
	\framesubtitle{Théorie : Apprentissage (vraisemblance - loi multinomiale)}
	
	\[P(x_j = v_j|Y_k) = \frac{|X_j = v_j|_{Y_k}}{|Y = k|}\]
	
	\begin{itemize}
		\item $x_j$ est la caractéristique $j$
		\item $v_j$ est une valeur parmi les valeurs existantes dans la caractéristique $j$
		\item $|X_j = v_j|_{Y_k} = |\{Y^{(i)} / Y^{(i)} = k \wedge X^{(i)}_j = v_j\}|$ le nombre des échantillons de la caractéristique $j$ ayant la valeur $v_j$ et appartenant à la classe $k$
		\item $|Y = k| = |\{Y^{(i)} / Y^{(i)} = k \}|$ le nombre des échantillons appartenant à la classe $k$
		\item $V_j$ est la liste des valeurs uniques dans la caractéristique $j$
		\item \expword{sklearn.naive\_bayes.CategoricalNB}
	\end{itemize}

	\[P(x_j = v_j|Y_k) = \frac{|X_j = v_j|_{Y_k} + \alpha}{|Y = k| + \alpha |V_j|}\]
	
	
\end{frame}

\begin{frame}
	\frametitle{Naïve Bayes}
	\framesubtitle{Théorie : Apprentissage (vraisemblance - loi multinomiale - texte)}
	
	\begin{itemize}
		\item Un texte peut être vu comme une seule caractéristique où les mots sont les catégories
		\item $C_k$ est le compte de la classe $k$ dans tous les échantillons
		\item $C_{jk}$ est le compte du mot $w_j$ dans les textes appartenant à la classe $Y_k$
		\item $V$ est le vocabulaire (les différents mots dans le dataset d'entraînement)
		\item \expword{sklearn.naive\_bayes.MultinomialNB}
	\end{itemize}
	
	\[P(mot = w_j|Y_k) = \frac{C_{jk} + \alpha}{C_k + \alpha |V|}\]
	\[\hat{Y} = \arg\max_{Y_k} P(Y_k) * \prod_{m \in texte} P(mot = m|Y_k)\]
	
\end{frame}

\begin{frame}
	\frametitle{Naïve Bayes}
	\framesubtitle{Théorie : Apprentissage (vraisemblance - loi de Bernoulli)}
	
	\[P(x_j|Y_k) = P(X_j|Y_k) x_j + (1-P(X_j|Y_k)) (1-x_j)\]
	\[P(X_j|Y_k) = \frac{|\{X_j^{(i)} = 1 \wedge y^{(i)} = k\}|}{|\{y^{(i)} = k\}|}\]
	\begin{itemize}
		\item $x_j \in \{0, 1\}$ est une valeur donnée de la caractéristique $j$
		\item $X_j$ est l'ensemble des valeurs booléennes de l'attribut $j$ dans l'entraînement
		\item \expword{sklearn.naive\_bayes.BernoulliNB}
	\end{itemize}
	
	
\end{frame}

\begin{frame}
	\frametitle{Naïve Bayes}
	\framesubtitle{Théorie : Apprentissage (vraisemblance - loi normale)}
	
	\[P(X_j = v|Y_k) = \frac{1}{\sqrt{2\pi \sigma_{kj}^2}} e^\frac{-(v-\mu_{kj})^2}{2 \sigma_{kj}^2}\]
	\begin{itemize}
		\item $x_j \in \{0, 1\}$ est représentée par une valeur numérique
		\item $\mu_{kj}$ est l'espérance (la moyenne) des valeurs de l'attribut $j$ appartenant à la classe $k$
		\item $\sigma_{kj}$ est la variance des valeurs de l'attribut $j$ appartenant à la classe $k$
		\item $v$ est une valeur donnée
		\item \expword{sklearn.naive\_bayes.GaussianNB}
	\end{itemize}
	
	
\end{frame}

\section{Application}

\subsection{Loi multinomiale}

\begin{frame}
	\frametitle{Naïve Bayes}
	\framesubtitle{Application : Loi multinomiale}
	
	\scriptsize
	\SetTblrInner{rowsep=0pt,colsep=1pt}
\begin{center}
		\begin{tblr}{
			colspec = {p{.2\textwidth}p{.2\textwidth}p{.2\textwidth}p{.2\textwidth}},
			row{odd} = {lightblue, font=\small},
			row{even} = {lightyellow, font=\small},
			row{1} = {darkblue, font=\bfseries},
		}
		\textcolor{white}{temps} & \textcolor{white}{temperature} & \textcolor{white}{humidite} & \textcolor{white}{vent} & \textcolor{white}{jouer} \\
		ensoleile & chaude & haute & non & non \\
		ensoleile & chaude & haute & oui & non \\
		nuageux & chaude & haute & non & oui \\
		pluvieux & douce & haute & non & oui \\
		pluvieux & fraiche & normale & non & oui \\
		pluvieux & fraiche & normale & oui & non \\
		nuageux & fraiche & normale & oui & oui \\
		ensoleile & douce & haute & non & non \\
		ensoleile & fraiche & normale & non & oui \\
		pluvieux & douce & normale & non & oui \\
		ensoleile & douce & normale & oui & oui \\
		nuageux & douce & haute & oui & oui \\
		nuageux & chaude & normale & non & oui \\
		pluvieux & douce & haute & oui & non \\
	\end{tblr}
\end{center}

	\begin{itemize}
		\item Trouver la classe de \{pluvieux, chaude, haute, non \}
	\end{itemize}
	
	
\end{frame}

\begin{frame}
	\frametitle{Naïve Bayes}
	\framesubtitle{Application : Loi multinomiale (calcul)}
	
	\begin{itemize}
		\item Probabilité à priori
		\begin{itemize}
			\item $P(jouer=oui) = \frac{nbr(jouer = oui)}{nbr(echantillons)} = \frac{9}{14}$
			\item $P(jouer=non) = \frac{nbr(jouer = non)}{nbr(echantillons)} = \frac{5}{14}$
		\end{itemize}
	\item Probabilité de vraisemblance (par caractéristique)
	\begin{itemize}
		\item $P(temps=pluvieux|jouer=oui) = \frac{nbr(temps=pluvieux \wedge jouer=oui)}{nbr(jouer=oui)} = \frac{3}{9}$
		\item $P(temps=pluvieux|jouer=non) = \frac{nbr(temps=pluvieux \wedge jouer=non)}{nbr(jouer=non)} = \frac{2}{5}$
		\item $P(tempur=chaude|jouer=oui) = \frac{nbr(tempurature=chaude \wedge jouer=oui)}{nbr(jouer=oui)} = \frac{2}{9}$
		\item $P(tempur=chaude|jouer=non) = \frac{nbr(tempurature=chaude \wedge jouer=non)}{nbr(jouer=non)} = \frac{2}{5}$
		\item $P(humidite=haute|jouer=oui) = \frac{nbr(humidite=haute \wedge jouer=oui)}{nbr(jouer=oui)} = \frac{3}{9}$
		\item $P(humidite=haute|jouer=non) = \frac{nbr(humidite=haute \wedge jouer=non)}{nbr(jouer=non)} = \frac{4}{5}$
		\item $P(vent=non|jouer=oui) = \frac{nbr(vent=non \wedge jouer=oui)}{nbr(jouer=oui)} = \frac{6}{9}$
		\item $P(vent=non|jouer=non) = \frac{nbr(vent=non \wedge jouer=non)}{nbr(jouer=non)} = \frac{2}{5}$
	\end{itemize}
	\end{itemize}
	
	
\end{frame}

\begin{frame}
	\frametitle{Naïve Bayes}
	\framesubtitle{Application : Loi multinomiale (calcul - suite)}
	
	\begin{itemize}
		\item Probabilité de vraisemblance de l'échantillon $x$
		\begin{itemize}
			\item $P(X=x|jouer=oui) = P(temps=pluvieux|jouer=oui) * P(temps=pluvieux|jouer=non) 
			* P(tempurature=chaude|jouer=oui) * \cdots$
			\item $P(X=x|jouer=oui) = \frac{3}{9} * \frac{2}{9} * \frac{3}{9} * \frac{6}{9} = \frac{12}{9^3}$
			\item $P(X=x|jouer=non) = \frac{2}{5} * \frac{2}{5} * \frac{4}{5} * \frac{2}{5} = \frac{2^5}{5^4}$
		\end{itemize}
		\item Probabilité postérieure $P(y|X=x)$
		\begin{itemize}
			\item $P(jouer=oui|X=x) \propto P(jouer=oui) * P(X=x|jouer=oui) = \frac{9}{14} * \frac{12}{9^3} = \frac{6}{7 * 9^2} = \frac{6}{567} \approx 0.0106$
			\item $P(jouer=non|X=x) \propto P(jouer=non) * P(X=x|jouer=non) = \frac{5}{14} * \frac{2^5}{5^4} = \frac{2^4}{7 * 5^3} = \frac{16}{875} \approx 0.0183$
		\end{itemize}
		\item Décision
		\begin{itemize}
			\item $y=Non$
		\end{itemize}
	\end{itemize}
	
	
\end{frame}


\subsection{Loi de Bernoulli}

\begin{frame}
	\frametitle{Naïve Bayes}
	\framesubtitle{Application : Loi de Bernoulli}
	
	\scriptsize
	\SetTblrInner{rowsep=0pt,colsep=1pt}
\begin{center}
		\begin{tblr}{
			colspec = {p{.2\textwidth}p{.2\textwidth}p{.2\textwidth}p{.2\textwidth}},
			row{odd} = {lightblue, font=\small},
			row{even} = {lightyellow, font=\small},
			row{1} = {darkblue, font=\bfseries},
		}
			\textcolor{white}{confiant} & \textcolor{white}{étudié} & \textcolor{white}{malade} & \textcolor{white}{résultat} \\
			Oui & Non & Non & échouer \\
			Oui & Non & Oui & passer \\
			Non & Oui & Oui & échouer \\
			Non & Oui & Non & passer \\
			Oui & Oui & Oui & passer \\
	\end{tblr}
\end{center}
	
	\begin{itemize}
		\item Trouver la classe de \{Non, Non, Non \}
	\end{itemize}
	
	
\end{frame}


\subsection{Loi normale}

\begin{frame}
	\frametitle{Naïve Bayes}
	\framesubtitle{Application : Loi normale}
	
	\scriptsize
	\SetTblrInner{rowsep=0pt,colsep=1pt}
\begin{center}
		\begin{tblr}{
			colspec = {p{.2\textwidth}p{.2\textwidth}p{.2\textwidth}p{.2\textwidth}},
			row{odd} = {lightblue, font=\small},
			row{even} = {lightyellow, font=\small},
			row{1} = {darkblue, font=\bfseries},
		}
			\textcolor{white}{taille} & \textcolor{white}{poids} & \textcolor{white}{pointure} & \textcolor{white}{sexe} \\
			182 & 81.6 & 30 & masculin\\
			180 & 86.2 & 28 & masculin\\
			170 & 77.1 & 30 & masculin\\
			180 & 74.8 & 25 & masculin\\
			152 & 45.4 & 15 & féminin\\
			168 & 68.0 & 20 & féminin\\
			165 & 59.0 & 18 & féminin\\
			175 & 68.0 & 23 & féminin\\
	\end{tblr}
\end{center}
	
	\begin{itemize}
		\item Trouver la classe de \{183, 59, 20 \}
	\end{itemize}
	
	
\end{frame}

%\insertbibliography{ML-RN}{*}

\end{document}

