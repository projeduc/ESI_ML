% !TEX TS-program = pdflatex
% !TeX program = pdflatex
% !TEX encoding = UTF-8
% !TeX spellcheck = en_US

\documentclass[xcolor=table]{beamer}

\usepackage{../extra/beamer/karimml}

\input{options}

\subtitle[MultiReg. \& AdaGrad]{Multinomial regression and AdaGrad} 

\changegraphpath{../img/rlmul-adagrad/}

\begin{document}
	
\begin{frame}
	\frametitle{\inserttitle}
	\framesubtitle{\insertshortsubtitle: Introduction}
	
	\begin{itemize}
		\item We saw ...
		\begin{itemize}
			\item how to estimate the probability of a class
			\item using logistic regression
			\item in the binary case (belonging to a class or not)
		\end{itemize}
		\item But ...
		\begin{itemize}
			\item what to do when there are several classes?
		\end{itemize}
		\item We saw ...
		\begin{itemize}
			\item how to optimize the objective function
			\item using gradient descent
			\item with a fixed learning rate
		\end{itemize}
		\item But ...
		\begin{itemize}
			\item can we adjust this rate automatically?
		\end{itemize}
	\end{itemize}
\end{frame}


\begin{frame}
	\frametitle{\inserttitle}
	\framesubtitle{\insertshortsubtitle: Plan}
	
	\begin{multicols}{2}
		%	\small
		\tableofcontents
	\end{multicols}
\end{frame}

\section{Binary logistic regression (review)}

\begin{frame}
	\frametitle{\insertshortsubtitle}
	\framesubtitle{\insertsection}
	
	\hgraphpage{RLbin.pdf}
	
\end{frame}

\subsection{Probability estimation}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection}

	\[Z = \sum_{j=1}^{N} \theta_j X_j = X \cdot \theta\]
	\begin{itemize}
		\item $Z[M]$ est un vecteur de $M$ éléments (échantillons)
		\item $X[M, N]$ est une matrice de $M$ échantillons et $N$ caractéristiques
		\item $\theta[N]$ est un vecteur des paramètres de $N$ caractéristiques
	\end{itemize}

	\[H = \sigma(Z) = \frac{1}{1+e^{-Z}}\]

	\begin{itemize}
		\item $H[M]$ est un vecteur de $M$ probabilités (échantillons)
	\end{itemize}

\end{frame}

\subsection{Error and gradient}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection}
	
	\[J_\theta = BCE = \frac{-1}{M} \sum\limits_{i=1}^{M} [Y^{(i)} \log(H^{(i)}) + (1- Y^{(i)}) \log(1 - H^{(i)})]\]
	\begin{itemize}
		\item $Y[M]$ et $H[M]$ sont deux vecteurs de $M$ éléments (échantillons)
		\item $J_\theta$ est un scalaire
	\end{itemize}
	
	
	\[
	\frac{\partial BCE}{\partial \theta_j} = \frac{1}{M} \sum\limits_{i=1}^{M} (H^{(i)} - Y^{(i)}) X_j^{(i)}
	\]
	
	\[
	\frac{\partial BCE}{\partial \theta} = \frac{1}{M} (H - Y) \cdot X
	\]
	
	\begin{itemize}
		\item $\frac{\partial BCE}{\partial \theta}[N]$ est un vecteur de $N$ éléments (caractéristiques)
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection: gradient (derivation)}
	
	\scriptsize
	\vspace{-16pt}
	\begin{align*}
	\frac{\partial BCE}{\partial \theta_j} 
	& = & \frac{-1}{M} \sum\limits_{i=1}^{M} \frac{\partial}{\partial \theta_j} [Y^{(i)} \log(H^{(i)}) + (1- Y^{(i)}) \log(1 - H^{(i)})] \\
	& = & \frac{-1}{M} \sum\limits_{i=1}^{M} [ Y^{(i)} \frac{\partial}{\partial \theta_j} \log(H^{(i)}) + (1- Y^{(i)}) \frac{\partial}{\partial \theta_j}\log(1 - H^{(i)})]\\
	& = & \frac{-1}{M} \sum\limits_{i=1}^{M} [ Y^{(i)} \frac{1}{H^{(i)}} \frac{\partial}{\partial \theta_j} H^{(i)} + (1- Y^{(i)}) \frac{-1}{1-H^{(i)}} \frac{\partial}{\partial \theta_j} H^{(i)})] \\
	& = & \frac{-1}{M} \sum\limits_{i=1}^{M} \frac{Y^{(i)}-H^{(i)}}{H^{(i)}(1-H^{(i)})} \frac{\partial}{\partial \theta_j} H^{(i)} \\
	\end{align*}
	
	\vspace{-26pt}
	\begin{align*}
	\frac{\partial H^{(i)}}{\partial \theta_j} 
	& = & \frac{\partial \sigma(Z^{(i)})}{\partial Z^{(i)}} \frac{\partial Z^{(i)}}{\partial \theta_j}
	 =  [\sigma(Z^{(i)}) (1-\sigma(Z^{(i)}))]\frac{\partial}{\partial \theta_j} \sum\limits_{k=0}^{N} \theta_k X_k^{(i)} 
	 =  H^{(i)} (1-H^{(i)})  X_j^{(i)}\\
	\end{align*}
	
	\vspace{-24pt}
	\begin{align*}
	\frac{\partial BCE}{\partial \theta_j} 
	& = & \frac{-1}{M} \sum\limits_{i=1}^{M} \frac{Y^{(i)}-H^{(i)}}{H^{(i)}(1-H^{(i)})} [H^{(i)} (1-H^{(i)}) X_j^{(i)}] \\
	& = & \frac{1}{M} \sum\limits_{i=1}^{M} (H^{(i)} - Y^{(i)}) X_j^{(i)}\\
	\end{align*}
		
\end{frame}

\subsection{Parameters' update}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection}
	
	\[\theta = \theta - \alpha \frac{\partial J_\theta}{\partial \theta}\]
	
	\begin{itemize}
		\item $\frac{\partial J_\theta}{\partial \theta}[N]$ est un vecteur de $N$ éléments (caractéristiques)
		\item $\theta[N]$ est un vecteur de $N$ éléments (caractéristiques)
		\item $\alpha \in [0, 1]$ taux d'apprentissage
	\end{itemize}
	
\end{frame}

\section{Multinomial logistic regression}

\begin{frame}
	\frametitle{\insertshortsubtitle}
	\framesubtitle{\insertsection}
	
	
\end{frame}

\subsection{Maximum entropy}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection}
	
	\hgraphpage{RLovr.pdf}
	
\end{frame}

\subsection{One-vs-Rest}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection}
	
	\hgraphpage{RLovr.pdf}
	
\end{frame}

\subsection{One-vs-One}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection}
	
	\hgraphpage{RLovo.pdf}
	
\end{frame}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection: Probability estimation}
	
	\[Z = \sum_{j=1}^{N} \theta_j X_j = X \cdot \theta\]
	\begin{itemize}
		\item $Z[M, L]$ est une matrice de $M$ lignes (échantillons), $L$ colonnes (classes)
		\item $X[M, N]$ est une matrice de $M$ lignes (échantillons), $N$ colonnes (caractéristiques)
		\item $\theta[N, L]$ est une matrice de $N$ lignes (caractéristiques), $L$ colonnes (classes)
	\end{itemize}
	
	\[H = softmax(Z) = \frac{e^{Z}}{\sum_{k=1}^{L} e^{Z_k}}\]
	
	\begin{itemize}
		\item $H[M, L]$ est une matrice de $M$ lignes (échantillons), $L$ colonnes (classes)
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection: Error and gradient}
	
	\[J_\theta = \frac{-1}{M} \sum\limits_{i=1}^{M} \sum_{k=1}^{L} Y^{(i)}_k \log(H^{(i)}_k)\]
	\begin{itemize}
		\item $Y[M, L]$ et $H[M, L]$ sont deux matrices de $M\times L$ éléments (échantillons X classes)
		\item $J_\theta$ est un scalaire
	\end{itemize}
	
	
	\[
	\frac{\partial BCE}{\partial \theta_j} = \frac{1}{M} \sum\limits_{i=1}^{M} (H^{(i)} - Y^{(i)}) X_j^{(i)}
	\]
	
	\[
	\frac{\partial BCE}{\partial \theta} = \frac{1}{M} (H - Y) \cdot X
	\]
	
	\begin{itemize}
		\item $\frac{\partial BCE}{\partial \theta}[N, L]$ est une matrice de $N\times L$ éléments (caractéristiques X classes)
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection: Parameters' update}
	
	\[\theta = \theta - \alpha \frac{\partial J_\theta}{\partial \theta}\]
	
	\begin{itemize}
		\item $\frac{\partial BCE}{\partial \theta}[N, L]$ est une matrice de $N\times L$ éléments (caractéristiques X classes)
		\item $\theta[N, L]$ est une matrice de $N$ lignes (caractéristiques), $L$ colonnes (classes)
		\item $\alpha \in [0, 1]$ taux d'apprentissage
	\end{itemize}
	
\end{frame}

\section{AdaGrad}

\begin{frame}
	\frametitle{\insertshortsubtitle}
	\framesubtitle{\insertsection}
	
	
\end{frame}

\subsection{Gradient descent (review)}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection}
	
\begin{columns}
\begin{column}{.7\textwidth}
	\begin{algorithm}[H]
		\KwData{$ X, Y, \alpha, T $}
		\KwResult{$ \theta $}
		initialiser $ \theta $ ; $ t = 0 $\;
		\While{$ t < T$ et pas de convergence}{
			$ \theta = \theta - \alpha \Delta_\theta J(X, Y; \theta) $\;
			%		Mettre à jours les $\theta$\;
			t = t + 1 \;
		}
		\caption{descente du gradient}
	\end{algorithm}
\end{column}
\begin{column}{.3\textwidth}
	\hgraphpage{DG-J.png}
\end{column}
\end{columns}
	
\end{frame}

\subsection{AdaGrad}

\begin{frame}
	\frametitle{\insertshortsubtitle: \insertsection}
	\framesubtitle{\insertsubsection}
\begin{columns}
\begin{column}{.7\textwidth}
	\begin{algorithm}[H]
		\KwData{$ X, Y, \alpha, T$}
		\KwResult{$ \theta $}
		initialiser $ \theta $ ; $ t = 0 $\;
		initialiser $V$ ($|V| = |\theta|$) à zéro\;
		\While{$ t < T$ et pas de convergence}{
			$ V = V + (\Delta_\theta J(X, Y; \theta))^2 $\;
			$ \theta = \theta - \frac{\alpha}{\sqrt{v + \epsilon}} \Delta_\theta J(X, Y; \theta) $\;
			%		Mettre à jours les $\theta$\;
			\tcp{epsilon est utilisé pour éviter la division par 0, en général 1e-8}
			t = t + 1 \;
		}
		\caption{AdaGrad}
	\end{algorithm}
\end{column}
\begin{column}{.3\textwidth}
	\hgraphpage{AdaGrad-J.png}
\end{column}
\end{columns}
\end{frame}


%\insertbibliography{ML-preparation}{*}

\end{document}



